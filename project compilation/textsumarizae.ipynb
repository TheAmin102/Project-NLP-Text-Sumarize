{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C46oF36Bg0V",
        "outputId": "2d6daa44-62ee-40b2-b87a-d2399a1b74d0"
      },
      "outputs": [],
      "source": [
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install tensorflow\n",
        "# !pip install transformers\n",
        "# !pip install nltk bs4\n",
        "# !pip install lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Preprocessing Data\n",
        "1) Load Data\n",
        "2) Clean Data:-\n",
        "   1) Remove special characters\n",
        "   2) Remove unwanted spaces\n",
        "3) Write data back to a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuhz3v4zwIcn"
      },
      "outputs": [],
      "source": [
        "# LOAD PAPER\n",
        "with open('paperThailand.txt', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "f.close()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEAN DATA\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import bs4 as bs\n",
        "import re\n",
        "\n",
        "parsed_text = bs.BeautifulSoup(text, 'lxml')\n",
        "\n",
        "# Removing Square Brackets and Extra Spaces\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', parsed_text.get_text())\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "\n",
        "# Removing special characters and digits\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE DATA BACK TO TEXT FILE\n",
        "# Save Preprocessed file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95urUhWR2rh7"
      },
      "outputs": [],
      "source": [
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaJzkGHn2uIN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4fFmkkg2wCF"
      },
      "outputs": [],
      "source": [
        "stopwords = list(STOP_WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOXYtmbn2x9x"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# For some OS you need to make a correctino installing the package separately using this command:\n",
        "# !pip3 install -U spacy\n",
        "# !python3 -m spacy download en_core_web_sm\n",
        "# run both command above to install a separate package from the spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehejsUuR2zfs"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KttOAXos272b",
        "outputId": "530e4132-cbab-40cb-a2f8-ade43b4f5ad6"
      },
      "outputs": [],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2y6WfuOJ28OQ",
        "outputId": "5fb27a6e-5eaf-4a31-fc58-909d649624f1"
      },
      "outputs": [],
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNk7VDiI29-s"
      },
      "outputs": [],
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOh7L2ZD3AFD",
        "outputId": "6983aa8c-6b67-4333-da58-98e09ab69dce"
      },
      "outputs": [],
      "source": [
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbRUtgdR3BuR"
      },
      "outputs": [],
      "source": [
        "max_frequency = max(word_frequencies.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hF7k2wP3DYR",
        "outputId": "aa809d5b-9d4f-41a0-f512-4448bd607c1c"
      },
      "outputs": [],
      "source": [
        "max_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4q0KDAr3GAC"
      },
      "outputs": [],
      "source": [
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlBB0xdJ3INI",
        "outputId": "4f536368-21f2-4b9b-fbc3-3fbeed5c8e4e"
      },
      "outputs": [],
      "source": [
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyDY-NLw3J4b",
        "outputId": "b96554c6-de90-4a30-f4ce-c38a931b24a3"
      },
      "outputs": [],
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPZdMgs_3KP2"
      },
      "outputs": [],
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8vLdWTa3MdB",
        "outputId": "3d70da4e-58ce-4244-d890-12a50bf865cf"
      },
      "outputs": [],
      "source": [
        "sentence_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfZLsCg53wCp"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiRTSRGj3xg6",
        "outputId": "dcd4a9b3-173c-408a-f34d-eb9a827d0d87"
      },
      "outputs": [],
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xw5ftgc3y_d"
      },
      "outputs": [],
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rBD_6w_30dv",
        "outputId": "9a8dc1f3-a9e5-491e-ac37-a5a10801c10f"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuiWQWIB311O"
      },
      "outputs": [],
      "source": [
        "final_summary = [word.text for word in summary]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGs6IsSb33tk"
      },
      "outputs": [],
      "source": [
        "summary = ' '.join(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIpxcY4X35CS",
        "outputId": "9f092df7-8971-4898-c5cc-c578e05d900d"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmbwOjsO36qg",
        "outputId": "3580101a-eb3c-4d1d-d5ab-fffa6c5da12b"
      },
      "outputs": [],
      "source": [
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcZib3Q_rTh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    cleaned_text = re.sub('<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters\n",
        "    cleaned_text = re.sub('[^a-zA-Z0-9]', ' ', cleaned_text)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = [word.lower() for sent in sentences for word in word_tokenize(sent)]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return sentences, words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYr1m1cy_tza"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def score_sentences(sentences, words):\n",
        "    word_frequencies = Counter(words)\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for index, sent in enumerate(sentences):\n",
        "        for word in word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    if index not in sentence_scores:\n",
        "                        sentence_scores[index] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[index] += word_frequencies[word]\n",
        "\n",
        "    return sentence_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciawg8Fe_71O"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "def select_sentences(sentences, sentence_scores, num_sentences):\n",
        "    selected_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    summary = [sentences[index] for index in selected_sentences]\n",
        "    return ' '.join(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UeU9J-5vACUE",
        "outputId": "990696b1-b2d9-4d13-e57e-f6eefccc1231"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2hRLgM1sAxUf",
        "outputId": "525e98bb-fa6b-4137-fc49-4c6ebb2af1b6"
      },
      "outputs": [],
      "source": [
        "#import gensim\n",
        "\n",
        "# Training Word2Vec model\n",
        "#sentences = [['sentence', 'one'], ['sentence', 'two'], ...]\n",
        "#model = gensim.models.Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Get word vector representation\n",
        "#word_vector = model['word']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3buEXMQAyPG"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Build LSTM model\n",
        "#model = Sequential()\n",
        "#model.add(LSTM(128, input_shape=(max_sequence_length, embedding_dim)))\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Train the model\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.fit(X_train, y_train, batch_size=32, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n5iWT0EA3BF"
      },
      "outputs": [],
      "source": [
        "#from transformers import pipeline\n",
        "\n",
        "# Load BERT-based summarization model\n",
        "#summarizer = pipeline('summarization')\n",
        "\n",
        "# Summarize text using BERT\n",
        "#text = \"Your input text here\"\n",
        "#summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# Access summary\n",
        "#print(summary[0]['summary_text'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
