{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "text_string = \"ABSTRACT In recent years, people are seeking for a solution to improve text summarization for Thai language. Although several solutions such as PageRank, Graph Rank, Latent Semantic Analysis (LSA) models, etc., have been proposed, research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar. This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document. We compared LSA and Non-negative Matrix Factorization (NMF) to find the algorithm that is suitable with Thai travel news. The suitable compression rates for Generic Sentence Relevance score (GRS) and K-means clustering were also evaluated. From these experiments, we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News, compared with human with the best compression rate of 20%. CCS Concepts • Information systems ➝ Information retrieval ➝ Retrieval tasks and goals➝ Summarization Keywords Text summarization; extractive summarization; non-negative matrix factorization 1. INTRODUCTION Daily newspaper has abundant of text that users do not have enough time for reading them. It is difficult to identify the relevant information to satisfy the information needed by users. Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages. However, there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar. Text Summarization [1] is a technique for summarizing the content of the documents. It consists of three steps: 1) create an intermediate representation of the input text, 2) calculate score for the sentences based on the concepts, and 3) choose important sentences to be included in the summary. Text summarization can be divided into 2 approaches. The first approach is the extractive summarization, which relies on a method for extracting words and searching for keywords from the original document. The second approach is the abstractive summarization, which analyzes words by linguistic principles with transcription or interpretation from the original document. This approach implies more effective and accurate summary than the extractive methods. However, with the lack of Thai corpus, we chose to apply an extractive summarization method for Thai text summarization. This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score (GRS), calculated from Latent Semantic Analysis (LSA) and Non-negative Matrix Factorization (NMF). We also tried using K-means clustering for document summarization. In this experiment, we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20%, 30% and 40% and reported the rate and method that produced the best result from the experiment. 2. RELATED WORKS In recent years, several models in Thai Text summarization have been introduced. Suwanno, N. et al. [2] proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns, term frequency method, and headline score for generating a summary. Chongsuntornsri, A., et al. [3] proposed a new approach for Text summarization in Thai based on content- and graph-based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments. Jaruskulchai C., et al. [4] proposed a method to summarize documents by extracting important sentences from combining the specific properties (Local Property) and the overall properties (Global Property) of the sentences. The overall properties were based on the relationship between sentences in the document. From their experiments, the summarization of the industrial news got 60% precision, 44% recall, and 50.9% F-measure, the general news got the 51.8% precision, 38.5% recall, and 43.1% F-measure while the fashion magazines got 53.0% precision, 33.0% recall, and 40.4% F-measure.\"\n"
      ],
      "metadata": {
        "id": "jLKEZe8t7SY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsBxkipy7KLw"
      },
      "outputs": [],
      "source": [
        "def _create_frequency_table(text_string) -> dict:\n",
        "\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def _create_frequency_table(text_string) -> dict:\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "    print(freqTable)\n",
        "\n",
        "sentences = sent_tokenize(text_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUULjjD_7h0C",
        "outputId": "7164d120-5f48-4b0b-a47e-9d3595ac785a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "    print(sentenceValue)"
      ],
      "metadata": {
        "id": "bcIiu89M8kee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_average_score(sentenceValue) -> int:\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original text\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "    print(average)"
      ],
      "metadata": {
        "id": "2pTyM_1v8ppK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "o3QE1usF88vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ABSTRACT In recent years, people are seeking for a solution to improve text summarization for Thai language. Although several solutions such as PageRank, Graph Rank, Latent Semantic Analysis (LSA) models, etc., have been proposed, research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar. This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document. We compared LSA and Non-negative Matrix Factorization (NMF) to find the algorithm that is suitable with Thai travel news. The suitable compression rates for Generic Sentence Relevance score (GRS) and K-means clustering were also evaluated. From these experiments, we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News, compared with human with the best compression rate of 20%. CCS Concepts • Information systems ➝ Information retrieval ➝ Retrieval tasks and goals➝ Summarization Keywords Text summarization; extractive summarization; non-negative matrix factorization 1. INTRODUCTION Daily newspaper has abundant of text that users do not have enough time for reading them. It is difficult to identify the relevant information to satisfy the information needed by users. Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages. However, there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar. Text Summarization [1] is a technique for summarizing the content of the documents. It consists of three steps: 1) create an intermediate representation of the input text, 2) calculate score for the sentences based on the concepts, and 3) choose important sentences to be included in the summary. Text summarization can be divided into 2 approaches. The first approach is the extractive summarization, which relies on a method for extracting words and searching for keywords from the original document. The second approach is the abstractive summarization, which analyzes words by linguistic principles with transcription or interpretation from the original document. This approach implies more effective and accurate summary than the extractive methods. However, with the lack of Thai corpus, we chose to apply an extractive summarization method for Thai text summarization. This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score (GRS), calculated from Latent Semantic Analysis (LSA) and Non-negative Matrix Factorization (NMF). We also tried using K-means clustering for document summarization. In this experiment, we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20%, 30% and 40% and reported the rate and method that produced the best result from the experiment. 2. RELATED WORKS In recent years, several models in Thai Text summarization have been introduced. Suwanno, N. et al. [2] proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns, term frequency method, and headline score for generating a summary. Chongsuntornsri, A., et al. [3] proposed a new approach for Text summarization in Thai based on content- and graph-based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments. Jaruskulchai C., et al. [4] proposed a method to summarize documents by extracting important sentences from combining the specific properties (Local Property) and the overall properties (Global Property) of the sentences. The overall properties were based on the relationship between sentences in the document. From their experiments, the summarization of the industrial news got 60% precision, 44% recall, and 50.9% F-measure, the general news got the 51.8% precision, 38.5% recall, and 43.1% F-measure while the fashion magazines got 53.0% precision, 33.0% recall, and 40.4% F-measure.\"\n"
      ],
      "metadata": {
        "id": "Y4wCUVct9GjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK1ttl6H-ZmG",
        "outputId": "08eb97df-f214-4269-fa21-35ae9e65f5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP — Text summarization using NLTK: WordFrequency Algorithm"
      ],
      "metadata": {
        "id": "KDwgMjfj-wXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =  \"ABSTRACT In recent years, people are seeking for a solution to improve text summarization for Thai language. Although several solutions such as PageRank, Graph Rank, Latent Semantic Analysis (LSA) models, etc., have been proposed, research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar. This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document. We compared LSA and Non-negative Matrix Factorization (NMF) to find the algorithm that is suitable with Thai travel news. The suitable compression rates for Generic Sentence Relevance score (GRS) and K-means clustering were also evaluated. From these experiments, we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News, compared with human with the best compression rate of 20%. CCS Concepts • Information systems ➝ Information retrieval ➝ Retrieval tasks and goals➝ Summarization Keywords Text summarization; extractive summarization; non-negative matrix factorization 1. INTRODUCTION Daily newspaper has abundant of text that users do not have enough time for reading them. It is difficult to identify the relevant information to satisfy the information needed by users. Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages. However, there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar. Text Summarization [1] is a technique for summarizing the content of the documents. It consists of three steps: 1) create an intermediate representation of the input text, 2) calculate score for the sentences based on the concepts, and 3) choose important sentences to be included in the summary. Text summarization can be divided into 2 approaches. The first approach is the extractive summarization, which relies on a method for extracting words and searching for keywords from the original document. The second approach is the abstractive summarization, which analyzes words by linguistic principles with transcription or interpretation from the original document. This approach implies more effective and accurate summary than the extractive methods. However, with the lack of Thai corpus, we chose to apply an extractive summarization method for Thai text summarization. This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score (GRS), calculated from Latent Semantic Analysis (LSA) and Non-negative Matrix Factorization (NMF). We also tried using K-means clustering for document summarization. In this experiment, we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20%, 30% and 40% and reported the rate and method that produced the best result from the experiment. 2. RELATED WORKS In recent years, several models in Thai Text summarization have been introduced. Suwanno, N. et al. [2] proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns, term frequency method, and headline score for generating a summary. Chongsuntornsri, A., et al. [3] proposed a new approach for Text summarization in Thai based on content- and graph-based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments. Jaruskulchai C., et al. [4] proposed a method to summarize documents by extracting important sentences from combining the specific properties (Local Property) and the overall properties (Global Property) of the sentences. The overall properties were based on the relationship between sentences in the document. From their experiments, the summarization of the industrial news got 60% precision, 44% recall, and 50.9% F-measure, the general news got the 51.8% precision, 38.5% recall, and 43.1% F-measure while the fashion magazines got 53.0% precision, 33.0% recall, and 40.4% F-measure.\"\n",
        "\n",
        "\n",
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = len(word_tokenize(sentence))\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from the original text\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > threshold:\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "# 1 Create the word frequency table\n",
        "freq_table = _create_frequency_table(text)\n",
        "\n",
        "# 2 Tokenize the sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# 3 Important Algorithm: score the sentences\n",
        "sentence_scores = _score_sentences(sentences, freq_table)\n",
        "\n",
        "# 4 Find the threshold\n",
        "threshold = _find_average_score(sentence_scores)\n",
        "\n",
        "# 5 Important Algorithm: Generate the summary\n",
        "summary = _generate_summary(sentences, sentence_scores, 1.5 * threshold)\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPhrfHQQ9Cm4",
        "outputId": "5f25f473-2a08-4771-c27c-aee08f68f462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Text summarization can be divided into 2 approaches. 2. RELATED WORKS In recent years, several models in Thai Text summarization have been introduced. Suwanno, N. et al. Chongsuntornsri, A., et al. Jaruskulchai C., et al.\n"
          ]
        }
      ]
    }
  ]
}