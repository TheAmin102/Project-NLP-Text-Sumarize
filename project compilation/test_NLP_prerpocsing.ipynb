{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Preprocessing Data\n",
        "1) Load Data\n",
        "2) Clean Data:-\n",
        "   1) Remove special characters\n",
        "   2) Remove unwanted spaces\n",
        "3) Write data back to a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD DATA\n",
        "with open('paperThailand.txt', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "f.close()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEAN DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write Data to Text File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Preprocessed Text to a File\n",
        "textFileOutput = 'CleanedText.txt'\n",
        "\n",
        "# Save Preprocessed Text\n",
        "with open(textFileOutput, 'w') as file:\n",
        "    for sentence in text:\n",
        "        file.write(sentence)\n",
        "        file.write('\\n')\n",
        "\n",
        "print('ClenedText File saved to: ', textFileOutput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB2JTFhwuv3r",
        "outputId": "59be1d2f-e6e2-48e4-8da0-1487e94aec6e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import tokenize, stem, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = tokenize.sent_tokenize(text)\n",
        "print('Total sentences in the given text:', len(sentences))\n",
        "print(sentences)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = tokenize.word_tokenize(text)\n",
        "print('Total words in the given text:', len(words))\n",
        "print(words)\n",
        "\n",
        "# Stemming using PorterStemmer\n",
        "stemmer = stem.PorterStemmer()\n",
        "stem_words = [stemmer.stem(word) for word in words]\n",
        "print('After stemming:', stem_words)\n",
        "\n",
        "# Lemmatization using WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print('After lemmatization:', lemma_words)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print('After removing stopwords:', filtered_words)\n",
        "\n",
        "# Part-of-speech tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print('POS tags:', pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOwDS8D1vG_Q",
        "outputId": "254cb335-f60d-487d-cb5f-119ce8289c50"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import tokenize, stem, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import heapq\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Preprocess the text\n",
        "sentences = tokenize.sent_tokenize(text)\n",
        "words = tokenize.word_tokenize(text)\n",
        "stemmer = stem.PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_word(word):\n",
        "    word = word.lower()\n",
        "    if word not in stop_words:\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "        word = stemmer.stem(word)\n",
        "        return word\n",
        "    return None\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    processed_sentence = []\n",
        "    for word in tokenize.word_tokenize(sentence):\n",
        "        processed_word = preprocess_word(word)\n",
        "        if processed_word:\n",
        "            processed_sentence.append(processed_word)\n",
        "    return processed_sentence\n",
        "\n",
        "preprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "# Text summarization using TF-IDF\n",
        "word_frequencies = {}\n",
        "for sentence in preprocessed_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "maximum_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "sentence_scores = {}\n",
        "for i, sentence in enumerate(preprocessed_sentences):\n",
        "    score = 0\n",
        "    for word in sentence:\n",
        "        if word in word_frequencies.keys():\n",
        "            score += word_frequencies[word]\n",
        "    sentence_scores[i] = score\n",
        "\n",
        "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "summary = [sentences[i] for i in summary_sentences]\n",
        "\n",
        "# Print the summary\n",
        "print(\"Summary:\")\n",
        "for sentence in summary:\n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNsbiu207rkC",
        "outputId": "20962780-6ac0-444d-b070-5aff0243be35"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import tokenize, stem, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import heapq\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Preprocess the text\n",
        "sentences = tokenize.sent_tokenize(text)\n",
        "words = tokenize.word_tokenize(text)\n",
        "stemmer = stem.PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_word(word):\n",
        "    word = word.lower()\n",
        "    if word not in stop_words:\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "        word = stemmer.stem(word)\n",
        "        return word\n",
        "    return None\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    processed_sentence = []\n",
        "    for word in tokenize.word_tokenize(sentence):\n",
        "        processed_word = preprocess_word(word)\n",
        "        if processed_word:\n",
        "            processed_sentence.append(processed_word)\n",
        "    return processed_sentence\n",
        "\n",
        "preprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "# Text summarization using TF-IDF\n",
        "word_frequencies = {}\n",
        "for sentence in preprocessed_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "maximum_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "sentence_scores = {}\n",
        "for i, sentence in enumerate(preprocessed_sentences):\n",
        "    score = 0\n",
        "    for word in sentence:\n",
        "        if word in word_frequencies.keys():\n",
        "            score += word_frequencies[word]\n",
        "    sentence_scores[i] = score\n",
        "\n",
        "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "summary = [sentences[i] for i in summary_sentences]\n",
        "\n",
        "# Save preprocessed text and summary to a new file\n",
        "preprocessed_filename = 'preprocessed_text.txt'\n",
        "summary_filename = 'summary.txt'\n",
        "\n",
        "# Save preprocessed text\n",
        "with open(preprocessed_filename, 'w') as file:\n",
        "    for sentence in preprocessed_sentences:\n",
        "        file.write(' '.join(sentence))\n",
        "        file.write('\\n')\n",
        "\n",
        "# Save summary\n",
        "with open(summary_filename, 'w') as file:\n",
        "    for sentence in summary:\n",
        "        file.write(sentence)\n",
        "        file.write('\\n')\n",
        "\n",
        "print(\"Preprocessed text saved to:\", preprocessed_filename)\n",
        "print(\"Summary saved to:\", summary_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
