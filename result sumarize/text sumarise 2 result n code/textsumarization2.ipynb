{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU2vZO0SDbQP",
        "outputId": "bd1597cc-ee35-43e8-fd6f-234908031ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.27.1)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (22.3.5)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def calculate_word_frequencies(text):\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "    freqTable = {}\n",
        "\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in stopwords_set:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_scores(text, freqTable):\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence in sentenceValue:\n",
        "                    sentenceValue[sentence] += freq\n",
        "                else:\n",
        "                    sentenceValue[sentence] = freq\n",
        "\n",
        "    return sentenceValue"
      ],
      "metadata": {
        "id": "DgWIiviMDk2V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average_score(sentenceValue):\n",
        "    sumValues = sum(sentenceValue.values())\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "    return average\n",
        "\n",
        "def generate_summary(sentences, sentenceValue, average):\n",
        "    summary = ''\n",
        "    for sentence in sentences:\n",
        "        if sentence in sentenceValue and sentenceValue[sentence] > (1.2 * average):\n",
        "            summary += sentence\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "2aNRwxtGOh_Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve(text):\n",
        "    freqTable = calculate_word_frequencies(text)\n",
        "    sentenceValue = calculate_sentence_scores(text, freqTable)\n",
        "    average = calculate_average_score(sentenceValue)\n",
        "    sentences = sent_tokenize(text)\n",
        "    summary = generate_summary(sentences, sentenceValue, average)\n",
        "\n",
        "    return summary\n",
        "\n",
        "with open('/content/CleanedTextWords.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = solve(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/solve_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE2FbTmODrnl",
        "outputId": "b1d9f55b-08ec-4424-d57e-f5898a5e31c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sumy_method(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = summarizer(parser.document, 2)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    final_sentence = ' '.join(summary_sentences)\n",
        "    return final_sentence\n",
        "\n",
        "with open('/content/CleanedTextWords.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = sumy_method(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/sumy_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lymIrOmSDz1G",
        "outputId": "8a2beb86-bdc8-43ff-bc7c-7c9c28c571ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chula ac th sentences to be included in the summary Text summarization can be divided into 2 approaches The first approach is the extractive summarization which relies on a method for extracting words and searching for keywords from the original document The second approach is the abstractive summarization which analyzes words by linguistic principles with transcription or interpretation from the original document This approach implies more effective and accurate summary than the extractive methods However with the lack of Thai corpus we chose to apply an extractive summarization method for Thai text summarization This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ and both terms are non negative as shown in Eq 2 and Eq 3 4 ğ‘–ğ‘– 1 ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡ ğ»ğ»ğ‘–ğ‘– ğ‘›ğ‘›ğ‘ğ‘ 1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘Ÿğ‘Ÿ ğ‘ğ‘ 1 ğ‘›ğ‘›ğ‘ğ‘ 1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘ 5 The ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡ ğ»ğ»ğ‘–ğ‘– is the relative relevance of the ith semantic feature ğ‘Šğ‘Šğ‘–ğ‘– where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘ğ‘ and ğ»ğ»ğ‘ğ‘ğ‘ğ‘ is the weight of the topic ğ‘ğ‘ in the sentence ğ‘ğ‘ The sentences can be ranked by Generic Relevance Sentence scores Sentences with the maximum score will be selected into the summary 3 5 Cosine Similarity Cosine similarity 18 is a widely used method to measure the similarity between vectors representing the documents The result of cosine similarity is ranging from 0 to 1 If it is closer to 1 that means both vectors are similar Eq 6 and Eq 7 represents the cosine similarity equation where cos Î¸ is the dot product between vectors of sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def luhn_method(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer_luhn = LuhnSummarizer()\n",
        "    summary = summarizer_luhn(parser.document, 2)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    final_sentence = ' '.join(summary_sentences)\n",
        "    return final_sentence\n",
        "\n",
        "with open('/content/CleanedTextWords.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = luhn_method(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/luhn_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7lHFYhRN3V3",
        "outputId": "480202ab-2bf8-4dc3-f24d-9ac4125656ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daily newspaper has abundant of data that users do not have enough time for reading them It is difficult to identify the relevant information to satisfy the information needed by users Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages However there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar Text Summarization 1 is a technique for summarizing the content of the documents It consists of three steps 1 create an intermediate representation of the input text 2 calculate score for the sentences based on the concepts and 3 choose important Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ and both terms are non negative as shown in Eq 2 and Eq 3 4 ğ‘–ğ‘– 1 ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡ ğ»ğ»ğ‘–ğ‘– ğ‘›ğ‘›ğ‘ğ‘ 1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘Ÿğ‘Ÿ ğ‘ğ‘ 1 ğ‘›ğ‘›ğ‘ğ‘ 1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘ 5 The ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡ ğ»ğ»ğ‘–ğ‘– is the relative relevance of the ith semantic feature ğ‘Šğ‘Šğ‘–ğ‘– where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘ğ‘ and ğ»ğ»ğ‘ğ‘ğ‘ğ‘ is the weight of the topic ğ‘ğ‘ in the sentence ğ‘ğ‘ The sentences can be ranked by Generic Relevance Sentence scores Sentences with the maximum score will be selected into the summary 3 5 Cosine Similarity Cosine similarity 18 is a widely used method to measure the similarity between vectors representing the documents The result of cosine similarity is ranging from 0 to 1 If it is closer to 1 that means both vectors are similar Eq 6 and Eq 7 represents the cosine similarity equation where cos Î¸ is the dot product between vectors of sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lsa_method(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer_lsa = LsaSummarizer()\n",
        "    summary = summarizer_lsa(parser.document, 2)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    final_sentence = ' '.join(summary_sentences)\n",
        "    return final_sentence\n",
        "\n",
        "with open('/content/CleanedTextWords.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = lsa_method(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/lsa_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3vE_BMWN6ck",
        "outputId": "4de8190d-f3af-4e80-b001-f2d08d9e4028"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score hence we used Cutkum for this step Table 1 Comparison of Thai word tokenization programs Tools can be found by solving the optimization problem as follows whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— 0 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– 0 ğ‘šğ‘š ğ‘›ğ‘› ğ‘Ÿğ‘Ÿ ğ‘—ğ‘— 1 ğ‘–ğ‘– 1 ğ‘™ğ‘™ 1 2 ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š\n"
          ]
        }
      ]
    }
  ]
}