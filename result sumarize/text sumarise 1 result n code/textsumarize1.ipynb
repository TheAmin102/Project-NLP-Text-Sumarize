{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IJy6VR12jhxK"
      },
      "outputs": [],
      "source": [
        "# Read text from TXT file\n",
        "with open('/content/CleanedText.txt', 'r') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBUam9ePjtpd",
        "outputId": "4a8aa8e1-2d92-4084-dad3-1ed1438692e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.3\n",
            "    Uninstalling spacy-3.5.3:\n",
            "      Successfully uninstalled spacy-3.5.3\n",
            "Successfully installed spacy-3.5.4\n",
            "2023-07-06 05:09:46.490107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-06 05:09:47.617789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "Vqcv0edZjyY1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "v3x_ygkPj0uR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# For some OS you need to make a correctino installing the package separately using this command:\n",
        "# !pip3 install -U spacy\n",
        "# !python3 -m spacy download en_core_web_sm\n",
        "# run both command above to install a separate package from the spacy"
      ],
      "metadata": {
        "id": "EYiLBvztkAIC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "2RwaCAQKkCqz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ozz4GmkEdI",
        "outputId": "c6ab53bf-11c0-4601-ad86-fe6466b57372"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Extractive', 'Text', 'Summarization', 'for', 'Thai', 'Travel', 'News', 'Based', 'on', 'Keyword', 'Scored', 'in', 'Thai', 'Language', 'Sarunya', 'Nathonghor', 'Duangdao', 'Wichadakul', 'Department', 'of', 'Computer', 'Engineering', 'Chulalongkorn', 'University', 'Bangkok', 'Thailand', 'Department', 'of', 'Computer', 'Engineering', 'Chulalongkorn', 'University', 'Bangkok', 'Thailand', 'Sarunya', 'N', 'Student', 'Chula', 'ac', 'th', 'ABSTRACT', 'In', 'recent', 'years', 'people', 'are', 'seeking', 'for', 'a', 'solution', 'to', 'improve', 'text', 'summarization', 'for', 'Thai', 'language', 'Although', 'several', 'solutions', 'such', 'as', 'PageRank', 'Graph', 'Rank', 'Latent', 'Semantic', 'Analysis', 'LSA', 'models', 'etc', 'have', 'been', 'proposed', 'research', 'results', 'in', 'Thai', 'text', 'summarization', 'were', 'restricted', 'due', 'to', 'limited', 'corpus', 'in', 'Thai', 'language', 'with', 'complex', 'grammar', 'This', 'paper', 'applied', 'a', 'text', 'summarization', 'system', 'for', 'Thai', 'travel', 'news', 'based', 'on', 'keyword', 'scored', 'in', 'Thai', 'language', 'by', 'extracting', 'the', 'most', 'relevant', 'sentences', 'from', 'the', 'original', 'document', 'We', 'compared', 'LSA', 'and', 'Non', 'negative', 'Matrix', 'Factorization', 'NMF', 'to', 'find', 'the', 'algorithm', 'that', 'is', 'suitable', 'with', 'Thai', 'travel', 'news', 'The', 'suitable', 'compression', 'rates', 'for', 'Generic', 'Sentence', 'Relevance', 'score', 'GRS', 'and', 'K', 'means', 'clustering', 'were', 'also', 'evaluated', 'From', 'these', 'experiments', 'we', 'concluded', 'that', 'keyword', 'scored', 'calculation', 'by', 'LSA', 'with', 'sentence', 'selection', 'by', 'GRS', 'is', 'the', 'best', 'algorithm', 'for', 'summarizing', 'Thai', 'Travel', 'News', 'compared', 'with', 'human', 'with', 'the', 'best', 'compression', 'rate', 'of', '20', 'CCS', 'Concepts', 'Information', 'systems', 'Information', 'retrieval', 'Retrieval', 'tasks', 'and', 'goals', 'Summarization', 'Keywords', 'Text', 'summarization', 'extractive', 'summarization', 'non', 'negative', 'matrix', 'factorization', '1', 'INTRODUCTION', 'Daily', 'newspaper', 'has', 'abundant', 'of', 'data', 'that', 'users', 'do', 'not', 'have', 'enough', 'time', 'for', 'reading', 'them', 'It', 'is', 'difficult', 'to', 'identify', 'the', 'relevant', 'information', 'to', 'satisfy', 'the', 'information', 'needed', 'by', 'users', 'Automatic', 'summarization', 'can', 'reduce', 'the', 'problem', 'of', 'information', 'overloading', 'and', 'it', 'has', 'been', 'proposed', 'previously', 'in', 'English', 'and', 'other', 'languages', 'However', 'there', 'were', 'only', 'a', 'few', 'research', 'results', 'in', 'Thai', 'text', 'summarization', 'due', 'to', 'the', 'lack', 'of', 'corpus', 'in', 'Thai', 'language', 'and', 'the', 'complicated', 'grammar', 'Text', 'Summarization', '1', 'is', 'a', 'technique', 'for', 'summarizing', 'the', 'content', 'of', 'the', 'documents', 'It', 'consists', 'of', 'three', 'steps', '1', 'create', 'an', 'intermediate', 'representation', 'of', 'the', 'input', 'text', '2', 'calculate', 'score', 'for', 'the', 'sentences', 'based', 'on', 'the', 'concepts', 'and', '3', 'choose', 'important', 'Permission', 'to', 'make', 'digital', 'or', 'hard', 'copies', 'of', 'all', 'or', 'part', 'of', 'this', 'work', 'for', 'personal', 'or', 'classroom', 'use', 'is', 'granted', 'without', 'fee', 'provided', 'that', 'copies', 'are', 'not', 'made', 'or', 'distributed', 'for', 'profit', 'or', 'commercial', 'advantage', 'and', 'that', 'copies', 'bear', 'this', 'notice', 'and', 'the', 'full', 'citation', 'on', 'the', 'first', 'page', 'Copyrights', 'for', 'components', 'of', 'this', 'work', 'owned', 'by', 'others', 'than', 'ACM', 'must', 'be', 'honored', 'Abstracting', 'with', 'credit', 'is', 'permitted', 'To', 'copy', 'otherwise', 'or', 'republish', 'to', 'post', 'on', 'servers', 'or', 'to', 'redistribute', 'to', 'lists', 'requires', 'prior', 'specific', 'permission', 'and', 'or', 'a', 'fee', 'Request', 'permissions', 'from', 'Permissions', 'acm', 'org', 'ITCC', '2020', 'August', '12', '14', '2020', 'Kuala', 'Lumpur', 'Malaysia', '2020', 'Association', 'for', 'Computing', 'Machinery', 'ACM', 'ISBN', '978', '1', '4503', '7539', '9', '20', '08', '15', '00', 'DOI', 'https', 'doi', 'org', '10', '1145', '3417473', '3417479', 'Duangdao', 'W', 'Chula', 'ac', 'th', 'sentences', 'to', 'be', 'included', 'in', 'the', 'summary', 'Text', 'summarization', 'can', 'be', 'divided', 'into', '2', 'approaches', 'The', 'first', 'approach', 'is', 'the', 'extractive', 'summarization', 'which', 'relies', 'on', 'a', 'method', 'for', 'extracting', 'words', 'and', 'searching', 'for', 'keywords', 'from', 'the', 'original', 'document', 'The', 'second', 'approach', 'is', 'the', 'abstractive', 'summarization', 'which', 'analyzes', 'words', 'by', 'linguistic', 'principles', 'with', 'transcription', 'or', 'interpretation', 'from', 'the', 'original', 'document', 'This', 'approach', 'implies', 'more', 'effective', 'and', 'accurate', 'summary', 'than', 'the', 'extractive', 'methods', 'However', 'with', 'the', 'lack', 'of', 'Thai', 'corpus', 'we', 'chose', 'to', 'apply', 'an', 'extractive', 'summarization', 'method', 'for', 'Thai', 'text', 'summarization', 'This', 'research', 'focused', 'on', 'the', 'sentence', 'extraction', 'function', 'based', 'on', 'keyword', 'score', 'calculation', 'then', 'selecting', 'important', 'sentences', 'based', 'on', 'the', 'Generic', 'Sentence', 'Relevance', 'score', 'GRS', 'calculated', 'from', 'Latent', 'Semantic', 'Analysis', 'LSA', 'and', 'Non', 'negative', 'Matrix', 'Factorization', 'NMF', 'We', 'also', 'tried', 'using', 'K', 'means', 'clustering', 'for', 'document', 'summarization', 'In', 'this', 'experiment', 'we', 'compared', '5', 'models', 'for', '5', 'rounds', 'with', 'Thai', 'travel', 'news', 'using', 'the', 'compression', 'rates', 'of', '20', '30', 'and', '40', 'and', 'reported', 'the', 'rate', 'and', 'method', 'that', 'produced', 'the', 'best', 'result', 'from', 'the', 'experiment', '2', 'RELATED', 'WORKS', 'In', 'recent', 'years', 'several', 'models', 'in', 'Thai', 'Text', 'summarization', 'have', 'been', 'introduced', 'Suwanno', 'N', 'et', 'al', '2', 'proposed', 'a', 'Thai', 'text', 'summarization', 'that', 'extracted', 'a', 'paragraph', 'from', 'a', 'document', 'based', 'on', 'Thai', 'compound', 'nouns', 'term', 'frequency', 'method', 'and', 'headline', 'score', 'for', 'generating', 'a', 'summary', 'Chongsuntornsri', 'A', 'et', 'al', '3', 'proposed', 'a', 'new', 'approach', 'for', 'Text', 'summarization', 'in', 'Thai', 'based', 'on', 'content', 'and', 'graph', 'based', 'with', 'the', 'use', 'of', 'Topic', 'Sensitive', 'PageRank', 'algorithm', 'for', 'summarizing', 'and', 'ranking', 'of', 'text', 'segments', 'Jaruskulchai', 'C', 'et', 'al', '4', 'proposed', 'a', 'method', 'to', 'summarize', 'documents', 'by', 'extracting', 'important', 'sentences', 'from', 'combining', 'the', 'specific', 'properties', 'Local', 'Property', 'and', 'the', 'overall', 'properties', 'Global', 'Property', 'of', 'the', 'sentences', 'The', 'overall', 'properties', 'were', 'based', 'on', 'the', 'relationship', 'between', 'sentences', 'in', 'the', 'document', 'From', 'their', 'experiments', 'the', 'summarization', 'of', 'the', 'industrial', 'news', 'got', '60', 'precision', '44', 'recall', 'and', '50', '9', 'F', 'measure', 'the', 'general', 'news', 'got', 'the', '51', '8', 'precision', '38', '5', 'recall', 'and', '43', '1', 'F', 'measure', 'while', 'the', 'fashion', 'magazines', 'got', '53', '0', 'precision', '33', '0', 'recall', 'and', '40', '4', 'F', 'measure', 'Mani', 'I', 'et', 'al', '5', 'proposed', 'techniques', 'of', 'text', 'summarization', 'by', 'using', 'word', 'frequency', 'in', 'the', 'document', 'and', 'calculated', 'the', 'weight', 'of', 'word', 'to', 'create', 'a', 'keyword', 'group', 'They', 'then', 'calculated', 'the', 'cosine', 'similarity', 'of', 'sentences', 'The', 'researcher', 'used', 'A', 'search', 'algorithm', 'to', 'find', 'the', 'shortest', 'sequence', 'of', 'sentences', 'from', 'keyword', 'group', 'by', 'topic', 'calculation', 'sentence', 'segmentation', 'and', 'word', 'grouping', 'The', 'sequence', 'of', 'sentences', 'that', 'were', 'in', 'the', 'main', 'group', 'were', 'selected', 'as', 'important', 'sentences', 'Their', 'summarization', 'of', 'the', 'agricultural', 'news', 'got', '68', '57', 'precision', '51', '95', 'recall', 'and', '56', '72', 'F', 'measure', 'Lee', 'J', 'et', 'al', '6', 'proposed', 'a', 'document', 'summarization', 'method', 'using', 'Non', 'negative', 'Matrix', 'Factorization', 'NMF', 'They', 'compared', 'between', 'Latent', 'Semantic', 'Analysis', 'LSA', 'and', 'NMF', 'to', 'find', 'the', 'weight', 'of', 'each', 'word', 'and', 'calculated', 'the', 'summation', 'of', 'weights', 'The', 'important', 'sentences', 'were', 'ranked', 'and', 'selected', 'into', 'the', 'summary', 'based', 'on', 'their', 'summed', 'weight', 'Based', 'on', 'LSA', 'they', 'found', 'many', 'weights', 'with', 'zero', 'and', 'negative', 'values', 'However', 'when', 'applied', 'NMF', 'they', 'found', 'only', 'the', 'positive', 'values', 'and', 'the', 'scope', 'of', 'the', 'semantic', 'features', 'meaning', 'was', 'narrow', 'Therefore', 'they', 'proposed', 'that', 'NMF', 'provided', 'a', 'greater', 'possibility', 'for', 'extracting', 'important', 'sentences', '3', 'PREPROCESSING', 'FOR', 'THAI', 'TEXT', 'The', 'first', 'step', 'for', 'working', 'with', 'Thai', 'Text', 'is', 'word', 'tokenization', 'Even', 'though', 'Thai', 'writing', 'system', 'has', 'no', 'delimiters', 'to', 'indicate', 'word', 'boundaries', 'together', 'with', 'many', 'rules', 'for', 'word', 'segmentation', 'several', 'Thai', 'word', 'tokenization', 'programs', 'have', 'been', 'proposed', 'Table', '1', 'shows', 'F1', 'score', 'of', 'the', 'recent', 'programs', 'trained', 'and', 'tested', 'by', 'one', 'of', 'our', 'laboratory', 'members', 'with', 'the', 'data', 'from', 'BEST2010', 'corpus', '7', 'Cutkum', '8', 'got', 'the', 'highest', 'F1', 'score', 'hence', 'we', 'used', 'Cutkum', 'for', 'this', 'step', 'Table', '1', 'Comparison', 'of', 'Thai', 'word', 'tokenization', 'programs', 'Tools', 'F1', 'Score', 'Validate', 'PyICU', '9', 'Article', '100', '0', '6155', 'Encyclopedia', '100', '0', '6932', 'News', '100', '0', '5987', 'Novel', '100', '0', '6800', 'Lexto', '10', '0', '7267', '0', '7709', '0', '6994', '0', '7701', 'Cutkum', 'wordcutpy', '11', '0', '9322', '0', '6212', '0', '9299', '0', '6286', '0', '8987', '0', '6571', '0', '7140', '0', '6247', 'cunlp', '12', '0', '6910', '0', '6172', '0', '5748', '0', '0000', 'SWATH', '13', '0', '6347', '0', '6858', '0', '6200', '0', '6867', '3', '1', 'Latent', 'Semantic', 'Analysis', 'Latent', 'Semantic', 'Analysis', 'LSA', '14', 'is', 'the', 'algorithm', 'which', 'reduces', 'the', 'dimensionality', 'of', 'term', 'document', 'The', 'algorithm', 'creates', 'a', 'matrix', 'by', 'using', 'word', 'frequency', 'applies', 'the', 'singular', 'value', 'decomposition', 'SVD', '15', 'and', 'then', 'finds', 'closely', 'related', 'terms', 'and', 'documents', 'The', 'original', 'matrix', 'A', 'can', 'be', 'separated', 'into', 'three', 'matrices', 'where', 'U', 'is', 'the', 'm', 'x', 'r', 'words', 'x', 'extracted', 'concept', 'matrix', 'V', 'is', 'the', 'n', 'x', 'r', 'sentences', 'x', 'extracted', 'concepts', 'matrix', 'and', 'Σ', 'is', 'the', 'r', 'x', 'r', 'diagonal', 'matrix', 'which', 'can', 'be', 'reconstructed', 'to', 'find', 'the', 'original', 'matrix', 'A', 'The', 'SVD', 'can', 'be', 'represented', 'in', 'Eq', '1', '3', '2', 'A', '𝑈𝑈𝑈𝑈𝑉𝑉', '𝑇𝑇', 'of', 'the', 'related', 'singular', 'value', 'over', 'the', 'sum', 'of', 'all', 'singular', 'values', 'for', 'each', 'concept', '3', '3', '2', 'A', '𝑊𝑊𝑊𝑊', 'Factors', 'W', 'and', 'H', 'can', 'be', 'found', 'by', 'solving', 'the', 'optimization', 'problem', 'as', 'follows', 'where𝑊𝑊𝑗𝑗𝑗𝑗', '0', '𝐻𝐻𝑖𝑖𝑖𝑖', '0', '𝑚𝑚', '𝑛𝑛', '𝑟𝑟', '𝑗𝑗', '1', '𝑖𝑖', '1', '𝑙𝑙', '1', '2', '𝑚𝑚𝑚𝑚𝑚𝑚', '𝐹𝐹', '𝑊𝑊', '𝐻𝐻', '𝐴𝐴', '𝑊𝑊𝑊𝑊', '2𝐹𝐹', '𝐴𝐴𝑖𝑖𝑖𝑖', '𝑊𝑊𝑖𝑖𝑖𝑖', '𝐻𝐻𝑖𝑖𝑖𝑖', '3', 'NMF', 'and', 'LSA', 'are', 'both', 'matrix', 'factorization', 'algorithms', 'However', 'when', 'using', 'NMF', 'to', 'find', 'keywords', 'NMF', 'will', 'return', 'the', 'keywords', 'that', 'are', 'closely', 'related', 'because', 'its', 'components', 'have', 'only', 'nonnegative', 'values', 'As', 'LSA', 'has', 'both', 'positive', 'and', 'negative', 'values', 'as', 'well', 'as', 'some', 'zeroes', 'it', 'gets', 'a', 'wider', 'distribution', 'The', 'semantic', 'feature', 'represents', 'a', 'concept', 'of', 'meaning', 'for', 'root', 'of', 'words', 'that', 'have', 'a', 'relationship', 'For', 'example', 'man', 'human', 'male', 'and', 'adult', 'have', 'the', 'same', 'semantic', 'hence', 'their', 'semantic', 'values', 'are', 'close', 'In', 'this', 'paper', 'we', 'applied', 'LSA', 'and', 'NMF', 'on', 'the', 'Thai', 'Travel', 'News', 'dataset', 'for', 'calculating', 'the', 'semantic', 'weights', 'which', 'represented', 'the', 'relationship', 'between', 'sentences', 'and', 'words', 'in', 'order', 'to', 'select', 'the', 'representative', 'sentences', 'for', 'summarization', '3', '4', 'Generic', 'document', 'summarization', 'by', 'NMF', 'Lee', 'J', 'et', 'al', 'proposed', 'Eq', '4', 'and', 'Eq', '5', 'to', 'select', 'a', 'number', 'of', 'sentences', 'based', 'on', 'NMF', 'which', 'got', 'the', 'highest', 'semantic', 'weight', 'values', 'where', '𝐻𝐻𝑖𝑖𝑖𝑖', 'is', 'the', 'weight', 'of', 'the', 'topic', '𝑖𝑖', 'in', 'the', 'sentence', '𝑗𝑗', 'Generic', 'Relevance', 'of', 'jth', 'sentence', '𝑟𝑟', '1', '𝐻𝐻𝑖𝑖𝑖𝑖', '𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡', '𝐻𝐻𝑖𝑖', 'Document', 'summarization', 'using', 'LSA', 'Gong', 'Y', 'et', 'al', '16', 'proposed', 'a', 'document', 'summarization', 'based', 'on', 'SVD', 'matrices', 'In', 'our', 'work', 'after', 'applying', 'SVD', 'to', 'matrix', 'A', '𝑉𝑉', '𝑇𝑇', 'matrix', 'used', 'for', 'selecting', 'the', 'important', 'sentences', 'The', 'cell', 'value', 'of', 'the', 'matrix', 'shows', 'the', 'relationship', 'between', 'sentence', 'and', 'extracted', 'concepts', 'A', 'sentence', 'with', 'the', 'highest', 'cell', 'value', 'of', 'each', 'concept', 'will', 'be', 'selected', 'into', 'the', 'summary', 'starting', 'from', 'the', 'most', 'important', 'concept', 'The', 'total', 'number', 'of', 'sentences', 'in', 'the', 'summary', 'will', 'be', 'equal', 'to', 'the', 'number', 'all', 'detected', 'concepts', 'Murray', 'G', 'et', 'al', '17', 'proposed', 'a', 'document', 'summarization', 'based', 'on', 'SVD', 'matrices', 'using', '𝑉𝑉', '𝑇𝑇', 'and', 'Σ', 'matrices', 'for', 'sentence', 'selection', 'The', 'authors', 'proposed', 'that', 'more', 'than', 'one', 'sentence', 'could', 'be', 'collected', 'from', 'the', 'more', 'important', 'concepts', 'The', 'decision', 'of', 'how', 'many', 'sentences', 'would', 'be', 'collected', 'from', 'each', 'concept', 'depending', 'on', 'the', 'Σ', 'matrix', 'The', 'value', 'was', 'decided', 'by', 'getting', 'the', 'percentage', 'Non', 'negative', 'Matrix', 'Factorization', 'Non', 'negative', 'Matrix', 'Factorization', 'NMF', 'is', 'a', 'method', 'of', 'matrix', 'factorization', 'subject', 'to', 'the', 'non', 'negative', 'constraint', 'Lee', 'J', 'et', 'al', 'proposed', 'the', 'model', 'based', 'on', 'NMF', 'for', 'document', 'summarization', 'NMF', 'decomposes', 'a', 'non', 'negative', 'matrix', '𝐴𝐴', '𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚', 'into', 'two', 'nonnegative', 'matrices', 'The', 'first', 'matrix', '𝑚𝑚', 'x', '𝑟𝑟', 'is', 'a', 'non', 'negative', 'semantic', 'feature', 'matrix', 'NSFM', '𝑊𝑊', 'The', 'second', 'matrix', '𝑟𝑟', 'x', '𝑛𝑛', 'is', 'a', 'nonnegative', 'semantic', 'variable', 'matrix', 'NSVM', '𝐻𝐻', 'So', 'we', 'have', '𝑊𝑊', '𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚', 'and', '𝐻𝐻', '𝑅𝑅𝑟𝑟𝑟𝑟𝑟𝑟', 'and', 'both', 'terms', 'are', 'non', 'negative', 'as', 'shown', 'in', 'Eq', '2', 'and', 'Eq', '3', '4', '𝑖𝑖', '1', '𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡', '𝐻𝐻𝑖𝑖', '𝑛𝑛𝑞𝑞', '1', '𝐻𝐻𝑖𝑖𝑖𝑖', '𝑟𝑟', '𝑝𝑝', '1', '𝑛𝑛𝑞𝑞', '1', '𝐻𝐻𝑝𝑝𝑝𝑝', '5', 'The', '𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡', '𝐻𝐻𝑖𝑖', 'is', 'the', 'relative', 'relevance', 'of', 'the', 'ith', 'semantic', 'feature', '𝑊𝑊𝑖𝑖', 'where', '𝐻𝐻𝑖𝑖𝑖𝑖', 'is', 'the', 'weight', 'of', 'the', 'topic', '𝑖𝑖', 'in', 'the', 'sentence', '𝑞𝑞', 'and', '𝐻𝐻𝑝𝑝𝑝𝑝', 'is', 'the', 'weight', 'of', 'the', 'topic', '𝑝𝑝', 'in', 'the', 'sentence', '𝑞𝑞', 'The', 'sentences', 'can', 'be', 'ranked', 'by', 'Generic', 'Relevance', 'Sentence', 'scores', 'Sentences', 'with', 'the', 'maximum', 'score', 'will', 'be', 'selected', 'into', 'the', 'summary', '3', '5', 'Cosine', 'Similarity', 'Cosine', 'similarity', '18', 'is', 'a', 'widely', 'used', 'method', 'to', 'measure', 'the', 'similarity', 'between', 'vectors', 'representing', 'the', 'documents', 'The', 'result', 'of', 'cosine', 'similarity', 'is', 'ranging', 'from', '0', 'to', '1', 'If', 'it', 'is', 'closer', 'to', '1', 'that', 'means', 'both', 'vectors', 'are', 'similar', 'Eq', '6', 'and', 'Eq', '7', 'represents', 'the', 'cosine', 'similarity', 'equation', 'where', 'cos', 'θ', 'is', 'the', 'dot', 'product', 'between', 'vectors', 'of', 'sentences', 'A', 'and', 'B', 'and', 'divided', 'by', 'the', 'product', 'of', 'the', 'two', 'vectors', 'lengths', 'In', 'this', 'paper', 'we', 'deployed', 'cosine', 'similarity', 'to', 'measure', 'the', 'similarity', 'of', 'sentences', 'in', 'K', 'means', 'clustering', 'A', 'B', 'A', 'B', 'ni', '1', 'Ai', 'Bi', 'Similarity', 'A', 'B', 'cos', 'θ', '7', 'K', 'means', 'Clustering', '15', '67', '7', '7', '13', '13', '55', '38', 'Table', '2', 'shows', 'the', 'overall', 'number', 'of', 'sentences', 'of', 'news', 'within', 'each', 'dataset', 'The', 'average', 'numbers', 'of', 'sentences', 'per', 'news', 'of', 'the', '5', 'sets', 'were', '21', '16', '15', '13', 'and', '13', 'sentences', 'respectively', '5', 'PIPELINE', 'FOR', 'GENERATING', 'SUMMARIES', 'In', 'this', 'section', 'we', 'demonstrate', 'our', 'pipeline', 'Figure', '1', 'used', 'for', 'text', 'summarization', 'to', 'generate', 'a', 'summary', 'for', 'a', 'Thai', 'travel', 'news', 'Word', 'S9', '6', 'Round', '4', 'Round', '5', 'Table', '3', 'Example', 'of', 'Word', 'by', 'Sentence', 'Matrix', 'A', 'S8', 'Round', '3', 'Avg', 'Number', 'of', 'Sentences', 'S7', '21', '16', 'Round', '1', 'Round', '2', 'Min', 'Number', 'of', 'Sentences', 'S6', '7', '7', 'Max', 'Number', 'of', 'Sentences', '58', '58', 'Dataset', 'S5', 'Table', '2', 'Overall', 'Sentence', 'Language', 'of', 'each', 'Dataset', 'S4', 'DATA', 'PREPARATION', 'The', 'standard', 'data', 'sets', 'in', 'Thai', 'language', 'are', 'unavailable', 'for', 'evaluating', 'text', 'summarization', 'system', 'Therefore', 'we', 'collected', '400', 'Thai', 'travel', 'news', 'from', 'Thairath', 'and', 'Manager', 'online', 'newspapers', 'to', 'be', 'used', 'as', 'datasets', 'for', 'our', 'experiments', 'We', 'split', '400', 'travel', 'news', 'into', '5', 'sets', 'of', '80', 'news', 'each', 'We', 'then', 'evaluated', 'the', 'performance', 'of', 'text', 'summarization', 'methods', 'which', 'were', 'LSA', 'and', 'NMF', 'by', 'comparing', 'their', 'results', 'with', 'the', 'summaries', 'manually', 'curated', 'by', 'two', 'experts', 'from', 'the', 'Faculty', 'of', 'Liberal', 'Arts', 'Ubon', 'Ratchathani', 'University', 'The', 'open', 'source', 'python', 'libraries', 'such', 'as', 'numpy', '19', 'and', 'sklearn', '20', 'were', 'used', 'in', 'our', 'system', 'We', 'converted', 'the', 'Thai', 'travel', 'news', 'obtained', 'from', 'Thairath', 'and', 'Manager', 'online', 'newspapers', 'to', 'plain', 'text', 'Then', 'the', 'sentences', 'of', 'each', 'news', 'were', 'segmented', 'by', 'human', 'with', 'the', 'following', 'format', 'Si', 'xxx', 'where', 'Si', 'represents', 'the', 'order', 'of', 'the', 'sentence', 'in', 'the', 'original', 'document', 'and', 'xxx', 'represents', 'the', 'content', 'of', 'that', 'sentence', 'After', 'removing', 'stop', 'words', 'and', 'duplicate', 'words', 'we', 'built', 'a', 'document', 'term', 'matrix', 'or', 'matrix', 'A', 'then', 'applied', 'SVD', 'and', 'NMF', 'to', 'the', 'matrix', 'Then', 'we', 'used', 'python', 'modules', 'numpy', 'linalg', 'svd', 'to', 'calculate', 'SVD', 'and', 'sklearn', 'decomposition', 'to', 'calculate', 'NMF', 'For', 'sentence', 'selection', 'we', 'used', 'Gong', 'Y', 'et', 'al', 'and', 'Murray', 'G', 'et', 'al', 'approaches', 'for', 'calculating', 'weight', 'of', 'the', 'sentence', 'scores', 'then', 'selected', 'sentences', 'with', 'the', 'highest', 'scores', 'into', 'the', 'summary', 'For', 'keyword', 'score', 'calculation', 'of', 'NMF', 'we', 'calculated', 'the', 'keyword', 'score', 'from', 'Eq', '5', 'and', 'then', 'selected', 'the', 'sentence', 'with', 'the', 'highest', 'score', 'from', 'each', 'concept', 'The', 'python', 'module', 'sklearn', 'cluster', 'was', 'used', 'for', 'K', 'means', 'clustering', 'The', 'selected', 'sentences', 'from', 'all', 'approaches', 'were', 'in', 'the', 'same', 'order', 'as', 'the', 'original', 'document', 'In', 'this', 'paper', 'we', 'performed', 'the', '20', '30', 'and', '40', 'document', 'compression', 'This', 'meant', '80', '70', 'and', '60', 'of', 'the', 'sentences', 'will', 'be', 'selected', 'into', 'the', 'summary', 'S3', '4', 'Figure', '1', 'Document', 'summarization', 'pipeline', 'based', 'on', 'LSA', 'and', 'NMF', 'S2', 'For', 'sentence', 'selection', 'by', 'K', 'means', 'clustering', 'we', 'grouped', 'similar', 'sentences', 'into', 'the', 'same', 'cluster', 'using', 'the', 'following', 'steps', '1', 'Randomly', 'select', 'K', 'sentences', 'as', 'the', 'representative', 'of', 'K', 'groups', 'K', 'in', 'this', 'paper', 'is', 'the', 'number', 'of', 'sentences', 'that', 'will', 'be', 'selected', 'into', 'the', 'summary', '2', 'Calculate', 'centroid', 'of', 'each', 'group', 'by', 'using', 'the', 'value', 'of', 'sentence', 'vector', 'from', 'V', 'matrix', 'for', 'LSA', 'and', '𝐻𝐻𝑇𝑇', 'matrix', 'for', 'NMF', '3', 'Use', 'cosine', 'similarity', 'to', 'calculate', 'sentence', 'similarity', 'between', 'a', 'sentence', 'and', 'the', 'centroid', 'of', 'each', 'group', 'Then', 'assign', 'that', 'sentence', 'to', 'the', 'group', 'with', 'the', 'highest', 'similarity', '4', 'Repeat', 'steps', '2', '3', 'until', 'all', 'sentences', 'are', 'assigned', 'to', 'a', 'group', 'no', 'sentences', 'change', 'the', 'group', 'or', 'the', 'similarity', 'between', 'sentences', 'and', 'their', 'centroid', 'is', 'close', '5', 'Select', 'a', 'sentence', 'with', 'the', 'maximum', 'similarity', 'score', 'with', 'the', 'centroid', 'of', 'the', 'group', 'and', 'add', 'it', 'into', 'the', 'summary', 'S1', '3', '6', 'A', 'B', 'n', 'n', 'A', 'B', 'i', '1', 'A2i', 'i', '1', 'Bi2', '6', 'Mr', 'Yontas', 'ak', '1', '0', '0', '0', '0', '0', '0', '0', '0', 'Supason', '1', '0', '0', '0', '0', '0', '0', '0', '0', 'Tourism', 'Authority', 'of', 'Thailand', '1', '0', '0', '0', '0', '0', '0', '0', '0', 'Table', '3', 'demonstrates', 'an', 'example', 'of', 'a', 'matrix', '𝐴𝐴', 'constructed', 'from', 'word', 'count', 'by', 'sentence', 'of', 'a', 'Thai', 'travel', 'news', 'It', 'was', 'composed', 'of', '98', 'words', 'and', '9', 'sentences', 'This', 'matrix', '𝐴𝐴', 'was', 'then', 'applied', 'with', 'the', 'LSA', 'and', 'NMF', 'The', 'sentence', 'vectors', 'were', 'calculated', 'from', 'the', 'term', 'weight', 'and', 'the', 'semantic', 'feature', 'vectors', 'from', 'Eq', '1', 'for', 'LSA', 'and', 'Eq', '2', 'for', 'NMF', 'sentences', 'from', 'all', 'concepts', 'The', 'Generic', 'Sentence', 'Relevance', 'score', 'for', 'NMF', 'also', 'collected', 'one', 'sentence', 'for', 'each', 'concept', 'the', 'same', 'as', 'Gong', 'Y', 'et', 'al', 'but', 'with', 'the', 'highest', 'score', 'calculated', 'by', 'Eq', '5', 'As', 'multiple', 'important', 'sentences', 'could', 'be', 'selected', 'from', 'a', 'more', 'important', 'concept', 'Murray', 'G', 'et', 'al', 'outperformed', 'both', 'Gong', 'Y', 'et', 'al', 'and', 'the', 'GRS', 'method', '6', 'EXPERIMENT', 'AND', 'RESULTS', '6', '1', 'Performance', 'Evaluations', 'Measure', '7', 'We', 'evaluated', 'the', 'results', 'of', 'the', 'summarization', 'by', 'using', 'standard', 'accuracy', 'precision', 'recall', 'and', 'F1', 'score', '21', 'These', 'measurements', 'quantify', 'the', 'differences', 'between', 'the', 'summary', 'from', 'human', 'and', 'the', 'experimental', 'methods', 'The', 'precision', 'shows', 'the', 'correctness', 'of', 'the', 'extracted', 'sentences', 'and', 'the', 'recall', 'reflects', 'the', 'number', 'of', 'good', 'sentences', 'missed', 'by', 'the', 'method', '6', '2', 'Experiment', 'Results', 'In', 'this', 'experimental', 'set', 'we', 'would', 'like', 'to', 'explore', 'how', 'the', 'different', 'sentence', 'selection', 'methods', 'the', 'Generic', 'Sentence', 'Relevance', 'score', 'and', 'K', 'means', 'clustering', 'affected', 'the', 'text', 'summarization', 'result', 'For', 'K', 'means', 'clustering', 'both', 'SVD', 'and', 'NMF', 'had', 'similar', 'summarization', 'efficiency', 'The', 'F1', 'score', 'of', 'SVD', 'with', 'K', 'means', 'clustering', 'was', '0', '83', '0', '72', 'and', '0', '62', 'for', 'the', 'compression', 'rate', 'of', '20', '30', 'and', '40', 'For', 'the', 'NMF', 'with', 'K', 'means', 'clustering', 'the', 'F1', 'score', 'for', 'the', 'three', 'compression', 'rates', 'was', '0', '83', '0', '74', 'and', '0', '64', 'For', 'the', 'Generic', 'Sentence', 'Relevance', 'score', 'the', 'best', 'F1', 'score', 'for', 'the', 'compression', 'rate', 'of', '20', '30', 'and', '40', 'was', '0', '86', '0', '78', 'and', '0', '68', 'respectively', 'and', 'the', 'best', 'F1', 'scores', 'for', 'all', 'compression', 'rates', 'were', 'from', 'the', 'approach', 'of', 'Murray', 'G', 'et', 'al', 'Figure', '2', 'Thai', 'text', 'summarization', 'efficiency', 'of', '5', 'models', 'Figure', '2', 'shows', 'the', 'Thai', 'text', 'summarization', 'efficiency', 'of', '5', 'models', '1', 'NMF', 'with', 'GRS', '2', 'NMF', 'with', 'K', 'means', '3', 'SVD', 'with', 'sentence', 'score', 'by', 'Gong', 'Y', 'et', 'al', '4', 'SVD', 'with', 'K', 'means', 'and', '5', 'SVD', 'with', 'sentence', 'score', 'by', 'Murray', 'G', 'et', 'al', 'applied', 'to', '400', 'Thai', 'travel', 'news', 'divided', 'into', '5', 'sets', 'of', '80', 'news', 'each', 'with', 'the', 'varied', 'compression', 'rates', 'of', '20', '30', 'and', '40', 'From', 'this', 'experiment', 'the', 'best', 'model', 'based', 'on', 'keyword', 'score', 'for', 'Thai', 'travel', 'news', 'summarization', 'was', 'SVD', 'with', 'sentence', 'selection', 'by', 'Murray', 'G', 'et', 'al', 'This', 'model', 'with', 'the', 'compression', 'rate', 'of', '20', 'got', 'the', 'highest', 'score', 'because', 'Murray', 'G', 'et', 'al', 'method', 'determined', 'the', 'number', 'of', 'sentences', 'to', 'be', 'extracted', 'from', 'each', 'concept', 'based', 'on', 'the', 'importance', 'of', 'that', 'concept', 'The', 'method', 'of', 'Gong', 'Y', 'et', 'al', 'on', 'the', 'other', 'hand', 'was', 'proposed', 'to', 'select', 'only', 'one', 'sentence', 'with', 'the', 'highest', 'score', 'from', 'each', 'concept', 'so', 'that', 'the', 'summary', 'would', 'include', 'CONCLUSIONS', 'In', 'this', 'paper', 'we', 'applied', 'several', 'text', 'summarization', 'methods', 'to', 'Thai', 'Travel', 'News', 'based', 'on', 'keyword', 'scored', 'in', 'Thai', 'language', 'by', 'extracting', 'the', 'most', 'relevant', 'sentences', 'from', 'the', 'original', 'document', 'We', 'compared', 'LSA', 'and', 'NMF', 'together', 'with', 'different', 'sentence', 'selection', 'methods', 'to', 'find', 'the', 'algorithm', 'suitable', 'with', 'this', 'paper', 's', 'data', 'source', 'We', 'concluded', 'that', 'keyword', 'scored', 'calculation', 'by', 'LSA', 'with', 'sentence', 'selection', 'by', 'Generic', 'Sentence', 'Relevance', 'score', 'by', 'Murray', 'G', 'et', 'al', 'was', 'the', 'best', 'algorithm', 'while', 'the', 'best', 'compression', 'rate', 'of', 'all', 'models', 'was', '20', 'for', 'summarizing', 'Thai', 'Travel', 'News', 'compared', 'with', 'humans', 'In', 'future', 'work', 'we', 'plan', 'to', 'perform', 'the', 'experiments', 'with', 'different', 'types', 'of', 'documents', 'and', 'improve', 'word', 'segmentation', 'of', 'compound', 'nouns', 'that', 'was', 'not', 'handled', 'by', 'Cutkum', '8', 'ACKNOWLEDGMENTS', 'We', 'would', 'like', 'to', 'thank', 'the', 'department', 'of', 'computer', 'engineering', 'faculty', 'of', 'engineering', 'Chulalongkorn', 'University', 'for', 'providing', 'computing', 'facilities']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "owXJcM81kJfR",
        "outputId": "5e9266d9-bb90-44d8-88b6-bf92515bdafe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1"
      ],
      "metadata": {
        "id": "PtOwiuTrkJ7J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCFsR0mmkMWE",
        "outputId": "6bdea3af-c7c2-419a-fe33-2de5503f2a9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Extractive': 1, 'Text': 7, 'Summarization': 3, 'Thai': 35, 'Travel': 5, 'News': 6, 'Based': 2, 'Keyword': 1, 'Scored': 1, 'Language': 2, 'Sarunya': 2, 'Nathonghor': 1, 'Duangdao': 2, 'Wichadakul': 1, 'Department': 2, 'Computer': 2, 'Engineering': 2, 'Chulalongkorn': 3, 'University': 4, 'Bangkok': 2, 'Thailand': 3, 'N': 2, 'Student': 1, 'Chula': 2, 'ac': 2, 'th': 2, 'ABSTRACT': 1, 'recent': 3, 'years': 2, 'people': 1, 'seeking': 1, 'solution': 1, 'improve': 2, 'text': 17, 'summarization': 37, 'language': 6, 'solutions': 1, 'PageRank': 2, 'Graph': 1, 'Rank': 1, 'Latent': 5, 'Semantic': 5, 'Analysis': 5, 'LSA': 18, 'models': 6, 'etc': 1, 'proposed': 15, 'research': 3, 'results': 4, 'restricted': 1, 'limited': 1, 'corpus': 4, 'complex': 1, 'grammar': 2, 'paper': 7, 'applied': 7, 'system': 4, 'travel': 10, 'news': 18, 'based': 17, 'keyword': 10, 'scored': 4, 'extracting': 5, 'relevant': 3, 'sentences': 43, 'original': 8, 'document': 18, 'compared': 6, 'Non': 5, 'negative': 12, 'Matrix': 6, 'Factorization': 5, 'NMF': 29, 'find': 6, 'algorithm': 8, 'suitable': 3, 'compression': 11, 'rates': 5, 'Generic': 9, 'Sentence': 9, 'Relevance': 8, 'score': 26, 'GRS': 5, 'K': 15, 'means': 13, 'clustering': 9, 'evaluated': 3, 'experiments': 4, 'concluded': 2, 'calculation': 5, 'sentence': 32, 'selection': 8, 'best': 8, 'summarizing': 4, 'human': 4, 'rate': 6, '20': 10, 'CCS': 1, 'Concepts': 1, 'Information': 2, 'systems': 1, 'retrieval': 1, 'Retrieval': 1, 'tasks': 1, 'goals': 1, 'Keywords': 1, 'extractive': 4, 'non': 5, 'matrix': 25, 'factorization': 3, '1': 32, 'INTRODUCTION': 1, 'Daily': 1, 'newspaper': 1, 'abundant': 1, 'data': 4, 'users': 2, 'time': 1, 'reading': 1, 'difficult': 1, 'identify': 1, 'information': 3, 'satisfy': 1, 'needed': 1, 'Automatic': 1, 'reduce': 1, 'problem': 2, 'overloading': 1, 'previously': 1, 'English': 1, 'languages': 1, 'lack': 2, 'complicated': 1, 'technique': 1, 'content': 3, 'documents': 5, 'consists': 1, 'steps': 3, 'create': 2, 'intermediate': 1, 'representation': 1, 'input': 1, '2': 18, 'calculate': 4, 'concepts': 6, '3': 18, 'choose': 1, 'important': 11, 'Permission': 1, 'digital': 1, 'hard': 1, 'copies': 3, 'work': 4, 'personal': 1, 'classroom': 1, 'use': 2, 'granted': 1, 'fee': 2, 'provided': 2, 'distributed': 1, 'profit': 1, 'commercial': 1, 'advantage': 1, 'bear': 1, 'notice': 1, 'citation': 1, 'page': 1, 'Copyrights': 1, 'components': 2, 'owned': 1, 'ACM': 2, 'honored': 1, 'Abstracting': 1, 'credit': 1, 'permitted': 1, 'copy': 1, 'republish': 1, 'post': 1, 'servers': 1, 'redistribute': 1, 'lists': 1, 'requires': 1, 'prior': 1, 'specific': 2, 'permission': 1, 'Request': 1, 'permissions': 1, 'Permissions': 1, 'acm': 1, 'org': 2, 'ITCC': 1, '2020': 3, 'August': 1, '12': 2, '14': 2, 'Kuala': 1, 'Lumpur': 1, 'Malaysia': 1, 'Association': 1, 'Computing': 1, 'Machinery': 1, 'ISBN': 1, '978': 1, '4503': 1, '7539': 1, '9': 4, '08': 1, '15': 4, '00': 1, 'DOI': 1, 'https': 1, 'doi': 1, '10': 2, '1145': 1, '3417473': 1, '3417479': 1, 'W': 2, 'included': 1, 'summary': 14, 'divided': 3, 'approaches': 3, 'approach': 5, 'relies': 1, 'method': 12, 'words': 8, 'searching': 1, 'keywords': 3, 'second': 2, 'abstractive': 1, 'analyzes': 1, 'linguistic': 1, 'principles': 1, 'transcription': 1, 'interpretation': 1, 'implies': 1, 'effective': 1, 'accurate': 1, 'methods': 6, 'chose': 1, 'apply': 1, 'focused': 1, 'extraction': 1, 'function': 1, 'selecting': 2, 'calculated': 7, 'tried': 1, 'experiment': 3, '5': 18, 'rounds': 1, '30': 5, '40': 6, 'reported': 1, 'produced': 1, 'result': 3, 'RELATED': 1, 'WORKS': 1, 'introduced': 1, 'Suwanno': 1, 'et': 21, 'al': 21, 'extracted': 6, 'paragraph': 1, 'compound': 2, 'nouns': 2, 'term': 4, 'frequency': 3, 'headline': 1, 'generating': 1, 'Chongsuntornsri': 1, 'new': 1, 'graph': 1, 'Topic': 1, 'Sensitive': 1, 'ranking': 1, 'segments': 1, 'Jaruskulchai': 1, 'C': 1, '4': 9, 'summarize': 1, 'combining': 1, 'properties': 3, 'Local': 1, 'Property': 2, 'overall': 3, 'Global': 1, 'relationship': 4, 'industrial': 1, 'got': 7, '60': 2, 'precision': 6, '44': 1, 'recall': 6, '50': 1, 'F': 4, 'measure': 6, 'general': 1, '51': 2, '8': 3, '38': 2, '43': 1, 'fashion': 1, 'magazines': 1, '53': 1, '0': 62, '33': 1, 'Mani': 1, 'techniques': 1, 'word': 12, 'weight': 9, 'group': 9, 'cosine': 5, 'similarity': 12, 'researcher': 1, 'search': 1, 'shortest': 1, 'sequence': 2, 'topic': 4, 'segmentation': 3, 'grouping': 1, 'main': 1, 'selected': 10, 'agricultural': 1, '68': 2, '57': 1, '95': 1, '56': 1, '72': 2, 'Lee': 3, 'J': 3, '6': 8, 'summation': 1, 'weights': 3, 'ranked': 2, 'summed': 1, 'found': 3, 'zero': 1, 'values': 7, 'positive': 2, 'scope': 1, 'semantic': 10, 'features': 1, 'meaning': 2, 'narrow': 1, 'greater': 1, 'possibility': 1, 'PREPROCESSING': 1, 'THAI': 1, 'TEXT': 1, 'step': 2, 'working': 1, 'tokenization': 3, 'writing': 1, 'delimiters': 1, 'indicate': 1, 'boundaries': 1, 'rules': 1, 'programs': 3, 'Table': 6, 'shows': 5, 'F1': 8, 'trained': 1, 'tested': 1, 'laboratory': 1, 'members': 1, 'BEST2010': 1, '7': 8, 'Cutkum': 4, 'highest': 9, 'Comparison': 1, 'Tools': 1, 'Score': 1, 'Validate': 1, 'PyICU': 1, 'Article': 1, '100': 4, '6155': 1, 'Encyclopedia': 1, '6932': 1, '5987': 1, 'Novel': 1, '6800': 1, 'Lexto': 1, '7267': 1, '7709': 1, '6994': 1, '7701': 1, 'wordcutpy': 1, '11': 1, '9322': 1, '6212': 1, '9299': 1, '6286': 1, '8987': 1, '6571': 1, '7140': 1, '6247': 1, 'cunlp': 1, '6910': 1, '6172': 1, '5748': 1, '0000': 1, 'SWATH': 1, '13': 5, '6347': 1, '6858': 1, '6200': 1, '6867': 1, 'reduces': 1, 'dimensionality': 1, 'creates': 1, 'applies': 1, 'singular': 3, 'value': 6, 'decomposition': 2, 'SVD': 13, 'finds': 1, 'closely': 2, 'related': 3, 'terms': 2, 'separated': 1, 'matrices': 5, 'U': 1, 'm': 1, 'x': 7, 'r': 4, 'concept': 12, 'V': 2, 'n': 3, 'Σ': 3, 'diagonal': 1, 'reconstructed': 1, 'represented': 2, 'Eq': 11, '𝑈𝑈𝑈𝑈𝑉𝑉': 1, '𝑇𝑇': 3, 'sum': 1, '𝑊𝑊𝑊𝑊': 2, 'Factors': 1, 'H': 1, 'solving': 1, 'optimization': 1, 'follows': 1, 'where𝑊𝑊𝑗𝑗𝑗𝑗': 1, '𝐻𝐻𝑖𝑖𝑖𝑖': 6, '𝑚𝑚': 2, '𝑛𝑛': 2, '𝑟𝑟': 5, '𝑗𝑗': 2, '𝑖𝑖': 4, '𝑙𝑙': 1, '𝑚𝑚𝑚𝑚𝑚𝑚': 1, '𝐹𝐹': 1, '𝑊𝑊': 3, '𝐻𝐻': 3, '𝐴𝐴': 4, '2𝐹𝐹': 1, '𝐴𝐴𝑖𝑖𝑖𝑖': 1, '𝑊𝑊𝑖𝑖𝑖𝑖': 1, 'algorithms': 1, 'return': 1, 'nonnegative': 3, 'zeroes': 1, 'gets': 1, 'wider': 1, 'distribution': 1, 'feature': 4, 'represents': 4, 'root': 1, 'example': 2, 'man': 1, 'male': 1, 'adult': 1, 'close': 2, 'dataset': 2, 'calculating': 2, 'order': 3, 'select': 4, 'representative': 2, 'number': 7, 'jth': 1, '𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡': 3, '𝐻𝐻𝑖𝑖': 3, 'Document': 2, 'Gong': 6, 'Y': 6, '16': 3, 'applying': 1, '𝑉𝑉': 2, 'cell': 2, 'starting': 1, 'total': 1, 'equal': 1, 'detected': 1, 'Murray': 8, 'G': 8, '17': 1, 'authors': 1, 'collected': 4, 'decision': 1, 'depending': 1, 'decided': 1, 'getting': 1, 'percentage': 1, 'subject': 1, 'constraint': 1, 'model': 3, 'decomposes': 1, '𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚': 2, 'NSFM': 1, 'variable': 1, 'NSVM': 1, '𝑅𝑅𝑟𝑟𝑟𝑟𝑟𝑟': 1, 'shown': 1, '𝑛𝑛𝑞𝑞': 2, '𝑝𝑝': 2, '𝐻𝐻𝑝𝑝𝑝𝑝': 2, 'relative': 1, 'relevance': 1, 'ith': 1, '𝑊𝑊𝑖𝑖': 1, '𝑞𝑞': 2, 'scores': 4, 'Sentences': 4, 'maximum': 2, 'Cosine': 2, 'Similarity': 2, '18': 1, 'widely': 1, 'vectors': 6, 'representing': 1, 'ranging': 1, 'closer': 1, 'similar': 3, 'equation': 1, 'cos': 2, 'θ': 2, 'dot': 1, 'product': 2, 'B': 6, 'lengths': 1, 'deployed': 1, 'ni': 1, 'Ai': 1, 'Bi': 1, 'Clustering': 1, '67': 1, '55': 1, 'average': 1, 'numbers': 1, 'sets': 4, '21': 3, 'respectively': 2, 'PIPELINE': 1, 'GENERATING': 1, 'SUMMARIES': 1, 'section': 1, 'demonstrate': 1, 'pipeline': 2, 'Figure': 4, 'generate': 1, 'Word': 2, 'S9': 1, 'Round': 5, 'Example': 1, 'S8': 1, 'Avg': 1, 'Number': 3, 'S7': 1, 'Min': 1, 'S6': 1, 'Max': 1, '58': 2, 'Dataset': 2, 'S5': 1, 'Overall': 1, 'S4': 1, 'DATA': 1, 'PREPARATION': 1, 'standard': 2, 'unavailable': 1, 'evaluating': 1, '400': 3, 'Thairath': 2, 'Manager': 2, 'online': 2, 'newspapers': 2, 'datasets': 1, 'split': 1, '80': 3, 'performance': 1, 'comparing': 1, 'summaries': 1, 'manually': 1, 'curated': 1, 'experts': 1, 'Faculty': 1, 'Liberal': 1, 'Arts': 1, 'Ubon': 1, 'Ratchathani': 1, 'open': 1, 'source': 2, 'python': 3, 'libraries': 1, 'numpy': 2, '19': 1, 'sklearn': 3, 'converted': 1, 'obtained': 1, 'plain': 1, 'segmented': 1, 'following': 2, 'format': 1, 'Si': 2, 'xxx': 2, 'removing': 1, 'stop': 1, 'duplicate': 1, 'built': 1, 'modules': 1, 'linalg': 1, 'svd': 1, 'module': 1, 'cluster': 2, 'performed': 1, 'meant': 1, '70': 1, 'S3': 1, 'S2': 1, 'grouped': 1, 'Randomly': 1, 'groups': 1, 'Calculate': 1, 'centroid': 4, 'vector': 1, '𝐻𝐻𝑇𝑇': 1, 'Use': 1, 'assign': 1, 'Repeat': 1, 'assigned': 1, 'change': 1, 'Select': 1, 'add': 1, 'S1': 1, 'A2i': 1, 'Bi2': 1, 'Mr': 1, 'Yontas': 1, 'ak': 1, 'Supason': 1, 'Tourism': 1, 'Authority': 1, 'demonstrates': 1, 'constructed': 1, 'count': 1, 'composed': 1, '98': 1, 'multiple': 1, 'outperformed': 1, 'EXPERIMENT': 1, 'RESULTS': 1, 'Performance': 1, 'Evaluations': 1, 'Measure': 1, 'accuracy': 1, 'measurements': 1, 'quantify': 1, 'differences': 1, 'experimental': 2, 'correctness': 1, 'reflects': 1, 'good': 1, 'missed': 1, 'Experiment': 1, 'Results': 1, 'set': 1, 'like': 2, 'explore': 1, 'different': 3, 'affected': 1, 'efficiency': 3, '83': 2, '62': 1, '74': 1, '64': 1, '86': 1, '78': 1, 'varied': 1, 'determined': 1, 'importance': 1, 'hand': 1, 'include': 1, 'CONCLUSIONS': 1, 's': 1, 'humans': 1, 'future': 1, 'plan': 1, 'perform': 1, 'types': 1, 'handled': 1, 'ACKNOWLEDGMENTS': 1, 'thank': 1, 'department': 1, 'computer': 1, 'engineering': 2, 'faculty': 1, 'providing': 1, 'computing': 1, 'facilities': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency = max(word_frequencies.values())"
      ],
      "metadata": {
        "id": "I7ixvA1BkOQO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe5vacsFocIl",
        "outputId": "388cdd12-3bf6-48e5-abd7-9f68dab2cae4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency"
      ],
      "metadata": {
        "id": "N2-sI3cOoubS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9za-mtsouyF",
        "outputId": "0a07477a-1b5c-46d0-f41f-d19cf58f1b7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Extractive': 0.016129032258064516, 'Text': 0.11290322580645161, 'Summarization': 0.04838709677419355, 'Thai': 0.5645161290322581, 'Travel': 0.08064516129032258, 'News': 0.0967741935483871, 'Based': 0.03225806451612903, 'Keyword': 0.016129032258064516, 'Scored': 0.016129032258064516, 'Language': 0.03225806451612903, 'Sarunya': 0.03225806451612903, 'Nathonghor': 0.016129032258064516, 'Duangdao': 0.03225806451612903, 'Wichadakul': 0.016129032258064516, 'Department': 0.03225806451612903, 'Computer': 0.03225806451612903, 'Engineering': 0.03225806451612903, 'Chulalongkorn': 0.04838709677419355, 'University': 0.06451612903225806, 'Bangkok': 0.03225806451612903, 'Thailand': 0.04838709677419355, 'N': 0.03225806451612903, 'Student': 0.016129032258064516, 'Chula': 0.03225806451612903, 'ac': 0.03225806451612903, 'th': 0.03225806451612903, 'ABSTRACT': 0.016129032258064516, 'recent': 0.04838709677419355, 'years': 0.03225806451612903, 'people': 0.016129032258064516, 'seeking': 0.016129032258064516, 'solution': 0.016129032258064516, 'improve': 0.03225806451612903, 'text': 0.27419354838709675, 'summarization': 0.5967741935483871, 'language': 0.0967741935483871, 'solutions': 0.016129032258064516, 'PageRank': 0.03225806451612903, 'Graph': 0.016129032258064516, 'Rank': 0.016129032258064516, 'Latent': 0.08064516129032258, 'Semantic': 0.08064516129032258, 'Analysis': 0.08064516129032258, 'LSA': 0.2903225806451613, 'models': 0.0967741935483871, 'etc': 0.016129032258064516, 'proposed': 0.24193548387096775, 'research': 0.04838709677419355, 'results': 0.06451612903225806, 'restricted': 0.016129032258064516, 'limited': 0.016129032258064516, 'corpus': 0.06451612903225806, 'complex': 0.016129032258064516, 'grammar': 0.03225806451612903, 'paper': 0.11290322580645161, 'applied': 0.11290322580645161, 'system': 0.06451612903225806, 'travel': 0.16129032258064516, 'news': 0.2903225806451613, 'based': 0.27419354838709675, 'keyword': 0.16129032258064516, 'scored': 0.06451612903225806, 'extracting': 0.08064516129032258, 'relevant': 0.04838709677419355, 'sentences': 0.6935483870967742, 'original': 0.12903225806451613, 'document': 0.2903225806451613, 'compared': 0.0967741935483871, 'Non': 0.08064516129032258, 'negative': 0.1935483870967742, 'Matrix': 0.0967741935483871, 'Factorization': 0.08064516129032258, 'NMF': 0.46774193548387094, 'find': 0.0967741935483871, 'algorithm': 0.12903225806451613, 'suitable': 0.04838709677419355, 'compression': 0.1774193548387097, 'rates': 0.08064516129032258, 'Generic': 0.14516129032258066, 'Sentence': 0.14516129032258066, 'Relevance': 0.12903225806451613, 'score': 0.41935483870967744, 'GRS': 0.08064516129032258, 'K': 0.24193548387096775, 'means': 0.20967741935483872, 'clustering': 0.14516129032258066, 'evaluated': 0.04838709677419355, 'experiments': 0.06451612903225806, 'concluded': 0.03225806451612903, 'calculation': 0.08064516129032258, 'sentence': 0.5161290322580645, 'selection': 0.12903225806451613, 'best': 0.12903225806451613, 'summarizing': 0.06451612903225806, 'human': 0.06451612903225806, 'rate': 0.0967741935483871, '20': 0.16129032258064516, 'CCS': 0.016129032258064516, 'Concepts': 0.016129032258064516, 'Information': 0.03225806451612903, 'systems': 0.016129032258064516, 'retrieval': 0.016129032258064516, 'Retrieval': 0.016129032258064516, 'tasks': 0.016129032258064516, 'goals': 0.016129032258064516, 'Keywords': 0.016129032258064516, 'extractive': 0.06451612903225806, 'non': 0.08064516129032258, 'matrix': 0.4032258064516129, 'factorization': 0.04838709677419355, '1': 0.5161290322580645, 'INTRODUCTION': 0.016129032258064516, 'Daily': 0.016129032258064516, 'newspaper': 0.016129032258064516, 'abundant': 0.016129032258064516, 'data': 0.06451612903225806, 'users': 0.03225806451612903, 'time': 0.016129032258064516, 'reading': 0.016129032258064516, 'difficult': 0.016129032258064516, 'identify': 0.016129032258064516, 'information': 0.04838709677419355, 'satisfy': 0.016129032258064516, 'needed': 0.016129032258064516, 'Automatic': 0.016129032258064516, 'reduce': 0.016129032258064516, 'problem': 0.03225806451612903, 'overloading': 0.016129032258064516, 'previously': 0.016129032258064516, 'English': 0.016129032258064516, 'languages': 0.016129032258064516, 'lack': 0.03225806451612903, 'complicated': 0.016129032258064516, 'technique': 0.016129032258064516, 'content': 0.04838709677419355, 'documents': 0.08064516129032258, 'consists': 0.016129032258064516, 'steps': 0.04838709677419355, 'create': 0.03225806451612903, 'intermediate': 0.016129032258064516, 'representation': 0.016129032258064516, 'input': 0.016129032258064516, '2': 0.2903225806451613, 'calculate': 0.06451612903225806, 'concepts': 0.0967741935483871, '3': 0.2903225806451613, 'choose': 0.016129032258064516, 'important': 0.1774193548387097, 'Permission': 0.016129032258064516, 'digital': 0.016129032258064516, 'hard': 0.016129032258064516, 'copies': 0.04838709677419355, 'work': 0.06451612903225806, 'personal': 0.016129032258064516, 'classroom': 0.016129032258064516, 'use': 0.03225806451612903, 'granted': 0.016129032258064516, 'fee': 0.03225806451612903, 'provided': 0.03225806451612903, 'distributed': 0.016129032258064516, 'profit': 0.016129032258064516, 'commercial': 0.016129032258064516, 'advantage': 0.016129032258064516, 'bear': 0.016129032258064516, 'notice': 0.016129032258064516, 'citation': 0.016129032258064516, 'page': 0.016129032258064516, 'Copyrights': 0.016129032258064516, 'components': 0.03225806451612903, 'owned': 0.016129032258064516, 'ACM': 0.03225806451612903, 'honored': 0.016129032258064516, 'Abstracting': 0.016129032258064516, 'credit': 0.016129032258064516, 'permitted': 0.016129032258064516, 'copy': 0.016129032258064516, 'republish': 0.016129032258064516, 'post': 0.016129032258064516, 'servers': 0.016129032258064516, 'redistribute': 0.016129032258064516, 'lists': 0.016129032258064516, 'requires': 0.016129032258064516, 'prior': 0.016129032258064516, 'specific': 0.03225806451612903, 'permission': 0.016129032258064516, 'Request': 0.016129032258064516, 'permissions': 0.016129032258064516, 'Permissions': 0.016129032258064516, 'acm': 0.016129032258064516, 'org': 0.03225806451612903, 'ITCC': 0.016129032258064516, '2020': 0.04838709677419355, 'August': 0.016129032258064516, '12': 0.03225806451612903, '14': 0.03225806451612903, 'Kuala': 0.016129032258064516, 'Lumpur': 0.016129032258064516, 'Malaysia': 0.016129032258064516, 'Association': 0.016129032258064516, 'Computing': 0.016129032258064516, 'Machinery': 0.016129032258064516, 'ISBN': 0.016129032258064516, '978': 0.016129032258064516, '4503': 0.016129032258064516, '7539': 0.016129032258064516, '9': 0.06451612903225806, '08': 0.016129032258064516, '15': 0.06451612903225806, '00': 0.016129032258064516, 'DOI': 0.016129032258064516, 'https': 0.016129032258064516, 'doi': 0.016129032258064516, '10': 0.03225806451612903, '1145': 0.016129032258064516, '3417473': 0.016129032258064516, '3417479': 0.016129032258064516, 'W': 0.03225806451612903, 'included': 0.016129032258064516, 'summary': 0.22580645161290322, 'divided': 0.04838709677419355, 'approaches': 0.04838709677419355, 'approach': 0.08064516129032258, 'relies': 0.016129032258064516, 'method': 0.1935483870967742, 'words': 0.12903225806451613, 'searching': 0.016129032258064516, 'keywords': 0.04838709677419355, 'second': 0.03225806451612903, 'abstractive': 0.016129032258064516, 'analyzes': 0.016129032258064516, 'linguistic': 0.016129032258064516, 'principles': 0.016129032258064516, 'transcription': 0.016129032258064516, 'interpretation': 0.016129032258064516, 'implies': 0.016129032258064516, 'effective': 0.016129032258064516, 'accurate': 0.016129032258064516, 'methods': 0.0967741935483871, 'chose': 0.016129032258064516, 'apply': 0.016129032258064516, 'focused': 0.016129032258064516, 'extraction': 0.016129032258064516, 'function': 0.016129032258064516, 'selecting': 0.03225806451612903, 'calculated': 0.11290322580645161, 'tried': 0.016129032258064516, 'experiment': 0.04838709677419355, '5': 0.2903225806451613, 'rounds': 0.016129032258064516, '30': 0.08064516129032258, '40': 0.0967741935483871, 'reported': 0.016129032258064516, 'produced': 0.016129032258064516, 'result': 0.04838709677419355, 'RELATED': 0.016129032258064516, 'WORKS': 0.016129032258064516, 'introduced': 0.016129032258064516, 'Suwanno': 0.016129032258064516, 'et': 0.3387096774193548, 'al': 0.3387096774193548, 'extracted': 0.0967741935483871, 'paragraph': 0.016129032258064516, 'compound': 0.03225806451612903, 'nouns': 0.03225806451612903, 'term': 0.06451612903225806, 'frequency': 0.04838709677419355, 'headline': 0.016129032258064516, 'generating': 0.016129032258064516, 'Chongsuntornsri': 0.016129032258064516, 'new': 0.016129032258064516, 'graph': 0.016129032258064516, 'Topic': 0.016129032258064516, 'Sensitive': 0.016129032258064516, 'ranking': 0.016129032258064516, 'segments': 0.016129032258064516, 'Jaruskulchai': 0.016129032258064516, 'C': 0.016129032258064516, '4': 0.14516129032258066, 'summarize': 0.016129032258064516, 'combining': 0.016129032258064516, 'properties': 0.04838709677419355, 'Local': 0.016129032258064516, 'Property': 0.03225806451612903, 'overall': 0.04838709677419355, 'Global': 0.016129032258064516, 'relationship': 0.06451612903225806, 'industrial': 0.016129032258064516, 'got': 0.11290322580645161, '60': 0.03225806451612903, 'precision': 0.0967741935483871, '44': 0.016129032258064516, 'recall': 0.0967741935483871, '50': 0.016129032258064516, 'F': 0.06451612903225806, 'measure': 0.0967741935483871, 'general': 0.016129032258064516, '51': 0.03225806451612903, '8': 0.04838709677419355, '38': 0.03225806451612903, '43': 0.016129032258064516, 'fashion': 0.016129032258064516, 'magazines': 0.016129032258064516, '53': 0.016129032258064516, '0': 1.0, '33': 0.016129032258064516, 'Mani': 0.016129032258064516, 'techniques': 0.016129032258064516, 'word': 0.1935483870967742, 'weight': 0.14516129032258066, 'group': 0.14516129032258066, 'cosine': 0.08064516129032258, 'similarity': 0.1935483870967742, 'researcher': 0.016129032258064516, 'search': 0.016129032258064516, 'shortest': 0.016129032258064516, 'sequence': 0.03225806451612903, 'topic': 0.06451612903225806, 'segmentation': 0.04838709677419355, 'grouping': 0.016129032258064516, 'main': 0.016129032258064516, 'selected': 0.16129032258064516, 'agricultural': 0.016129032258064516, '68': 0.03225806451612903, '57': 0.016129032258064516, '95': 0.016129032258064516, '56': 0.016129032258064516, '72': 0.03225806451612903, 'Lee': 0.04838709677419355, 'J': 0.04838709677419355, '6': 0.12903225806451613, 'summation': 0.016129032258064516, 'weights': 0.04838709677419355, 'ranked': 0.03225806451612903, 'summed': 0.016129032258064516, 'found': 0.04838709677419355, 'zero': 0.016129032258064516, 'values': 0.11290322580645161, 'positive': 0.03225806451612903, 'scope': 0.016129032258064516, 'semantic': 0.16129032258064516, 'features': 0.016129032258064516, 'meaning': 0.03225806451612903, 'narrow': 0.016129032258064516, 'greater': 0.016129032258064516, 'possibility': 0.016129032258064516, 'PREPROCESSING': 0.016129032258064516, 'THAI': 0.016129032258064516, 'TEXT': 0.016129032258064516, 'step': 0.03225806451612903, 'working': 0.016129032258064516, 'tokenization': 0.04838709677419355, 'writing': 0.016129032258064516, 'delimiters': 0.016129032258064516, 'indicate': 0.016129032258064516, 'boundaries': 0.016129032258064516, 'rules': 0.016129032258064516, 'programs': 0.04838709677419355, 'Table': 0.0967741935483871, 'shows': 0.08064516129032258, 'F1': 0.12903225806451613, 'trained': 0.016129032258064516, 'tested': 0.016129032258064516, 'laboratory': 0.016129032258064516, 'members': 0.016129032258064516, 'BEST2010': 0.016129032258064516, '7': 0.12903225806451613, 'Cutkum': 0.06451612903225806, 'highest': 0.14516129032258066, 'Comparison': 0.016129032258064516, 'Tools': 0.016129032258064516, 'Score': 0.016129032258064516, 'Validate': 0.016129032258064516, 'PyICU': 0.016129032258064516, 'Article': 0.016129032258064516, '100': 0.06451612903225806, '6155': 0.016129032258064516, 'Encyclopedia': 0.016129032258064516, '6932': 0.016129032258064516, '5987': 0.016129032258064516, 'Novel': 0.016129032258064516, '6800': 0.016129032258064516, 'Lexto': 0.016129032258064516, '7267': 0.016129032258064516, '7709': 0.016129032258064516, '6994': 0.016129032258064516, '7701': 0.016129032258064516, 'wordcutpy': 0.016129032258064516, '11': 0.016129032258064516, '9322': 0.016129032258064516, '6212': 0.016129032258064516, '9299': 0.016129032258064516, '6286': 0.016129032258064516, '8987': 0.016129032258064516, '6571': 0.016129032258064516, '7140': 0.016129032258064516, '6247': 0.016129032258064516, 'cunlp': 0.016129032258064516, '6910': 0.016129032258064516, '6172': 0.016129032258064516, '5748': 0.016129032258064516, '0000': 0.016129032258064516, 'SWATH': 0.016129032258064516, '13': 0.08064516129032258, '6347': 0.016129032258064516, '6858': 0.016129032258064516, '6200': 0.016129032258064516, '6867': 0.016129032258064516, 'reduces': 0.016129032258064516, 'dimensionality': 0.016129032258064516, 'creates': 0.016129032258064516, 'applies': 0.016129032258064516, 'singular': 0.04838709677419355, 'value': 0.0967741935483871, 'decomposition': 0.03225806451612903, 'SVD': 0.20967741935483872, 'finds': 0.016129032258064516, 'closely': 0.03225806451612903, 'related': 0.04838709677419355, 'terms': 0.03225806451612903, 'separated': 0.016129032258064516, 'matrices': 0.08064516129032258, 'U': 0.016129032258064516, 'm': 0.016129032258064516, 'x': 0.11290322580645161, 'r': 0.06451612903225806, 'concept': 0.1935483870967742, 'V': 0.03225806451612903, 'n': 0.04838709677419355, 'Σ': 0.04838709677419355, 'diagonal': 0.016129032258064516, 'reconstructed': 0.016129032258064516, 'represented': 0.03225806451612903, 'Eq': 0.1774193548387097, '𝑈𝑈𝑈𝑈𝑉𝑉': 0.016129032258064516, '𝑇𝑇': 0.04838709677419355, 'sum': 0.016129032258064516, '𝑊𝑊𝑊𝑊': 0.03225806451612903, 'Factors': 0.016129032258064516, 'H': 0.016129032258064516, 'solving': 0.016129032258064516, 'optimization': 0.016129032258064516, 'follows': 0.016129032258064516, 'where𝑊𝑊𝑗𝑗𝑗𝑗': 0.016129032258064516, '𝐻𝐻𝑖𝑖𝑖𝑖': 0.0967741935483871, '𝑚𝑚': 0.03225806451612903, '𝑛𝑛': 0.03225806451612903, '𝑟𝑟': 0.08064516129032258, '𝑗𝑗': 0.03225806451612903, '𝑖𝑖': 0.06451612903225806, '𝑙𝑙': 0.016129032258064516, '𝑚𝑚𝑚𝑚𝑚𝑚': 0.016129032258064516, '𝐹𝐹': 0.016129032258064516, '𝑊𝑊': 0.04838709677419355, '𝐻𝐻': 0.04838709677419355, '𝐴𝐴': 0.06451612903225806, '2𝐹𝐹': 0.016129032258064516, '𝐴𝐴𝑖𝑖𝑖𝑖': 0.016129032258064516, '𝑊𝑊𝑖𝑖𝑖𝑖': 0.016129032258064516, 'algorithms': 0.016129032258064516, 'return': 0.016129032258064516, 'nonnegative': 0.04838709677419355, 'zeroes': 0.016129032258064516, 'gets': 0.016129032258064516, 'wider': 0.016129032258064516, 'distribution': 0.016129032258064516, 'feature': 0.06451612903225806, 'represents': 0.06451612903225806, 'root': 0.016129032258064516, 'example': 0.03225806451612903, 'man': 0.016129032258064516, 'male': 0.016129032258064516, 'adult': 0.016129032258064516, 'close': 0.03225806451612903, 'dataset': 0.03225806451612903, 'calculating': 0.03225806451612903, 'order': 0.04838709677419355, 'select': 0.06451612903225806, 'representative': 0.03225806451612903, 'number': 0.11290322580645161, 'jth': 0.016129032258064516, '𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡': 0.04838709677419355, '𝐻𝐻𝑖𝑖': 0.04838709677419355, 'Document': 0.03225806451612903, 'Gong': 0.0967741935483871, 'Y': 0.0967741935483871, '16': 0.04838709677419355, 'applying': 0.016129032258064516, '𝑉𝑉': 0.03225806451612903, 'cell': 0.03225806451612903, 'starting': 0.016129032258064516, 'total': 0.016129032258064516, 'equal': 0.016129032258064516, 'detected': 0.016129032258064516, 'Murray': 0.12903225806451613, 'G': 0.12903225806451613, '17': 0.016129032258064516, 'authors': 0.016129032258064516, 'collected': 0.06451612903225806, 'decision': 0.016129032258064516, 'depending': 0.016129032258064516, 'decided': 0.016129032258064516, 'getting': 0.016129032258064516, 'percentage': 0.016129032258064516, 'subject': 0.016129032258064516, 'constraint': 0.016129032258064516, 'model': 0.04838709677419355, 'decomposes': 0.016129032258064516, '𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚': 0.03225806451612903, 'NSFM': 0.016129032258064516, 'variable': 0.016129032258064516, 'NSVM': 0.016129032258064516, '𝑅𝑅𝑟𝑟𝑟𝑟𝑟𝑟': 0.016129032258064516, 'shown': 0.016129032258064516, '𝑛𝑛𝑞𝑞': 0.03225806451612903, '𝑝𝑝': 0.03225806451612903, '𝐻𝐻𝑝𝑝𝑝𝑝': 0.03225806451612903, 'relative': 0.016129032258064516, 'relevance': 0.016129032258064516, 'ith': 0.016129032258064516, '𝑊𝑊𝑖𝑖': 0.016129032258064516, '𝑞𝑞': 0.03225806451612903, 'scores': 0.06451612903225806, 'Sentences': 0.06451612903225806, 'maximum': 0.03225806451612903, 'Cosine': 0.03225806451612903, 'Similarity': 0.03225806451612903, '18': 0.016129032258064516, 'widely': 0.016129032258064516, 'vectors': 0.0967741935483871, 'representing': 0.016129032258064516, 'ranging': 0.016129032258064516, 'closer': 0.016129032258064516, 'similar': 0.04838709677419355, 'equation': 0.016129032258064516, 'cos': 0.03225806451612903, 'θ': 0.03225806451612903, 'dot': 0.016129032258064516, 'product': 0.03225806451612903, 'B': 0.0967741935483871, 'lengths': 0.016129032258064516, 'deployed': 0.016129032258064516, 'ni': 0.016129032258064516, 'Ai': 0.016129032258064516, 'Bi': 0.016129032258064516, 'Clustering': 0.016129032258064516, '67': 0.016129032258064516, '55': 0.016129032258064516, 'average': 0.016129032258064516, 'numbers': 0.016129032258064516, 'sets': 0.06451612903225806, '21': 0.04838709677419355, 'respectively': 0.03225806451612903, 'PIPELINE': 0.016129032258064516, 'GENERATING': 0.016129032258064516, 'SUMMARIES': 0.016129032258064516, 'section': 0.016129032258064516, 'demonstrate': 0.016129032258064516, 'pipeline': 0.03225806451612903, 'Figure': 0.06451612903225806, 'generate': 0.016129032258064516, 'Word': 0.03225806451612903, 'S9': 0.016129032258064516, 'Round': 0.08064516129032258, 'Example': 0.016129032258064516, 'S8': 0.016129032258064516, 'Avg': 0.016129032258064516, 'Number': 0.04838709677419355, 'S7': 0.016129032258064516, 'Min': 0.016129032258064516, 'S6': 0.016129032258064516, 'Max': 0.016129032258064516, '58': 0.03225806451612903, 'Dataset': 0.03225806451612903, 'S5': 0.016129032258064516, 'Overall': 0.016129032258064516, 'S4': 0.016129032258064516, 'DATA': 0.016129032258064516, 'PREPARATION': 0.016129032258064516, 'standard': 0.03225806451612903, 'unavailable': 0.016129032258064516, 'evaluating': 0.016129032258064516, '400': 0.04838709677419355, 'Thairath': 0.03225806451612903, 'Manager': 0.03225806451612903, 'online': 0.03225806451612903, 'newspapers': 0.03225806451612903, 'datasets': 0.016129032258064516, 'split': 0.016129032258064516, '80': 0.04838709677419355, 'performance': 0.016129032258064516, 'comparing': 0.016129032258064516, 'summaries': 0.016129032258064516, 'manually': 0.016129032258064516, 'curated': 0.016129032258064516, 'experts': 0.016129032258064516, 'Faculty': 0.016129032258064516, 'Liberal': 0.016129032258064516, 'Arts': 0.016129032258064516, 'Ubon': 0.016129032258064516, 'Ratchathani': 0.016129032258064516, 'open': 0.016129032258064516, 'source': 0.03225806451612903, 'python': 0.04838709677419355, 'libraries': 0.016129032258064516, 'numpy': 0.03225806451612903, '19': 0.016129032258064516, 'sklearn': 0.04838709677419355, 'converted': 0.016129032258064516, 'obtained': 0.016129032258064516, 'plain': 0.016129032258064516, 'segmented': 0.016129032258064516, 'following': 0.03225806451612903, 'format': 0.016129032258064516, 'Si': 0.03225806451612903, 'xxx': 0.03225806451612903, 'removing': 0.016129032258064516, 'stop': 0.016129032258064516, 'duplicate': 0.016129032258064516, 'built': 0.016129032258064516, 'modules': 0.016129032258064516, 'linalg': 0.016129032258064516, 'svd': 0.016129032258064516, 'module': 0.016129032258064516, 'cluster': 0.03225806451612903, 'performed': 0.016129032258064516, 'meant': 0.016129032258064516, '70': 0.016129032258064516, 'S3': 0.016129032258064516, 'S2': 0.016129032258064516, 'grouped': 0.016129032258064516, 'Randomly': 0.016129032258064516, 'groups': 0.016129032258064516, 'Calculate': 0.016129032258064516, 'centroid': 0.06451612903225806, 'vector': 0.016129032258064516, '𝐻𝐻𝑇𝑇': 0.016129032258064516, 'Use': 0.016129032258064516, 'assign': 0.016129032258064516, 'Repeat': 0.016129032258064516, 'assigned': 0.016129032258064516, 'change': 0.016129032258064516, 'Select': 0.016129032258064516, 'add': 0.016129032258064516, 'S1': 0.016129032258064516, 'A2i': 0.016129032258064516, 'Bi2': 0.016129032258064516, 'Mr': 0.016129032258064516, 'Yontas': 0.016129032258064516, 'ak': 0.016129032258064516, 'Supason': 0.016129032258064516, 'Tourism': 0.016129032258064516, 'Authority': 0.016129032258064516, 'demonstrates': 0.016129032258064516, 'constructed': 0.016129032258064516, 'count': 0.016129032258064516, 'composed': 0.016129032258064516, '98': 0.016129032258064516, 'multiple': 0.016129032258064516, 'outperformed': 0.016129032258064516, 'EXPERIMENT': 0.016129032258064516, 'RESULTS': 0.016129032258064516, 'Performance': 0.016129032258064516, 'Evaluations': 0.016129032258064516, 'Measure': 0.016129032258064516, 'accuracy': 0.016129032258064516, 'measurements': 0.016129032258064516, 'quantify': 0.016129032258064516, 'differences': 0.016129032258064516, 'experimental': 0.03225806451612903, 'correctness': 0.016129032258064516, 'reflects': 0.016129032258064516, 'good': 0.016129032258064516, 'missed': 0.016129032258064516, 'Experiment': 0.016129032258064516, 'Results': 0.016129032258064516, 'set': 0.016129032258064516, 'like': 0.03225806451612903, 'explore': 0.016129032258064516, 'different': 0.04838709677419355, 'affected': 0.016129032258064516, 'efficiency': 0.04838709677419355, '83': 0.03225806451612903, '62': 0.016129032258064516, '74': 0.016129032258064516, '64': 0.016129032258064516, '86': 0.016129032258064516, '78': 0.016129032258064516, 'varied': 0.016129032258064516, 'determined': 0.016129032258064516, 'importance': 0.016129032258064516, 'hand': 0.016129032258064516, 'include': 0.016129032258064516, 'CONCLUSIONS': 0.016129032258064516, 's': 0.016129032258064516, 'humans': 0.016129032258064516, 'future': 0.016129032258064516, 'plan': 0.016129032258064516, 'perform': 0.016129032258064516, 'types': 0.016129032258064516, 'handled': 0.016129032258064516, 'ACKNOWLEDGMENTS': 0.016129032258064516, 'thank': 0.016129032258064516, 'department': 0.016129032258064516, 'computer': 0.016129032258064516, 'engineering': 0.03225806451612903, 'faculty': 0.016129032258064516, 'providing': 0.016129032258064516, 'computing': 0.016129032258064516, 'facilities': 0.016129032258064516}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-_T2sn6o1Sa",
        "outputId": "8c54c0b0-6d29-4dff-909c-b218c457b674"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Extractive Text Summarization for Thai Travel News Based on Keyword Scored in Thai Language Sarunya Nathonghor Duangdao Wichadakul Department of Computer Engineering Chulalongkorn University Bangkok Thailand Department of Computer Engineering Chulalongkorn University Bangkok Thailand Sarunya N Student Chula ac th ABSTRACT In recent years people are seeking for a solution to improve text summarization for Thai language Although several solutions such as PageRank Graph Rank Latent Semantic Analysis LSA models etc have been proposed research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and Non negative Matrix Factorization NMF to find the algorithm that is suitable with Thai travel news The suitable compression rates for Generic Sentence Relevance score GRS and K means clustering were also evaluated From these experiments we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News compared with human with the best compression rate of 20 CCS Concepts Information systems Information retrieval Retrieval tasks and goals Summarization Keywords Text summarization extractive summarization non negative matrix factorization 1 INTRODUCTION Daily newspaper has abundant of data that users do not have enough time for reading them It is difficult to identify the relevant information to satisfy the information needed by users Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages However there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar Text Summarization 1 is a technique for summarizing the content of the documents It consists of three steps 1 create an intermediate representation of the input text 2 calculate score for the sentences based on the concepts and 3 choose important Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee Request permissions from Permissions acm org ITCC 2020 August 12 14 2020 Kuala Lumpur Malaysia 2020 Association for Computing Machinery ACM ISBN 978 1 4503 7539 9 20 08 15 00, DOI https doi org 10 1145 3417473 3417479 Duangdao W Chula ac th sentences to be included in the summary Text summarization can be divided into 2 approaches The first approach is the extractive summarization which relies on a method for extracting words and searching for keywords from the original document The second approach is the abstractive summarization which analyzes words by linguistic principles with transcription or interpretation from the original document This approach implies more effective and accurate summary than the extractive methods However with the lack of Thai corpus we chose to apply an extractive summarization method for Thai text summarization This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score GRS calculated from Latent Semantic Analysis LSA and Non negative Matrix, Factorization NMF, We also tried using K means clustering for document summarization In this experiment we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20 30 and 40 and reported the rate and method that produced the best result from the experiment 2 RELATED WORKS In recent years several models in Thai Text summarization have been introduced Suwanno N et al 2 proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns term frequency method and headline score for generating a summary, Chongsuntornsri, A et al 3 proposed a new approach for Text summarization in Thai based on content and graph based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments Jaruskulchai C et al 4 proposed a method to summarize documents by extracting important sentences from combining the specific properties Local Property and the overall properties Global Property of the sentences The overall properties were based on the relationship between sentences in the document From their experiments the summarization of the industrial news got 60 precision 44 recall and 50 9 F measure the general news got the 51 8 precision 38 5 recall and 43 1 F measure while the fashion magazines got 53 0 precision 33 0 recall and 40 4 F measure Mani I et al 5 proposed techniques of text summarization by using word frequency in the document and calculated the weight of word to create a keyword group They then calculated the cosine similarity of sentences The researcher used A search algorithm to find the shortest sequence of sentences from keyword group by topic calculation sentence segmentation and word grouping The sequence of sentences that were in the main group were selected as important sentences Their summarization of the agricultural news got 68 57 precision 51 95 recall and 56 72 F measure Lee J et al 6 proposed a document summarization method using Non negative Matrix Factorization NMF, They compared between Latent Semantic Analysis LSA and NMF to find the weight of each word and calculated the summation of weights The important sentences were ranked and selected into the summary based on their summed weight Based on LSA they found many weights with zero and negative values However when applied NMF they found only the positive values and the scope of the semantic features meaning was narrow Therefore they proposed that NMF provided a greater possibility for extracting important sentences 3 PREPROCESSING FOR THAI TEXT The first step for working with Thai Text is word tokenization Even though Thai writing system has no delimiters to indicate word boundaries together with many rules for word segmentation several Thai word tokenization programs have been proposed Table 1 shows F1 score of the recent programs trained and tested by one of our laboratory members with the data from BEST2010 corpus 7 Cutkum 8 got the highest F1 score hence we used Cutkum for this step Table 1, Comparison of Thai word tokenization programs Tools F1 Score Validate PyICU 9 Article 100 0 6155 Encyclopedia 100 0 6932 News 100 0 5987 Novel 100 0 6800 Lexto 10 0 7267 0, 7709 0, 6994 0, 7701 Cutkum wordcutpy 11 0 9322 0, 6212 0 9299 0, 6286 0 8987 0, 6571 0, 7140 0 6247 cunlp 12 0 6910 0 6172 0, 5748 0 0000 SWATH 13 0, 6347 0, 6858 0 6200 0 6867 3 1 Latent Semantic Analysis Latent Semantic Analysis LSA 14 is the algorithm which reduces the dimensionality of term document The algorithm creates a matrix by using word frequency applies the singular value decomposition SVD 15 and then finds closely related terms and documents The original matrix A can be separated into three matrices where U is the m x r words x extracted concept matrix V is the n x r sentences x extracted concepts matrix and Σ is the r x r diagonal matrix which can be reconstructed to find the original matrix, A The SVD can be represented in Eq 1 3 2 A 𝑈𝑈𝑈𝑈𝑉𝑉 𝑇𝑇 of the related singular value over the sum of all singular values for each concept 3 3 2 A 𝑊𝑊𝑊𝑊 Factors W and H can be found by solving the optimization problem as follows where𝑊𝑊𝑗𝑗𝑗𝑗 0, 𝐻𝐻𝑖𝑖𝑖𝑖 0 𝑚𝑚 𝑛𝑛 𝑟𝑟 𝑗𝑗 1 𝑖𝑖 1 𝑙𝑙 1 2 𝑚𝑚𝑚𝑚𝑚𝑚 𝐹𝐹 𝑊𝑊 𝐻𝐻 𝐴𝐴 𝑊𝑊𝑊𝑊, 2𝐹𝐹 𝐴𝐴𝑖𝑖𝑖𝑖 𝑊𝑊𝑖𝑖𝑖𝑖 𝐻𝐻𝑖𝑖𝑖𝑖 3 NMF and LSA are both matrix factorization algorithms, However when using NMF to find keywords NMF will return the keywords that are closely related because its components have only nonnegative values As LSA has both positive and negative values as well as some zeroes it gets a wider distribution The semantic feature represents a concept of meaning for root of words that have a relationship For example man human male and adult have the same semantic hence their semantic values are close In this paper we applied LSA and NMF on the Thai Travel News dataset for calculating the semantic weights which represented the relationship between sentences and words in order to select the representative sentences for summarization 3 4 Generic document summarization by NMF Lee J et al proposed Eq 4 and Eq 5 to select a number of sentences based on NMF which got the highest semantic weight values where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑗𝑗 Generic Relevance of jth sentence 𝑟𝑟 1 𝐻𝐻𝑖𝑖𝑖𝑖 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 Document summarization using LSA Gong Y et al 16 proposed a document summarization based on SVD matrices In our work after applying SVD to matrix A 𝑉𝑉 𝑇𝑇 matrix used for selecting the important sentences The cell value of the matrix shows the relationship between sentence and extracted concepts A sentence with the highest cell value of each concept will be selected into the summary starting from the most important concept The total number of sentences in the summary will be equal to the number all detected concepts Murray G et al 17 proposed a document summarization based on SVD matrices using 𝑉𝑉 𝑇𝑇 and Σ matrices for sentence selection The authors proposed that more than one sentence could be collected from the more important concepts The decision of how many sentences would be collected from each concept depending on the Σ matrix The value was decided by getting the percentage Non negative Matrix Factorization Non negative Matrix Factorization NMF is a method of matrix factorization subject to the non negative constraint, Lee J et al proposed the model based on NMF for document summarization NMF decomposes a non negative matrix 𝐴𝐴 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚 into two nonnegative matrices The first matrix 𝑚𝑚 x 𝑟𝑟 is a non negative semantic feature matrix NSFM 𝑊𝑊 The second matrix 𝑟𝑟 x 𝑛𝑛 is a nonnegative semantic variable matrix NSVM 𝐻𝐻, So we have 𝑊𝑊 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚 and 𝐻𝐻 𝑅𝑅𝑟𝑟𝑟𝑟𝑟𝑟 and both terms are non negative as shown in Eq 2 and Eq 3 4 𝑖𝑖 1 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 𝑛𝑛𝑞𝑞 1 𝐻𝐻𝑖𝑖𝑖𝑖, 𝑟𝑟 𝑝𝑝 1 𝑛𝑛𝑞𝑞 1 𝐻𝐻𝑝𝑝𝑝𝑝 5 The 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 is the relative relevance of the ith semantic feature 𝑊𝑊𝑖𝑖 where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑞𝑞 and 𝐻𝐻𝑝𝑝𝑝𝑝 is the weight of the topic 𝑝𝑝 in the sentence 𝑞𝑞, The sentences can be ranked by Generic Relevance Sentence scores Sentences with the maximum score will be selected into the summary 3 5 Cosine Similarity Cosine similarity 18 is a widely used method to measure the similarity between vectors representing the documents The result of cosine similarity is ranging from 0 to 1, If it is closer to 1 that means both vectors are similar Eq 6 and Eq 7 represents the cosine similarity equation where cos θ is the dot product between vectors of sentences A and B and divided by the product of the two vectors lengths In this paper we deployed cosine similarity to measure the similarity of sentences in K means clustering A B A B, ni 1 Ai Bi Similarity A B cos θ 7 K means Clustering 15 67 7 7 13 13 55 38 Table 2 shows the overall number of sentences of news within each dataset The average numbers of sentences per news of the 5 sets were 21 16 15 13 and 13 sentences respectively 5 PIPELINE FOR GENERATING SUMMARIES, In this section we demonstrate our pipeline Figure 1 used for text summarization to generate a summary for a Thai travel news Word S9 6 Round 4 Round 5 Table 3, Example of Word by Sentence Matrix, A S8 Round 3 Avg Number of Sentences S7 21 16 Round 1 Round 2 Min Number of Sentences S6 7 7 Max Number of Sentences 58 58 Dataset S5 Table 2 Overall Sentence Language of each Dataset S4 DATA PREPARATION, The standard data sets in Thai language are unavailable for evaluating text summarization system Therefore we collected 400 Thai travel news from Thairath and Manager online newspapers to be used as datasets for our experiments We split 400 travel news into 5 sets of 80 news each We then evaluated the performance of text summarization methods which were LSA and NMF by comparing their results with the summaries manually curated by two experts from the Faculty of Liberal Arts Ubon Ratchathani University The open source python libraries such as numpy 19 and sklearn 20 were used in our system We converted the Thai travel news obtained from Thairath and Manager online newspapers to plain text, Then the sentences of each news were segmented by human with the following format Si xxx where Si represents the order of the sentence in the original document and xxx represents the content of that sentence After removing stop words and duplicate words we built a document term matrix or matrix A then applied SVD and NMF to the matrix Then we used python modules numpy linalg svd to calculate SVD and sklearn decomposition to calculate NMF, For sentence selection we used Gong Y et al and Murray G et al approaches for calculating weight of the sentence scores then selected sentences with the highest scores into the summary For keyword score calculation of NMF we calculated the keyword score from Eq 5 and then selected the sentence with the highest score from each concept The python module sklearn cluster was used for K means clustering The selected sentences from all approaches were in the same order as the original document In this paper we performed the 20 30 and 40 document compression This meant 80 70 and 60 of the sentences will be selected into the summary S3 4 Figure 1 Document summarization pipeline based on LSA and NMF S2 For sentence selection by K means clustering we grouped similar sentences into the same cluster using the following steps 1 Randomly select K sentences as the representative of K groups K in this paper is the number of sentences that will be selected into the summary 2 Calculate centroid of each group by using the value of sentence vector from V matrix for LSA and 𝐻𝐻𝑇𝑇 matrix for NMF 3 Use cosine similarity to calculate sentence similarity between a sentence and the centroid of each group Then assign that sentence to the group with the highest similarity 4 Repeat steps 2 3 until all sentences are assigned to a group no sentences change the group or the similarity between sentences and their centroid is close 5 Select a sentence with the maximum similarity score with the centroid of the group and add it into the summary S1 3 6 A B n n A B i 1 A2i, i 1 Bi2, 6 Mr Yontas ak 1 0 0 0 0 0 0 0 0, Supason 1 0 0 0 0 0 0 0 0, Tourism Authority of Thailand 1 0 0 0 0 0 0 0 0, Table 3 demonstrates an example of a matrix 𝐴𝐴 constructed from word count by sentence of a Thai travel news It was composed of 98 words and 9 sentences This matrix 𝐴𝐴 was then applied with the LSA and, NMF, The sentence vectors were calculated from the term weight and the semantic feature vectors from Eq 1 for LSA and Eq 2 for NMF sentences from all concepts The Generic Sentence Relevance score for NMF also collected one sentence for each concept the same as Gong Y et al, but with the highest score calculated by Eq 5 As multiple important sentences could be selected from a more important concept Murray G et al outperformed both Gong Y et al and the GRS method 6 EXPERIMENT AND RESULTS 6 1 Performance Evaluations Measure 7, We evaluated the results of the summarization by using standard accuracy precision recall and F1 score 21, These measurements quantify the differences between the summary from human and the experimental methods The precision shows the correctness of the extracted sentences and the recall reflects the number of good sentences missed by the method 6 2 Experiment Results In this experimental set we would like to explore how the different sentence selection methods the Generic Sentence Relevance score and K means clustering affected the text summarization result For K means clustering both SVD and NMF had similar summarization efficiency The F1 score of SVD with K means clustering was 0 83 0 72 and 0 62 for the compression rate of 20 30 and 40 For the NMF with K means clustering the F1 score for the three compression rates was 0 83 0 74 and 0 64 For the Generic Sentence Relevance score the best F1 score for the compression rate of 20 30 and 40 was 0 86 0 78 and 0 68 respectively and the best F1 scores for all compression rates were from the approach of Murray G et al Figure 2 Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models 1 NMF with GRS 2 NMF with K means 3 SVD with sentence score by Gong Y et al 4 SVD with K means and 5 SVD with sentence score by Murray G et al applied to 400 Thai travel news divided into 5 sets of 80 news each with the varied compression rates of 20 30 and 40 From this experiment the best model based on keyword score for Thai travel news summarization was SVD with sentence selection by Murray G, et al, This model with the compression rate of 20 got the highest score because Murray G et al method determined the number of sentences to be extracted from each concept based on the importance of that concept The method of Gong Y et al on the other hand was proposed to select only one sentence with the highest score from each concept so that the summary would include CONCLUSIONS In this paper we applied several text summarization methods to Thai Travel News based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and NMF together with different sentence selection methods to find the algorithm suitable with this paper s data source We concluded that keyword scored calculation by LSA with sentence selection by Generic Sentence Relevance score by Murray G et al was the best algorithm while the best compression rate of all models was 20 for summarizing Thai Travel News compared with humans In future work we plan to perform the experiments with different types of documents and improve word segmentation of compound nouns that was not handled by Cutkum 8 ACKNOWLEDGMENTS We would like to thank the department of computer engineering faculty of engineering Chulalongkorn University for providing computing facilities]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]"
      ],
      "metadata": {
        "id": "Dn9RQs5So4Ed"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCimHub2o4jW",
        "outputId": "89742ff1-b133-4995-dd9b-78c69d12f871"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Extractive Text Summarization for Thai Travel News Based on Keyword Scored in Thai Language Sarunya Nathonghor Duangdao Wichadakul Department of Computer Engineering Chulalongkorn University Bangkok Thailand Department of Computer Engineering Chulalongkorn University Bangkok Thailand Sarunya N Student Chula ac th ABSTRACT In recent years people are seeking for a solution to improve text summarization for Thai language Although several solutions such as PageRank Graph Rank Latent Semantic Analysis LSA models etc have been proposed research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and Non negative Matrix Factorization NMF to find the algorithm that is suitable with Thai travel news The suitable compression rates for Generic Sentence Relevance score GRS and K means clustering were also evaluated From these experiments we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News compared with human with the best compression rate of 20 CCS Concepts Information systems Information retrieval Retrieval tasks and goals Summarization Keywords Text summarization extractive summarization non negative matrix factorization 1 INTRODUCTION Daily newspaper has abundant of data that users do not have enough time for reading them It is difficult to identify the relevant information to satisfy the information needed by users Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages However there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar Text Summarization 1 is a technique for summarizing the content of the documents It consists of three steps 1 create an intermediate representation of the input text 2 calculate score for the sentences based on the concepts and 3 choose important Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee Request permissions from Permissions acm org ITCC 2020 August 12 14 2020 Kuala Lumpur Malaysia 2020 Association for Computing Machinery ACM ISBN 978 1 4503 7539 9 20 08 15 00: 27.62903225806448,\n",
              " DOI https doi org 10 1145 3417473 3417479 Duangdao W Chula ac th sentences to be included in the summary Text summarization can be divided into 2 approaches The first approach is the extractive summarization which relies on a method for extracting words and searching for keywords from the original document The second approach is the abstractive summarization which analyzes words by linguistic principles with transcription or interpretation from the original document This approach implies more effective and accurate summary than the extractive methods However with the lack of Thai corpus we chose to apply an extractive summarization method for Thai text summarization This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score GRS calculated from Latent Semantic Analysis LSA and Non negative Matrix: 12.41935483870968,\n",
              " Factorization NMF: 0.04838709677419355,\n",
              " We also tried using K means clustering for document summarization In this experiment we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20 30 and 40 and reported the rate and method that produced the best result from the experiment 2 RELATED WORKS In recent years several models in Thai Text summarization have been introduced Suwanno N et al 2 proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns term frequency method and headline score for generating a summary: 8.951612903225808,\n",
              " A et al 3 proposed a new approach for Text summarization in Thai based on content and graph based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments Jaruskulchai C et al 4 proposed a method to summarize documents by extracting important sentences from combining the specific properties Local Property and the overall properties Global Property of the sentences The overall properties were based on the relationship between sentences in the document From their experiments the summarization of the industrial news got 60 precision 44 recall and 50 9 F measure the general news got the 51 8 precision 38 5 recall and 43 1 F measure while the fashion magazines got 53 0 precision 33 0 recall and 40 4 F measure Mani I et al 5 proposed techniques of text summarization by using word frequency in the document and calculated the weight of word to create a keyword group They then calculated the cosine similarity of sentences The researcher used A search algorithm to find the shortest sequence of sentences from keyword group by topic calculation sentence segmentation and word grouping The sequence of sentences that were in the main group were selected as important sentences Their summarization of the agricultural news got 68 57 precision 51 95 recall and 56 72 F measure Lee J et al 6 proposed a document summarization method using Non negative Matrix Factorization NMF: 26.806451612903242,\n",
              " They compared between Latent Semantic Analysis LSA and NMF to find the weight of each word and calculated the summation of weights The important sentences were ranked and selected into the summary based on their summed weight Based on LSA they found many weights with zero and negative values However when applied NMF they found only the positive values and the scope of the semantic features meaning was narrow Therefore they proposed that NMF provided a greater possibility for extracting important sentences 3 PREPROCESSING FOR THAI TEXT The first step for working with Thai Text is word tokenization Even though Thai writing system has no delimiters to indicate word boundaries together with many rules for word segmentation several Thai word tokenization programs have been proposed Table 1 shows F1 score of the recent programs trained and tested by one of our laboratory members with the data from BEST2010 corpus 7 Cutkum 8 got the highest F1 score hence we used Cutkum for this step Table 1: 10.048387096774198,\n",
              " Comparison of Thai word tokenization programs Tools F1 Score Validate PyICU 9 Article 100 0 6155 Encyclopedia 100 0 6932 News 100 0 5987 Novel 100 0 6800 Lexto 10 0 7267 0: 7.435483870967743,\n",
              " 7709 0: 1.0161290322580645,\n",
              " 6994 0: 1.0161290322580645,\n",
              " 7701 Cutkum wordcutpy 11 0 9322 0: 2.064516129032258,\n",
              " 6212 0 9299 0: 2.032258064516129,\n",
              " 6286 0 8987 0: 2.032258064516129,\n",
              " 6571 0: 1.0161290322580645,\n",
              " 7140 0 6247 cunlp 12 0 6910 0 6172 0: 4.112903225806452,\n",
              " 5748 0 0000 SWATH 13 0: 2.1129032258064515,\n",
              " 6347 0: 1.0161290322580645,\n",
              " 6858 0 6200 0 6867 3 1 Latent Semantic Analysis Latent Semantic Analysis LSA 14 is the algorithm which reduces the dimensionality of term document The algorithm creates a matrix by using word frequency applies the singular value decomposition SVD 15 and then finds closely related terms and documents The original matrix A can be separated into three matrices where U is the m x r words x extracted concept matrix V is the n x r sentences x extracted concepts matrix and Σ is the r x r diagonal matrix which can be reconstructed to find the original matrix: 9.693548387096776,\n",
              " A The SVD can be represented in Eq 1 3 2 A 𝑈𝑈𝑈𝑈𝑉𝑉 𝑇𝑇 of the related singular value over the sum of all singular values for each concept 3 3 2 A 𝑊𝑊𝑊𝑊 Factors W and H can be found by solving the optimization problem as follows where𝑊𝑊𝑗𝑗𝑗𝑗 0: 3.82258064516129,\n",
              " 𝐻𝐻𝑖𝑖𝑖𝑖 0 𝑚𝑚 𝑛𝑛 𝑟𝑟 𝑗𝑗 1 𝑖𝑖 1 𝑙𝑙 1 2 𝑚𝑚𝑚𝑚𝑚𝑚 𝐹𝐹 𝑊𝑊 𝐻𝐻 𝐴𝐴 𝑊𝑊𝑊𝑊: 3.419354838709677,\n",
              " 2𝐹𝐹 𝐴𝐴𝑖𝑖𝑖𝑖 𝑊𝑊𝑖𝑖𝑖𝑖 𝐻𝐻𝑖𝑖𝑖𝑖 3 NMF and LSA are both matrix factorization algorithms: 0.9032258064516128,\n",
              " However when using NMF to find keywords NMF will return the keywords that are closely related because its components have only nonnegative values As LSA has both positive and negative values as well as some zeroes it gets a wider distribution The semantic feature represents a concept of meaning for root of words that have a relationship For example man human male and adult have the same semantic hence their semantic values are close In this paper we applied LSA and NMF on the Thai Travel News dataset for calculating the semantic weights which represented the relationship between sentences and words in order to select the representative sentences for summarization 3 4 Generic document summarization by NMF Lee J et al proposed Eq 4 and Eq 5 to select a number of sentences based on NMF which got the highest semantic weight values where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑗𝑗 Generic Relevance of jth sentence 𝑟𝑟 1 𝐻𝐻𝑖𝑖𝑖𝑖 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 Document summarization using LSA Gong Y et al 16 proposed a document summarization based on SVD matrices In our work after applying SVD to matrix A 𝑉𝑉 𝑇𝑇 matrix used for selecting the important sentences The cell value of the matrix shows the relationship between sentence and extracted concepts A sentence with the highest cell value of each concept will be selected into the summary starting from the most important concept The total number of sentences in the summary will be equal to the number all detected concepts Murray G et al 17 proposed a document summarization based on SVD matrices using 𝑉𝑉 𝑇𝑇 and Σ matrices for sentence selection The authors proposed that more than one sentence could be collected from the more important concepts The decision of how many sentences would be collected from each concept depending on the Σ matrix The value was decided by getting the percentage Non negative Matrix Factorization Non negative Matrix Factorization NMF is a method of matrix factorization subject to the non negative constraint: 29.774193548387107,\n",
              " Lee J et al proposed the model based on NMF for document summarization NMF decomposes a non negative matrix 𝐴𝐴 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚 into two nonnegative matrices The first matrix 𝑚𝑚 x 𝑟𝑟 is a non negative semantic feature matrix NSFM 𝑊𝑊 The second matrix 𝑟𝑟 x 𝑛𝑛 is a nonnegative semantic variable matrix NSVM 𝐻𝐻: 5.967741935483873,\n",
              " So we have 𝑊𝑊 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚 and 𝐻𝐻 𝑅𝑅𝑟𝑟𝑟𝑟𝑟𝑟 and both terms are non negative as shown in Eq 2 and Eq 3 4 𝑖𝑖 1 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 𝑛𝑛𝑞𝑞 1 𝐻𝐻𝑖𝑖𝑖𝑖: 2.516129032258064,\n",
              " 𝑟𝑟 𝑝𝑝 1 𝑛𝑛𝑞𝑞 1 𝐻𝐻𝑝𝑝𝑝𝑝 5 The 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 is the relative relevance of the ith semantic feature 𝑊𝑊𝑖𝑖 where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑞𝑞 and 𝐻𝐻𝑝𝑝𝑝𝑝 is the weight of the topic 𝑝𝑝 in the sentence 𝑞𝑞: 3.629032258064515,\n",
              " The sentences can be ranked by Generic Relevance Sentence scores Sentences with the maximum score will be selected into the summary 3 5 Cosine Similarity Cosine similarity 18 is a widely used method to measure the similarity between vectors representing the documents The result of cosine similarity is ranging from 0 to 1: 6.548387096774194,\n",
              " If it is closer to 1 that means both vectors are similar Eq 6 and Eq 7 represents the cosine similarity equation where cos θ is the dot product between vectors of sentences A and B and divided by the product of the two vectors lengths In this paper we deployed cosine similarity to measure the similarity of sentences in K means clustering A B A B: 4.338709677419354,\n",
              " ni 1 Ai Bi Similarity A B cos θ 7 K means Clustering 15 67 7 7 13 13 55 38 Table 2 shows the overall number of sentences of news within each dataset The average numbers of sentences per news of the 5 sets were 21 16 15 13 and 13 sentences respectively 5 PIPELINE FOR GENERATING SUMMARIES: 6.145161290322582,\n",
              " In this section we demonstrate our pipeline Figure 1 used for text summarization to generate a summary for a Thai travel news Word S9 6 Round 4 Round 5 Table 3: 3.1935483870967745,\n",
              " Example of Word by Sentence Matrix: 1.1451612903225805,\n",
              " A S8 Round 3 Avg Number of Sentences S7 21 16 Round 1 Round 2 Min Number of Sentences S6 7 7 Max Number of Sentences 58 58 Dataset S5 Table 2 Overall Sentence Language of each Dataset S4 DATA PREPARATION: 5.016129032258064,\n",
              " The standard data sets in Thai language are unavailable for evaluating text summarization system Therefore we collected 400 Thai travel news from Thairath and Manager online newspapers to be used as datasets for our experiments We split 400 travel news into 5 sets of 80 news each We then evaluated the performance of text summarization methods which were LSA and NMF by comparing their results with the summaries manually curated by two experts from the Faculty of Liberal Arts Ubon Ratchathani University The open source python libraries such as numpy 19 and sklearn 20 were used in our system We converted the Thai travel news obtained from Thairath and Manager online newspapers to plain text: 5.612903225806456,\n",
              " Then the sentences of each news were segmented by human with the following format Si xxx where Si represents the order of the sentence in the original document and xxx represents the content of that sentence After removing stop words and duplicate words we built a document term matrix or matrix A then applied SVD and NMF to the matrix Then we used python modules numpy linalg svd to calculate SVD and sklearn decomposition to calculate NMF: 5.225806451612906,\n",
              " For sentence selection we used Gong Y et al and Murray G et al approaches for calculating weight of the sentence scores then selected sentences with the highest scores into the summary For keyword score calculation of NMF we calculated the keyword score from Eq 5 and then selected the sentence with the highest score from each concept The python module sklearn cluster was used for K means clustering The selected sentences from all approaches were in the same order as the original document In this paper we performed the 20 30 and 40 document compression This meant 80 70 and 60 of the sentences will be selected into the summary S3 4 Figure 1 Document summarization pipeline based on LSA and NMF S2 For sentence selection by K means clustering we grouped similar sentences into the same cluster using the following steps 1 Randomly select K sentences as the representative of K groups K in this paper is the number of sentences that will be selected into the summary 2 Calculate centroid of each group by using the value of sentence vector from V matrix for LSA and 𝐻𝐻𝑇𝑇 matrix for NMF 3 Use cosine similarity to calculate sentence similarity between a sentence and the centroid of each group Then assign that sentence to the group with the highest similarity 4 Repeat steps 2 3 until all sentences are assigned to a group no sentences change the group or the similarity between sentences and their centroid is close 5 Select a sentence with the maximum similarity score with the centroid of the group and add it into the summary S1 3 6 A B n n A B i 1 A2i: 29.129032258064523,\n",
              " i 1 Bi2: 0.5161290322580645,\n",
              " 6 Mr Yontas ak 1 0 0 0 0 0 0 0 0: 8.661290322580644,\n",
              " Supason 1 0 0 0 0 0 0 0 0: 8.516129032258064,\n",
              " Tourism Authority of Thailand 1 0 0 0 0 0 0 0 0: 8.516129032258064,\n",
              " Table 3 demonstrates an example of a matrix 𝐴𝐴 constructed from word count by sentence of a Thai travel news It was composed of 98 words and 9 sentences This matrix 𝐴𝐴 was then applied with the LSA and: 3.4999999999999996,\n",
              " The sentence vectors were calculated from the term weight and the semantic feature vectors from Eq 1 for LSA and Eq 2 for NMF sentences from all concepts The Generic Sentence Relevance score for NMF also collected one sentence for each concept the same as Gong Y et al: 5.258064516129032,\n",
              " but with the highest score calculated by Eq 5 As multiple important sentences could be selected from a more important concept Murray G et al outperformed both Gong Y et al and the GRS method 6 EXPERIMENT AND RESULTS 6 1 Performance Evaluations Measure 7: 5.080645161290324,\n",
              " We evaluated the results of the summarization by using standard accuracy precision recall and F1 score 21: 1.4193548387096775,\n",
              " These measurements quantify the differences between the summary from human and the experimental methods The precision shows the correctness of the extracted sentences and the recall reflects the number of good sentences missed by the method 6 2 Experiment Results In this experimental set we would like to explore how the different sentence selection methods the Generic Sentence Relevance score and K means clustering affected the text summarization result For K means clustering both SVD and NMF had similar summarization efficiency The F1 score of SVD with K means clustering was 0 83 0 72 and 0 62 for the compression rate of 20 30 and 40 For the NMF with K means clustering the F1 score for the three compression rates was 0 83 0 74 and 0 64 For the Generic Sentence Relevance score the best F1 score for the compression rate of 20 30 and 40 was 0 86 0 78 and 0 68 respectively and the best F1 scores for all compression rates were from the approach of Murray G et al Figure 2 Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models 1 NMF with GRS 2 NMF with K means 3 SVD with sentence score by Gong Y et al 4 SVD with K means and 5 SVD with sentence score by Murray G et al applied to 400 Thai travel news divided into 5 sets of 80 news each with the varied compression rates of 20 30 and 40 From this experiment the best model based on keyword score for Thai travel news summarization was SVD with sentence selection by Murray G: 35.58064516129032,\n",
              " et al: 0.6774193548387096,\n",
              " This model with the compression rate of 20 got the highest score because Murray G et al method determined the number of sentences to be extracted from each concept based on the importance of that concept The method of Gong Y et al on the other hand was proposed to select only one sentence with the highest score from each concept so that the summary would include CONCLUSIONS In this paper we applied several text summarization methods to Thai Travel News based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and NMF together with different sentence selection methods to find the algorithm suitable with this paper s data source We concluded that keyword scored calculation by LSA with sentence selection by Generic Sentence Relevance score by Murray G et al was the best algorithm while the best compression rate of all models was 20 for summarizing Thai Travel News compared with humans In future work we plan to perform the experiments with different types of documents and improve word segmentation of compound nouns that was not handled by Cutkum 8 ACKNOWLEDGMENTS We would like to thank the department of computer engineering faculty of engineering Chulalongkorn University for providing computing facilities: 16.306451612903224}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "ThRxqAyVo6PT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKKlQL4Ro8bU",
        "outputId": "8e9b52ef-8572-4703-a674-69fb58d66342"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)"
      ],
      "metadata": {
        "id": "mq62FJhlpoAN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpUXroatpqDc",
        "outputId": "edc80c81-6649-4e12-eece-b169648ec614"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[These measurements quantify the differences between the summary from human and the experimental methods The precision shows the correctness of the extracted sentences and the recall reflects the number of good sentences missed by the method 6 2 Experiment Results In this experimental set we would like to explore how the different sentence selection methods the Generic Sentence Relevance score and K means clustering affected the text summarization result For K means clustering both SVD and NMF had similar summarization efficiency The F1 score of SVD with K means clustering was 0 83 0 72 and 0 62 for the compression rate of 20 30 and 40 For the NMF with K means clustering the F1 score for the three compression rates was 0 83 0 74 and 0 64 For the Generic Sentence Relevance score the best F1 score for the compression rate of 20 30 and 40 was 0 86 0 78 and 0 68 respectively and the best F1 scores for all compression rates were from the approach of Murray G et al Figure 2 Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models 1 NMF with GRS 2 NMF with K means 3 SVD with sentence score by Gong Y et al 4 SVD with K means and 5 SVD with sentence score by Murray G et al applied to 400 Thai travel news divided into 5 sets of 80 news each with the varied compression rates of 20 30 and 40 From this experiment the best model based on keyword score for Thai travel news summarization was SVD with sentence selection by Murray G,\n",
              " However when using NMF to find keywords NMF will return the keywords that are closely related because its components have only nonnegative values As LSA has both positive and negative values as well as some zeroes it gets a wider distribution The semantic feature represents a concept of meaning for root of words that have a relationship For example man human male and adult have the same semantic hence their semantic values are close In this paper we applied LSA and NMF on the Thai Travel News dataset for calculating the semantic weights which represented the relationship between sentences and words in order to select the representative sentences for summarization 3 4 Generic document summarization by NMF Lee J et al proposed Eq 4 and Eq 5 to select a number of sentences based on NMF which got the highest semantic weight values where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑗𝑗 Generic Relevance of jth sentence 𝑟𝑟 1 𝐻𝐻𝑖𝑖𝑖𝑖 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 Document summarization using LSA Gong Y et al 16 proposed a document summarization based on SVD matrices In our work after applying SVD to matrix A 𝑉𝑉 𝑇𝑇 matrix used for selecting the important sentences The cell value of the matrix shows the relationship between sentence and extracted concepts A sentence with the highest cell value of each concept will be selected into the summary starting from the most important concept The total number of sentences in the summary will be equal to the number all detected concepts Murray G et al 17 proposed a document summarization based on SVD matrices using 𝑉𝑉 𝑇𝑇 and Σ matrices for sentence selection The authors proposed that more than one sentence could be collected from the more important concepts The decision of how many sentences would be collected from each concept depending on the Σ matrix The value was decided by getting the percentage Non negative Matrix Factorization Non negative Matrix Factorization NMF is a method of matrix factorization subject to the non negative constraint,\n",
              " For sentence selection we used Gong Y et al and Murray G et al approaches for calculating weight of the sentence scores then selected sentences with the highest scores into the summary For keyword score calculation of NMF we calculated the keyword score from Eq 5 and then selected the sentence with the highest score from each concept The python module sklearn cluster was used for K means clustering The selected sentences from all approaches were in the same order as the original document In this paper we performed the 20 30 and 40 document compression This meant 80 70 and 60 of the sentences will be selected into the summary S3 4 Figure 1 Document summarization pipeline based on LSA and NMF S2 For sentence selection by K means clustering we grouped similar sentences into the same cluster using the following steps 1 Randomly select K sentences as the representative of K groups K in this paper is the number of sentences that will be selected into the summary 2 Calculate centroid of each group by using the value of sentence vector from V matrix for LSA and 𝐻𝐻𝑇𝑇 matrix for NMF 3 Use cosine similarity to calculate sentence similarity between a sentence and the centroid of each group Then assign that sentence to the group with the highest similarity 4 Repeat steps 2 3 until all sentences are assigned to a group no sentences change the group or the similarity between sentences and their centroid is close 5 Select a sentence with the maximum similarity score with the centroid of the group and add it into the summary S1 3 6 A B n n A B i 1 A2i,\n",
              " Extractive Text Summarization for Thai Travel News Based on Keyword Scored in Thai Language Sarunya Nathonghor Duangdao Wichadakul Department of Computer Engineering Chulalongkorn University Bangkok Thailand Department of Computer Engineering Chulalongkorn University Bangkok Thailand Sarunya N Student Chula ac th ABSTRACT In recent years people are seeking for a solution to improve text summarization for Thai language Although several solutions such as PageRank Graph Rank Latent Semantic Analysis LSA models etc have been proposed research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and Non negative Matrix Factorization NMF to find the algorithm that is suitable with Thai travel news The suitable compression rates for Generic Sentence Relevance score GRS and K means clustering were also evaluated From these experiments we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News compared with human with the best compression rate of 20 CCS Concepts Information systems Information retrieval Retrieval tasks and goals Summarization Keywords Text summarization extractive summarization non negative matrix factorization 1 INTRODUCTION Daily newspaper has abundant of data that users do not have enough time for reading them It is difficult to identify the relevant information to satisfy the information needed by users Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages However there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar Text Summarization 1 is a technique for summarizing the content of the documents It consists of three steps 1 create an intermediate representation of the input text 2 calculate score for the sentences based on the concepts and 3 choose important Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee Request permissions from Permissions acm org ITCC 2020 August 12 14 2020 Kuala Lumpur Malaysia 2020 Association for Computing Machinery ACM ISBN 978 1 4503 7539 9 20 08 15 00,\n",
              " A et al 3 proposed a new approach for Text summarization in Thai based on content and graph based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments Jaruskulchai C et al 4 proposed a method to summarize documents by extracting important sentences from combining the specific properties Local Property and the overall properties Global Property of the sentences The overall properties were based on the relationship between sentences in the document From their experiments the summarization of the industrial news got 60 precision 44 recall and 50 9 F measure the general news got the 51 8 precision 38 5 recall and 43 1 F measure while the fashion magazines got 53 0 precision 33 0 recall and 40 4 F measure Mani I et al 5 proposed techniques of text summarization by using word frequency in the document and calculated the weight of word to create a keyword group They then calculated the cosine similarity of sentences The researcher used A search algorithm to find the shortest sequence of sentences from keyword group by topic calculation sentence segmentation and word grouping The sequence of sentences that were in the main group were selected as important sentences Their summarization of the agricultural news got 68 57 precision 51 95 recall and 56 72 F measure Lee J et al 6 proposed a document summarization method using Non negative Matrix Factorization NMF,\n",
              " This model with the compression rate of 20 got the highest score because Murray G et al method determined the number of sentences to be extracted from each concept based on the importance of that concept The method of Gong Y et al on the other hand was proposed to select only one sentence with the highest score from each concept so that the summary would include CONCLUSIONS In this paper we applied several text summarization methods to Thai Travel News based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and NMF together with different sentence selection methods to find the algorithm suitable with this paper s data source We concluded that keyword scored calculation by LSA with sentence selection by Generic Sentence Relevance score by Murray G et al was the best algorithm while the best compression rate of all models was 20 for summarizing Thai Travel News compared with humans In future work we plan to perform the experiments with different types of documents and improve word segmentation of compound nouns that was not handled by Cutkum 8 ACKNOWLEDGMENTS We would like to thank the department of computer engineering faculty of engineering Chulalongkorn University for providing computing facilities,\n",
              " DOI https doi org 10 1145 3417473 3417479 Duangdao W Chula ac th sentences to be included in the summary Text summarization can be divided into 2 approaches The first approach is the extractive summarization which relies on a method for extracting words and searching for keywords from the original document The second approach is the abstractive summarization which analyzes words by linguistic principles with transcription or interpretation from the original document This approach implies more effective and accurate summary than the extractive methods However with the lack of Thai corpus we chose to apply an extractive summarization method for Thai text summarization This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score GRS calculated from Latent Semantic Analysis LSA and Non negative Matrix,\n",
              " They compared between Latent Semantic Analysis LSA and NMF to find the weight of each word and calculated the summation of weights The important sentences were ranked and selected into the summary based on their summed weight Based on LSA they found many weights with zero and negative values However when applied NMF they found only the positive values and the scope of the semantic features meaning was narrow Therefore they proposed that NMF provided a greater possibility for extracting important sentences 3 PREPROCESSING FOR THAI TEXT The first step for working with Thai Text is word tokenization Even though Thai writing system has no delimiters to indicate word boundaries together with many rules for word segmentation several Thai word tokenization programs have been proposed Table 1 shows F1 score of the recent programs trained and tested by one of our laboratory members with the data from BEST2010 corpus 7 Cutkum 8 got the highest F1 score hence we used Cutkum for this step Table 1,\n",
              " 6858 0 6200 0 6867 3 1 Latent Semantic Analysis Latent Semantic Analysis LSA 14 is the algorithm which reduces the dimensionality of term document The algorithm creates a matrix by using word frequency applies the singular value decomposition SVD 15 and then finds closely related terms and documents The original matrix A can be separated into three matrices where U is the m x r words x extracted concept matrix V is the n x r sentences x extracted concepts matrix and Σ is the r x r diagonal matrix which can be reconstructed to find the original matrix,\n",
              " We also tried using K means clustering for document summarization In this experiment we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20 30 and 40 and reported the rate and method that produced the best result from the experiment 2 RELATED WORKS In recent years several models in Thai Text summarization have been introduced Suwanno N et al 2 proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns term frequency method and headline score for generating a summary,\n",
              " 6 Mr Yontas ak 1 0 0 0 0 0 0 0 0,\n",
              " Supason 1 0 0 0 0 0 0 0 0,\n",
              " Tourism Authority of Thailand 1 0 0 0 0 0 0 0 0]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_summary = [word.text for word in summary]"
      ],
      "metadata": {
        "id": "IUqyL4Ddpr28"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ' '.join(final_summary)"
      ],
      "metadata": {
        "id": "Wy4jaUf3ptqN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceN2d8YopvPW",
        "outputId": "248cf162-6b30-4bd6-a8da-0240d2b9ef7b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive Text Summarization for Thai Travel News Based on Keyword Scored in Thai Language Sarunya Nathonghor Duangdao Wichadakul Department of Computer Engineering Chulalongkorn University Bangkok Thailand Department of Computer Engineering Chulalongkorn University Bangkok Thailand Sarunya N Student Chula ac th ABSTRACT In recent years people are seeking for a solution to improve text summarization for Thai language Although several solutions such as PageRank Graph Rank Latent Semantic Analysis LSA models etc have been proposed research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and Non negative Matrix Factorization NMF to find the algorithm that is suitable with Thai travel news The suitable compression rates for Generic Sentence Relevance score GRS and K means clustering were also evaluated From these experiments we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News compared with human with the best compression rate of 20 CCS Concepts Information systems Information retrieval Retrieval tasks and goals Summarization Keywords Text summarization extractive summarization non negative matrix factorization 1 INTRODUCTION Daily newspaper has abundant of data that users do not have enough time for reading them It is difficult to identify the relevant information to satisfy the information needed by users Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages However there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar Text Summarization 1 is a technique for summarizing the content of the documents It consists of three steps 1 create an intermediate representation of the input text 2 calculate score for the sentences based on the concepts and 3 choose important Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee Request permissions from Permissions acm org ITCC 2020 August 12 14 2020 Kuala Lumpur Malaysia 2020 Association for Computing Machinery ACM ISBN 978 1 4503 7539 9 20 08 15 00 DOI https doi org 10 1145 3417473 3417479 Duangdao W Chula ac th sentences to be included in the summary Text summarization can be divided into 2 approaches The first approach is the extractive summarization which relies on a method for extracting words and searching for keywords from the original document The second approach is the abstractive summarization which analyzes words by linguistic principles with transcription or interpretation from the original document This approach implies more effective and accurate summary than the extractive methods However with the lack of Thai corpus we chose to apply an extractive summarization method for Thai text summarization This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score GRS calculated from Latent Semantic Analysis LSA and Non negative Matrix Factorization NMF We also tried using K means clustering for document summarization In this experiment we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20 30 and 40 and reported the rate and method that produced the best result from the experiment 2 RELATED WORKS In recent years several models in Thai Text summarization have been introduced Suwanno N et al 2 proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns term frequency method and headline score for generating a summary Chongsuntornsri A et al 3 proposed a new approach for Text summarization in Thai based on content and graph based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments Jaruskulchai C et al 4 proposed a method to summarize documents by extracting important sentences from combining the specific properties Local Property and the overall properties Global Property of the sentences The overall properties were based on the relationship between sentences in the document From their experiments the summarization of the industrial news got 60 precision 44 recall and 50 9 F measure the general news got the 51 8 precision 38 5 recall and 43 1 F measure while the fashion magazines got 53 0 precision 33 0 recall and 40 4 F measure Mani I et al 5 proposed techniques of text summarization by using word frequency in the document and calculated the weight of word to create a keyword group They then calculated the cosine similarity of sentences The researcher used A search algorithm to find the shortest sequence of sentences from keyword group by topic calculation sentence segmentation and word grouping The sequence of sentences that were in the main group were selected as important sentences Their summarization of the agricultural news got 68 57 precision 51 95 recall and 56 72 F measure Lee J et al 6 proposed a document summarization method using Non negative Matrix Factorization NMF They compared between Latent Semantic Analysis LSA and NMF to find the weight of each word and calculated the summation of weights The important sentences were ranked and selected into the summary based on their summed weight Based on LSA they found many weights with zero and negative values However when applied NMF they found only the positive values and the scope of the semantic features meaning was narrow Therefore they proposed that NMF provided a greater possibility for extracting important sentences 3 PREPROCESSING FOR THAI TEXT The first step for working with Thai Text is word tokenization Even though Thai writing system has no delimiters to indicate word boundaries together with many rules for word segmentation several Thai word tokenization programs have been proposed Table 1 shows F1 score of the recent programs trained and tested by one of our laboratory members with the data from BEST2010 corpus 7 Cutkum 8 got the highest F1 score hence we used Cutkum for this step Table 1 Comparison of Thai word tokenization programs Tools F1 Score Validate PyICU 9 Article 100 0 6155 Encyclopedia 100 0 6932 News 100 0 5987 Novel 100 0 6800 Lexto 10 0 7267 0 7709 0 6994 0 7701 Cutkum wordcutpy 11 0 9322 0 6212 0 9299 0 6286 0 8987 0 6571 0 7140 0 6247 cunlp 12 0 6910 0 6172 0 5748 0 0000 SWATH 13 0 6347 0 6858 0 6200 0 6867 3 1 Latent Semantic Analysis Latent Semantic Analysis LSA 14 is the algorithm which reduces the dimensionality of term document The algorithm creates a matrix by using word frequency applies the singular value decomposition SVD 15 and then finds closely related terms and documents The original matrix A can be separated into three matrices where U is the m x r words x extracted concept matrix V is the n x r sentences x extracted concepts matrix and Σ is the r x r diagonal matrix which can be reconstructed to find the original matrix A The SVD can be represented in Eq 1 3 2 A 𝑈𝑈𝑈𝑈𝑉𝑉 𝑇𝑇 of the related singular value over the sum of all singular values for each concept 3 3 2 A 𝑊𝑊𝑊𝑊 Factors W and H can be found by solving the optimization problem as follows where𝑊𝑊𝑗𝑗𝑗𝑗 0 𝐻𝐻𝑖𝑖𝑖𝑖 0 𝑚𝑚 𝑛𝑛 𝑟𝑟 𝑗𝑗 1 𝑖𝑖 1 𝑙𝑙 1 2 𝑚𝑚𝑚𝑚𝑚𝑚 𝐹𝐹 𝑊𝑊 𝐻𝐻 𝐴𝐴 𝑊𝑊𝑊𝑊 2𝐹𝐹 𝐴𝐴𝑖𝑖𝑖𝑖 𝑊𝑊𝑖𝑖𝑖𝑖 𝐻𝐻𝑖𝑖𝑖𝑖 3 NMF and LSA are both matrix factorization algorithms However when using NMF to find keywords NMF will return the keywords that are closely related because its components have only nonnegative values As LSA has both positive and negative values as well as some zeroes it gets a wider distribution The semantic feature represents a concept of meaning for root of words that have a relationship For example man human male and adult have the same semantic hence their semantic values are close In this paper we applied LSA and NMF on the Thai Travel News dataset for calculating the semantic weights which represented the relationship between sentences and words in order to select the representative sentences for summarization 3 4 Generic document summarization by NMF Lee J et al proposed Eq 4 and Eq 5 to select a number of sentences based on NMF which got the highest semantic weight values where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑗𝑗 Generic Relevance of jth sentence 𝑟𝑟 1 𝐻𝐻𝑖𝑖𝑖𝑖 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 Document summarization using LSA Gong Y et al 16 proposed a document summarization based on SVD matrices In our work after applying SVD to matrix A 𝑉𝑉 𝑇𝑇 matrix used for selecting the important sentences The cell value of the matrix shows the relationship between sentence and extracted concepts A sentence with the highest cell value of each concept will be selected into the summary starting from the most important concept The total number of sentences in the summary will be equal to the number all detected concepts Murray G et al 17 proposed a document summarization based on SVD matrices using 𝑉𝑉 𝑇𝑇 and Σ matrices for sentence selection The authors proposed that more than one sentence could be collected from the more important concepts The decision of how many sentences would be collected from each concept depending on the Σ matrix The value was decided by getting the percentage Non negative Matrix Factorization Non negative Matrix Factorization NMF is a method of matrix factorization subject to the non negative constraint Lee J et al proposed the model based on NMF for document summarization NMF decomposes a non negative matrix 𝐴𝐴 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚 into two nonnegative matrices The first matrix 𝑚𝑚 x 𝑟𝑟 is a non negative semantic feature matrix NSFM 𝑊𝑊 The second matrix 𝑟𝑟 x 𝑛𝑛 is a nonnegative semantic variable matrix NSVM 𝐻𝐻 So we have 𝑊𝑊 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚 and 𝐻𝐻 𝑅𝑅𝑟𝑟𝑟𝑟𝑟𝑟 and both terms are non negative as shown in Eq 2 and Eq 3 4 𝑖𝑖 1 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 𝑛𝑛𝑞𝑞 1 𝐻𝐻𝑖𝑖𝑖𝑖 𝑟𝑟 𝑝𝑝 1 𝑛𝑛𝑞𝑞 1 𝐻𝐻𝑝𝑝𝑝𝑝 5 The 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 is the relative relevance of the ith semantic feature 𝑊𝑊𝑖𝑖 where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑞𝑞 and 𝐻𝐻𝑝𝑝𝑝𝑝 is the weight of the topic 𝑝𝑝 in the sentence 𝑞𝑞 The sentences can be ranked by Generic Relevance Sentence scores Sentences with the maximum score will be selected into the summary 3 5 Cosine Similarity Cosine similarity 18 is a widely used method to measure the similarity between vectors representing the documents The result of cosine similarity is ranging from 0 to 1 If it is closer to 1 that means both vectors are similar Eq 6 and Eq 7 represents the cosine similarity equation where cos θ is the dot product between vectors of sentences A and B and divided by the product of the two vectors lengths In this paper we deployed cosine similarity to measure the similarity of sentences in K means clustering A B A B ni 1 Ai Bi Similarity A B cos θ 7 K means Clustering 15 67 7 7 13 13 55 38 Table 2 shows the overall number of sentences of news within each dataset The average numbers of sentences per news of the 5 sets were 21 16 15 13 and 13 sentences respectively 5 PIPELINE FOR GENERATING SUMMARIES In this section we demonstrate our pipeline Figure 1 used for text summarization to generate a summary for a Thai travel news Word S9 6 Round 4 Round 5 Table 3 Example of Word by Sentence Matrix A S8 Round 3 Avg Number of Sentences S7 21 16 Round 1 Round 2 Min Number of Sentences S6 7 7 Max Number of Sentences 58 58 Dataset S5 Table 2 Overall Sentence Language of each Dataset S4 DATA PREPARATION The standard data sets in Thai language are unavailable for evaluating text summarization system Therefore we collected 400 Thai travel news from Thairath and Manager online newspapers to be used as datasets for our experiments We split 400 travel news into 5 sets of 80 news each We then evaluated the performance of text summarization methods which were LSA and NMF by comparing their results with the summaries manually curated by two experts from the Faculty of Liberal Arts Ubon Ratchathani University The open source python libraries such as numpy 19 and sklearn 20 were used in our system We converted the Thai travel news obtained from Thairath and Manager online newspapers to plain text Then the sentences of each news were segmented by human with the following format Si xxx where Si represents the order of the sentence in the original document and xxx represents the content of that sentence After removing stop words and duplicate words we built a document term matrix or matrix A then applied SVD and NMF to the matrix Then we used python modules numpy linalg svd to calculate SVD and sklearn decomposition to calculate NMF For sentence selection we used Gong Y et al and Murray G et al approaches for calculating weight of the sentence scores then selected sentences with the highest scores into the summary For keyword score calculation of NMF we calculated the keyword score from Eq 5 and then selected the sentence with the highest score from each concept The python module sklearn cluster was used for K means clustering The selected sentences from all approaches were in the same order as the original document In this paper we performed the 20 30 and 40 document compression This meant 80 70 and 60 of the sentences will be selected into the summary S3 4 Figure 1 Document summarization pipeline based on LSA and NMF S2 For sentence selection by K means clustering we grouped similar sentences into the same cluster using the following steps 1 Randomly select K sentences as the representative of K groups K in this paper is the number of sentences that will be selected into the summary 2 Calculate centroid of each group by using the value of sentence vector from V matrix for LSA and 𝐻𝐻𝑇𝑇 matrix for NMF 3 Use cosine similarity to calculate sentence similarity between a sentence and the centroid of each group Then assign that sentence to the group with the highest similarity 4 Repeat steps 2 3 until all sentences are assigned to a group no sentences change the group or the similarity between sentences and their centroid is close 5 Select a sentence with the maximum similarity score with the centroid of the group and add it into the summary S1 3 6 A B n n A B i 1 A2i i 1 Bi2 6 Mr Yontas ak 1 0 0 0 0 0 0 0 0 Supason 1 0 0 0 0 0 0 0 0 Tourism Authority of Thailand 1 0 0 0 0 0 0 0 0 Table 3 demonstrates an example of a matrix 𝐴𝐴 constructed from word count by sentence of a Thai travel news It was composed of 98 words and 9 sentences This matrix 𝐴𝐴 was then applied with the LSA and NMF The sentence vectors were calculated from the term weight and the semantic feature vectors from Eq 1 for LSA and Eq 2 for NMF sentences from all concepts The Generic Sentence Relevance score for NMF also collected one sentence for each concept the same as Gong Y et al but with the highest score calculated by Eq 5 As multiple important sentences could be selected from a more important concept Murray G et al outperformed both Gong Y et al and the GRS method 6 EXPERIMENT AND RESULTS 6 1 Performance Evaluations Measure 7 We evaluated the results of the summarization by using standard accuracy precision recall and F1 score 21 These measurements quantify the differences between the summary from human and the experimental methods The precision shows the correctness of the extracted sentences and the recall reflects the number of good sentences missed by the method 6 2 Experiment Results In this experimental set we would like to explore how the different sentence selection methods the Generic Sentence Relevance score and K means clustering affected the text summarization result For K means clustering both SVD and NMF had similar summarization efficiency The F1 score of SVD with K means clustering was 0 83 0 72 and 0 62 for the compression rate of 20 30 and 40 For the NMF with K means clustering the F1 score for the three compression rates was 0 83 0 74 and 0 64 For the Generic Sentence Relevance score the best F1 score for the compression rate of 20 30 and 40 was 0 86 0 78 and 0 68 respectively and the best F1 scores for all compression rates were from the approach of Murray G et al Figure 2 Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models 1 NMF with GRS 2 NMF with K means 3 SVD with sentence score by Gong Y et al 4 SVD with K means and 5 SVD with sentence score by Murray G et al applied to 400 Thai travel news divided into 5 sets of 80 news each with the varied compression rates of 20 30 and 40 From this experiment the best model based on keyword score for Thai travel news summarization was SVD with sentence selection by Murray G et al This model with the compression rate of 20 got the highest score because Murray G et al method determined the number of sentences to be extracted from each concept based on the importance of that concept The method of Gong Y et al on the other hand was proposed to select only one sentence with the highest score from each concept so that the summary would include CONCLUSIONS In this paper we applied several text summarization methods to Thai Travel News based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and NMF together with different sentence selection methods to find the algorithm suitable with this paper s data source We concluded that keyword scored calculation by LSA with sentence selection by Generic Sentence Relevance score by Murray G et al was the best algorithm while the best compression rate of all models was 20 for summarizing Thai Travel News compared with humans In future work we plan to perform the experiments with different types of documents and improve word segmentation of compound nouns that was not handled by Cutkum 8 ACKNOWLEDGMENTS We would like to thank the department of computer engineering faculty of engineering Chulalongkorn University for providing computing facilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qei3mvKpxkY",
        "outputId": "d63e9e50-7034-43ba-db23-6f054f77b059"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These measurements quantify the differences between the summary from human and the experimental methods The precision shows the correctness of the extracted sentences and the recall reflects the number of good sentences missed by the method 6 2 Experiment Results In this experimental set we would like to explore how the different sentence selection methods the Generic Sentence Relevance score and K means clustering affected the text summarization result For K means clustering both SVD and NMF had similar summarization efficiency The F1 score of SVD with K means clustering was 0 83 0 72 and 0 62 for the compression rate of 20 30 and 40 For the NMF with K means clustering the F1 score for the three compression rates was 0 83 0 74 and 0 64 For the Generic Sentence Relevance score the best F1 score for the compression rate of 20 30 and 40 was 0 86 0 78 and 0 68 respectively and the best F1 scores for all compression rates were from the approach of Murray G et al Figure 2 Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models 1 NMF with GRS 2 NMF with K means 3 SVD with sentence score by Gong Y et al 4 SVD with K means and 5 SVD with sentence score by Murray G et al applied to 400 Thai travel news divided into 5 sets of 80 news each with the varied compression rates of 20 30 and 40 From this experiment the best model based on keyword score for Thai travel news summarization was SVD with sentence selection by Murray G However when using NMF to find keywords NMF will return the keywords that are closely related because its components have only nonnegative values As LSA has both positive and negative values as well as some zeroes it gets a wider distribution The semantic feature represents a concept of meaning for root of words that have a relationship For example man human male and adult have the same semantic hence their semantic values are close In this paper we applied LSA and NMF on the Thai Travel News dataset for calculating the semantic weights which represented the relationship between sentences and words in order to select the representative sentences for summarization 3 4 Generic document summarization by NMF Lee J et al proposed Eq 4 and Eq 5 to select a number of sentences based on NMF which got the highest semantic weight values where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑗𝑗 Generic Relevance of jth sentence 𝑟𝑟 1 𝐻𝐻𝑖𝑖𝑖𝑖 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡 𝐻𝐻𝑖𝑖 Document summarization using LSA Gong Y et al 16 proposed a document summarization based on SVD matrices In our work after applying SVD to matrix A 𝑉𝑉 𝑇𝑇 matrix used for selecting the important sentences The cell value of the matrix shows the relationship between sentence and extracted concepts A sentence with the highest cell value of each concept will be selected into the summary starting from the most important concept The total number of sentences in the summary will be equal to the number all detected concepts Murray G et al 17 proposed a document summarization based on SVD matrices using 𝑉𝑉 𝑇𝑇 and Σ matrices for sentence selection The authors proposed that more than one sentence could be collected from the more important concepts The decision of how many sentences would be collected from each concept depending on the Σ matrix The value was decided by getting the percentage Non negative Matrix Factorization Non negative Matrix Factorization NMF is a method of matrix factorization subject to the non negative constraint For sentence selection we used Gong Y et al and Murray G et al approaches for calculating weight of the sentence scores then selected sentences with the highest scores into the summary For keyword score calculation of NMF we calculated the keyword score from Eq 5 and then selected the sentence with the highest score from each concept The python module sklearn cluster was used for K means clustering The selected sentences from all approaches were in the same order as the original document In this paper we performed the 20 30 and 40 document compression This meant 80 70 and 60 of the sentences will be selected into the summary S3 4 Figure 1 Document summarization pipeline based on LSA and NMF S2 For sentence selection by K means clustering we grouped similar sentences into the same cluster using the following steps 1 Randomly select K sentences as the representative of K groups K in this paper is the number of sentences that will be selected into the summary 2 Calculate centroid of each group by using the value of sentence vector from V matrix for LSA and 𝐻𝐻𝑇𝑇 matrix for NMF 3 Use cosine similarity to calculate sentence similarity between a sentence and the centroid of each group Then assign that sentence to the group with the highest similarity 4 Repeat steps 2 3 until all sentences are assigned to a group no sentences change the group or the similarity between sentences and their centroid is close 5 Select a sentence with the maximum similarity score with the centroid of the group and add it into the summary S1 3 6 A B n n A B i 1 A2i Extractive Text Summarization for Thai Travel News Based on Keyword Scored in Thai Language Sarunya Nathonghor Duangdao Wichadakul Department of Computer Engineering Chulalongkorn University Bangkok Thailand Department of Computer Engineering Chulalongkorn University Bangkok Thailand Sarunya N Student Chula ac th ABSTRACT In recent years people are seeking for a solution to improve text summarization for Thai language Although several solutions such as PageRank Graph Rank Latent Semantic Analysis LSA models etc have been proposed research results in Thai text summarization were restricted due to limited corpus in Thai language with complex grammar This paper applied a text summarization system for Thai travel news based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and Non negative Matrix Factorization NMF to find the algorithm that is suitable with Thai travel news The suitable compression rates for Generic Sentence Relevance score GRS and K means clustering were also evaluated From these experiments we concluded that keyword scored calculation by LSA with sentence selection by GRS is the best algorithm for summarizing Thai Travel News compared with human with the best compression rate of 20 CCS Concepts Information systems Information retrieval Retrieval tasks and goals Summarization Keywords Text summarization extractive summarization non negative matrix factorization 1 INTRODUCTION Daily newspaper has abundant of data that users do not have enough time for reading them It is difficult to identify the relevant information to satisfy the information needed by users Automatic summarization can reduce the problem of information overloading and it has been proposed previously in English and other languages However there were only a few research results in Thai text summarization due to the lack of corpus in Thai language and the complicated grammar Text Summarization 1 is a technique for summarizing the content of the documents It consists of three steps 1 create an intermediate representation of the input text 2 calculate score for the sentences based on the concepts and 3 choose important Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee Request permissions from Permissions acm org ITCC 2020 August 12 14 2020 Kuala Lumpur Malaysia 2020 Association for Computing Machinery ACM ISBN 978 1 4503 7539 9 20 08 15 00 A et al 3 proposed a new approach for Text summarization in Thai based on content and graph based with the use of Topic Sensitive PageRank algorithm for summarizing and ranking of text segments Jaruskulchai C et al 4 proposed a method to summarize documents by extracting important sentences from combining the specific properties Local Property and the overall properties Global Property of the sentences The overall properties were based on the relationship between sentences in the document From their experiments the summarization of the industrial news got 60 precision 44 recall and 50 9 F measure the general news got the 51 8 precision 38 5 recall and 43 1 F measure while the fashion magazines got 53 0 precision 33 0 recall and 40 4 F measure Mani I et al 5 proposed techniques of text summarization by using word frequency in the document and calculated the weight of word to create a keyword group They then calculated the cosine similarity of sentences The researcher used A search algorithm to find the shortest sequence of sentences from keyword group by topic calculation sentence segmentation and word grouping The sequence of sentences that were in the main group were selected as important sentences Their summarization of the agricultural news got 68 57 precision 51 95 recall and 56 72 F measure Lee J et al 6 proposed a document summarization method using Non negative Matrix Factorization NMF This model with the compression rate of 20 got the highest score because Murray G et al method determined the number of sentences to be extracted from each concept based on the importance of that concept The method of Gong Y et al on the other hand was proposed to select only one sentence with the highest score from each concept so that the summary would include CONCLUSIONS In this paper we applied several text summarization methods to Thai Travel News based on keyword scored in Thai language by extracting the most relevant sentences from the original document We compared LSA and NMF together with different sentence selection methods to find the algorithm suitable with this paper s data source We concluded that keyword scored calculation by LSA with sentence selection by Generic Sentence Relevance score by Murray G et al was the best algorithm while the best compression rate of all models was 20 for summarizing Thai Travel News compared with humans In future work we plan to perform the experiments with different types of documents and improve word segmentation of compound nouns that was not handled by Cutkum 8 ACKNOWLEDGMENTS We would like to thank the department of computer engineering faculty of engineering Chulalongkorn University for providing computing facilities DOI https doi org 10 1145 3417473 3417479 Duangdao W Chula ac th sentences to be included in the summary Text summarization can be divided into 2 approaches The first approach is the extractive summarization which relies on a method for extracting words and searching for keywords from the original document The second approach is the abstractive summarization which analyzes words by linguistic principles with transcription or interpretation from the original document This approach implies more effective and accurate summary than the extractive methods However with the lack of Thai corpus we chose to apply an extractive summarization method for Thai text summarization This research focused on the sentence extraction function based on keyword score calculation then selecting important sentences based on the Generic Sentence Relevance score GRS calculated from Latent Semantic Analysis LSA and Non negative Matrix They compared between Latent Semantic Analysis LSA and NMF to find the weight of each word and calculated the summation of weights The important sentences were ranked and selected into the summary based on their summed weight Based on LSA they found many weights with zero and negative values However when applied NMF they found only the positive values and the scope of the semantic features meaning was narrow Therefore they proposed that NMF provided a greater possibility for extracting important sentences 3 PREPROCESSING FOR THAI TEXT The first step for working with Thai Text is word tokenization Even though Thai writing system has no delimiters to indicate word boundaries together with many rules for word segmentation several Thai word tokenization programs have been proposed Table 1 shows F1 score of the recent programs trained and tested by one of our laboratory members with the data from BEST2010 corpus 7 Cutkum 8 got the highest F1 score hence we used Cutkum for this step Table 1 6858 0 6200 0 6867 3 1 Latent Semantic Analysis Latent Semantic Analysis LSA 14 is the algorithm which reduces the dimensionality of term document The algorithm creates a matrix by using word frequency applies the singular value decomposition SVD 15 and then finds closely related terms and documents The original matrix A can be separated into three matrices where U is the m x r words x extracted concept matrix V is the n x r sentences x extracted concepts matrix and Σ is the r x r diagonal matrix which can be reconstructed to find the original matrix We also tried using K means clustering for document summarization In this experiment we compared 5 models for 5 rounds with Thai travel news using the compression rates of 20 30 and 40 and reported the rate and method that produced the best result from the experiment 2 RELATED WORKS In recent years several models in Thai Text summarization have been introduced Suwanno N et al 2 proposed a Thai text summarization that extracted a paragraph from a document based on Thai compound nouns term frequency method and headline score for generating a summary 6 Mr Yontas ak 1 0 0 0 0 0 0 0 0 Supason 1 0 0 0 0 0 0 0 0 Tourism Authority of Thailand 1 0 0 0 0 0 0 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write summary to file\n",
        "with open('/content/summarize_result.txt', 'w') as file:\n",
        "    file.write(summary)"
      ],
      "metadata": {
        "id": "AInX8aC9pzfy"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}