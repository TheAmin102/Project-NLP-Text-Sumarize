Extractive
Text
Summarization
for
Thai
Travel
News
Based
on
Keyword
Scored
in
Thai
Language
Sarunya
Nathonghor
Duangdao
Wichadakul
Department
of
Computer
Engineering
Chulalongkorn
University
Bangkok
Thailand
Department
of
Computer
Engineering
Chulalongkorn
University
Bangkok
Thailand
Sarunya
N
Student
Chula
ac
th
ABSTRACT
In
recent
years
people
are
seeking
for
a
solution
to
improve
text
summarization
for
Thai
language
Although
several
solutions
such
as
PageRank
Graph
Rank
Latent
Semantic
Analysis
LSA
models
etc
have
been
proposed
research
results
in
Thai
text
summarization
were
restricted
due
to
limited
corpus
in
Thai
language
with
complex
grammar
This
paper
applied
a
text
summarization
system
for
Thai
travel
news
based
on
keyword
scored
in
Thai
language
by
extracting
the
most
relevant
sentences
from
the
original
document
We
compared
LSA
and
Non
negative
Matrix
Factorization
NMF
to
find
the
algorithm
that
is
suitable
with
Thai
travel
news
The
suitable
compression
rates
for
Generic
Sentence
Relevance
score
GRS
and
K
means
clustering
were
also
evaluated
From
these
experiments
we
concluded
that
keyword
scored
calculation
by
LSA
with
sentence
selection
by
GRS
is
the
best
algorithm
for
summarizing
Thai
Travel
News
compared
with
human
with
the
best
compression
rate
of
20
CCS
Concepts
Information
systems
Information
retrieval
Retrieval
tasks
and
goals
Summarization
Keywords
Text
summarization
extractive
summarization
non
negative
matrix
factorization
1
INTRODUCTION
Daily
newspaper
has
abundant
of
data
that
users
do
not
have
enough
time
for
reading
them
It
is
difficult
to
identify
the
relevant
information
to
satisfy
the
information
needed
by
users
Automatic
summarization
can
reduce
the
problem
of
information
overloading
and
it
has
been
proposed
previously
in
English
and
other
languages
However
there
were
only
a
few
research
results
in
Thai
text
summarization
due
to
the
lack
of
corpus
in
Thai
language
and
the
complicated
grammar
Text
Summarization
1
is
a
technique
for
summarizing
the
content
of
the
documents
It
consists
of
three
steps
1
create
an
intermediate
representation
of
the
input
text
2
calculate
score
for
the
sentences
based
on
the
concepts
and
3
choose
important
Permission
to
make
digital
or
hard
copies
of
all
or
part
of
this
work
for
personal
or
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
for
profit
or
commercial
advantage
and
that
copies
bear
this
notice
and
the
full
citation
on
the
first
page
Copyrights
for
components
of
this
work
owned
by
others
than
ACM
must
be
honored
Abstracting
with
credit
is
permitted
To
copy
otherwise
or
republish
to
post
on
servers
or
to
redistribute
to
lists
requires
prior
specific
permission
and
or
a
fee
Request
permissions
from
Permissions
acm
org
ITCC
2020
August
12
14
2020
Kuala
Lumpur
Malaysia
2020
Association
for
Computing
Machinery
ACM
ISBN
978
1
4503
7539
9
20
08
15
00
DOI
https
doi
org
10
1145
3417473
3417479
Duangdao
W
Chula
ac
th
sentences
to
be
included
in
the
summary
Text
summarization
can
be
divided
into
2
approaches
The
first
approach
is
the
extractive
summarization
which
relies
on
a
method
for
extracting
words
and
searching
for
keywords
from
the
original
document
The
second
approach
is
the
abstractive
summarization
which
analyzes
words
by
linguistic
principles
with
transcription
or
interpretation
from
the
original
document
This
approach
implies
more
effective
and
accurate
summary
than
the
extractive
methods
However
with
the
lack
of
Thai
corpus
we
chose
to
apply
an
extractive
summarization
method
for
Thai
text
summarization
This
research
focused
on
the
sentence
extraction
function
based
on
keyword
score
calculation
then
selecting
important
sentences
based
on
the
Generic
Sentence
Relevance
score
GRS
calculated
from
Latent
Semantic
Analysis
LSA
and
Non
negative
Matrix
Factorization
NMF
We
also
tried
using
K
means
clustering
for
document
summarization
In
this
experiment
we
compared
5
models
for
5
rounds
with
Thai
travel
news
using
the
compression
rates
of
20
30
and
40
and
reported
the
rate
and
method
that
produced
the
best
result
from
the
experiment
2
RELATED
WORKS
In
recent
years
several
models
in
Thai
Text
summarization
have
been
introduced
Suwanno
N
et
al
2
proposed
a
Thai
text
summarization
that
extracted
a
paragraph
from
a
document
based
on
Thai
compound
nouns
term
frequency
method
and
headline
score
for
generating
a
summary
Chongsuntornsri
A
et
al
3
proposed
a
new
approach
for
Text
summarization
in
Thai
based
on
content
and
graph
based
with
the
use
of
Topic
Sensitive
PageRank
algorithm
for
summarizing
and
ranking
of
text
segments
Jaruskulchai
C
et
al
4
proposed
a
method
to
summarize
documents
by
extracting
important
sentences
from
combining
the
specific
properties
Local
Property
and
the
overall
properties
Global
Property
of
the
sentences
The
overall
properties
were
based
on
the
relationship
between
sentences
in
the
document
From
their
experiments
the
summarization
of
the
industrial
news
got
60
precision
44
recall
and
50
9
F
measure
the
general
news
got
the
51
8
precision
38
5
recall
and
43
1
F
measure
while
the
fashion
magazines
got
53
0
precision
33
0
recall
and
40
4
F
measure
Mani
I
et
al
5
proposed
techniques
of
text
summarization
by
using
word
frequency
in
the
document
and
calculated
the
weight
of
word
to
create
a
keyword
group
They
then
calculated
the
cosine
similarity
of
sentences
The
researcher
used
A
search
algorithm
to
find
the
shortest
sequence
of
sentences
from
keyword
group
by
topic
calculation
sentence
segmentation
and
word
grouping
The
sequence
of
sentences
that
were
in
the
main
group
were
selected
as
important
sentences
Their
summarization
of
the
agricultural
news
got
68
57
precision
51
95
recall
and
56
72
F
measure
Lee
J
et
al
6
proposed
a
document
summarization
method
using
Non
negative
Matrix
Factorization
NMF
They
compared
between
Latent
Semantic
Analysis
LSA
and
NMF
to
find
the
weight
of
each
word
and
calculated
the
summation
of
weights
The
important
sentences
were
ranked
and
selected
into
the
summary
based
on
their
summed
weight
Based
on
LSA
they
found
many
weights
with
zero
and
negative
values
However
when
applied
NMF
they
found
only
the
positive
values
and
the
scope
of
the
semantic
features
meaning
was
narrow
Therefore
they
proposed
that
NMF
provided
a
greater
possibility
for
extracting
important
sentences
3
PREPROCESSING
FOR
THAI
TEXT
The
first
step
for
working
with
Thai
Text
is
word
tokenization
Even
though
Thai
writing
system
has
no
delimiters
to
indicate
word
boundaries
together
with
many
rules
for
word
segmentation
several
Thai
word
tokenization
programs
have
been
proposed
Table
1
shows
F1
score
of
the
recent
programs
trained
and
tested
by
one
of
our
laboratory
members
with
the
data
from
BEST2010
corpus
7
Cutkum
8
got
the
highest
F1
score
hence
we
used
Cutkum
for
this
step
Table
1
Comparison
of
Thai
word
tokenization
programs
Tools
F1
Score
Validate
PyICU
9
Article
100
0
6155
Encyclopedia
100
0
6932
News
100
0
5987
Novel
100
0
6800
Lexto
10
0
7267
0
7709
0
6994
0
7701
Cutkum
wordcutpy
11
0
9322
0
6212
0
9299
0
6286
0
8987
0
6571
0
7140
0
6247
cunlp
12
0
6910
0
6172
0
5748
0
0000
SWATH
13
0
6347
0
6858
0
6200
0
6867
3
1
Latent
Semantic
Analysis
Latent
Semantic
Analysis
LSA
14
is
the
algorithm
which
reduces
the
dimensionality
of
term
document
The
algorithm
creates
a
matrix
by
using
word
frequency
applies
the
singular
value
decomposition
SVD
15
and
then
finds
closely
related
terms
and
documents
The
original
matrix
A
can
be
separated
into
three
matrices
where
U
is
the
m
x
r
words
x
extracted
concept
matrix
V
is
the
n
x
r
sentences
x
extracted
concepts
matrix
and
Î£
is
the
r
x
r
diagonal
matrix
which
can
be
reconstructed
to
find
the
original
matrix
A
The
SVD
can
be
represented
in
Eq
1
3
2
A
ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰
ğ‘‡ğ‘‡
of
the
related
singular
value
over
the
sum
of
all
singular
values
for
each
concept
3
3
2
A
ğ‘Šğ‘Šğ‘Šğ‘Š
Factors
W
and
H
can
be
found
by
solving
the
optimization
problem
as
follows
whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘—
0
ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–
0
ğ‘šğ‘š
ğ‘›ğ‘›
ğ‘Ÿğ‘Ÿ
ğ‘—ğ‘—
1
ğ‘–ğ‘–
1
ğ‘™ğ‘™
1
2
ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š
ğ¹ğ¹
ğ‘Šğ‘Š
ğ»ğ»
ğ´ğ´
ğ‘Šğ‘Šğ‘Šğ‘Š
2ğ¹ğ¹
ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘–
ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘–
ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–
3
NMF
and
LSA
are
both
matrix
factorization
algorithms
However
when
using
NMF
to
find
keywords
NMF
will
return
the
keywords
that
are
closely
related
because
its
components
have
only
nonnegative
values
As
LSA
has
both
positive
and
negative
values
as
well
as
some
zeroes
it
gets
a
wider
distribution
The
semantic
feature
represents
a
concept
of
meaning
for
root
of
words
that
have
a
relationship
For
example
man
human
male
and
adult
have
the
same
semantic
hence
their
semantic
values
are
close
In
this
paper
we
applied
LSA
and
NMF
on
the
Thai
Travel
News
dataset
for
calculating
the
semantic
weights
which
represented
the
relationship
between
sentences
and
words
in
order
to
select
the
representative
sentences
for
summarization
3
4
Generic
document
summarization
by
NMF
Lee
J
et
al
proposed
Eq
4
and
Eq
5
to
select
a
number
of
sentences
based
on
NMF
which
got
the
highest
semantic
weight
values
where
ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–
is
the
weight
of
the
topic
ğ‘–ğ‘–
in
the
sentence
ğ‘—ğ‘—
Generic
Relevance
of
jth
sentence
ğ‘Ÿğ‘Ÿ
1
ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–
ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡
ğ»ğ»ğ‘–ğ‘–
Document
summarization
using
LSA
Gong
Y
et
al
16
proposed
a
document
summarization
based
on
SVD
matrices
In
our
work
after
applying
SVD
to
matrix
A
ğ‘‰ğ‘‰
ğ‘‡ğ‘‡
matrix
used
for
selecting
the
important
sentences
The
cell
value
of
the
matrix
shows
the
relationship
between
sentence
and
extracted
concepts
A
sentence
with
the
highest
cell
value
of
each
concept
will
be
selected
into
the
summary
starting
from
the
most
important
concept
The
total
number
of
sentences
in
the
summary
will
be
equal
to
the
number
all
detected
concepts
Murray
G
et
al
17
proposed
a
document
summarization
based
on
SVD
matrices
using
ğ‘‰ğ‘‰
ğ‘‡ğ‘‡
and
Î£
matrices
for
sentence
selection
The
authors
proposed
that
more
than
one
sentence
could
be
collected
from
the
more
important
concepts
The
decision
of
how
many
sentences
would
be
collected
from
each
concept
depending
on
the
Î£
matrix
The
value
was
decided
by
getting
the
percentage
Non
negative
Matrix
Factorization
Non
negative
Matrix
Factorization
NMF
is
a
method
of
matrix
factorization
subject
to
the
non
negative
constraint
Lee
J
et
al
proposed
the
model
based
on
NMF
for
document
summarization
NMF
decomposes
a
non
negative
matrix
ğ´ğ´
ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š
into
two
nonnegative
matrices
The
first
matrix
ğ‘šğ‘š
x
ğ‘Ÿğ‘Ÿ
is
a
non
negative
semantic
feature
matrix
NSFM
ğ‘Šğ‘Š
The
second
matrix
ğ‘Ÿğ‘Ÿ
x
ğ‘›ğ‘›
is
a
nonnegative
semantic
variable
matrix
NSVM
ğ»ğ»
So
we
have
ğ‘Šğ‘Š
ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š
and
ğ»ğ»
ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ
and
both
terms
are
non
negative
as
shown
in
Eq
2
and
Eq
3
4
ğ‘–ğ‘–
1
ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡
ğ»ğ»ğ‘–ğ‘–
ğ‘›ğ‘›ğ‘ğ‘
1
ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–
ğ‘Ÿğ‘Ÿ
ğ‘ğ‘
1
ğ‘›ğ‘›ğ‘ğ‘
1
ğ»ğ»ğ‘ğ‘ğ‘ğ‘
5
The
ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡
ğ»ğ»ğ‘–ğ‘–
is
the
relative
relevance
of
the
ith
semantic
feature
ğ‘Šğ‘Šğ‘–ğ‘–
where
ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–
is
the
weight
of
the
topic
ğ‘–ğ‘–
in
the
sentence
ğ‘ğ‘
and
ğ»ğ»ğ‘ğ‘ğ‘ğ‘
is
the
weight
of
the
topic
ğ‘ğ‘
in
the
sentence
ğ‘ğ‘
The
sentences
can
be
ranked
by
Generic
Relevance
Sentence
scores
Sentences
with
the
maximum
score
will
be
selected
into
the
summary
3
5
Cosine
Similarity
Cosine
similarity
18
is
a
widely
used
method
to
measure
the
similarity
between
vectors
representing
the
documents
The
result
of
cosine
similarity
is
ranging
from
0
to
1
If
it
is
closer
to
1
that
means
both
vectors
are
similar
Eq
6
and
Eq
7
represents
the
cosine
similarity
equation
where
cos
Î¸
is
the
dot
product
between
vectors
of
sentences
A
and
B
and
divided
by
the
product
of
the
two
vectors
lengths
In
this
paper
we
deployed
cosine
similarity
to
measure
the
similarity
of
sentences
in
K
means
clustering
A
B
A
B
ni
1
Ai
Bi
Similarity
A
B
cos
Î¸
7
K
means
Clustering
15
67
7
7
13
13
55
38
Table
2
shows
the
overall
number
of
sentences
of
news
within
each
dataset
The
average
numbers
of
sentences
per
news
of
the
5
sets
were
21
16
15
13
and
13
sentences
respectively
5
PIPELINE
FOR
GENERATING
SUMMARIES
In
this
section
we
demonstrate
our
pipeline
Figure
1
used
for
text
summarization
to
generate
a
summary
for
a
Thai
travel
news
Word
S9
6
Round
4
Round
5
Table
3
Example
of
Word
by
Sentence
Matrix
A
S8
Round
3
Avg
Number
of
Sentences
S7
21
16
Round
1
Round
2
Min
Number
of
Sentences
S6
7
7
Max
Number
of
Sentences
58
58
Dataset
S5
Table
2
Overall
Sentence
Language
of
each
Dataset
S4
DATA
PREPARATION
The
standard
data
sets
in
Thai
language
are
unavailable
for
evaluating
text
summarization
system
Therefore
we
collected
400
Thai
travel
news
from
Thairath
and
Manager
online
newspapers
to
be
used
as
datasets
for
our
experiments
We
split
400
travel
news
into
5
sets
of
80
news
each
We
then
evaluated
the
performance
of
text
summarization
methods
which
were
LSA
and
NMF
by
comparing
their
results
with
the
summaries
manually
curated
by
two
experts
from
the
Faculty
of
Liberal
Arts
Ubon
Ratchathani
University
The
open
source
python
libraries
such
as
numpy
19
and
sklearn
20
were
used
in
our
system
We
converted
the
Thai
travel
news
obtained
from
Thairath
and
Manager
online
newspapers
to
plain
text
Then
the
sentences
of
each
news
were
segmented
by
human
with
the
following
format
Si
xxx
where
Si
represents
the
order
of
the
sentence
in
the
original
document
and
xxx
represents
the
content
of
that
sentence
After
removing
stop
words
and
duplicate
words
we
built
a
document
term
matrix
or
matrix
A
then
applied
SVD
and
NMF
to
the
matrix
Then
we
used
python
modules
numpy
linalg
svd
to
calculate
SVD
and
sklearn
decomposition
to
calculate
NMF
For
sentence
selection
we
used
Gong
Y
et
al
and
Murray
G
et
al
approaches
for
calculating
weight
of
the
sentence
scores
then
selected
sentences
with
the
highest
scores
into
the
summary
For
keyword
score
calculation
of
NMF
we
calculated
the
keyword
score
from
Eq
5
and
then
selected
the
sentence
with
the
highest
score
from
each
concept
The
python
module
sklearn
cluster
was
used
for
K
means
clustering
The
selected
sentences
from
all
approaches
were
in
the
same
order
as
the
original
document
In
this
paper
we
performed
the
20
30
and
40
document
compression
This
meant
80
70
and
60
of
the
sentences
will
be
selected
into
the
summary
S3
4
Figure
1
Document
summarization
pipeline
based
on
LSA
and
NMF
S2
For
sentence
selection
by
K
means
clustering
we
grouped
similar
sentences
into
the
same
cluster
using
the
following
steps
1
Randomly
select
K
sentences
as
the
representative
of
K
groups
K
in
this
paper
is
the
number
of
sentences
that
will
be
selected
into
the
summary
2
Calculate
centroid
of
each
group
by
using
the
value
of
sentence
vector
from
V
matrix
for
LSA
and
ğ»ğ»ğ‘‡ğ‘‡
matrix
for
NMF
3
Use
cosine
similarity
to
calculate
sentence
similarity
between
a
sentence
and
the
centroid
of
each
group
Then
assign
that
sentence
to
the
group
with
the
highest
similarity
4
Repeat
steps
2
3
until
all
sentences
are
assigned
to
a
group
no
sentences
change
the
group
or
the
similarity
between
sentences
and
their
centroid
is
close
5
Select
a
sentence
with
the
maximum
similarity
score
with
the
centroid
of
the
group
and
add
it
into
the
summary
S1
3
6
A
B
n
n
A
B
i
1
A2i
i
1
Bi2
6
Mr
Yontas
ak
1
0
0
0
0
0
0
0
0
Supason
1
0
0
0
0
0
0
0
0
Tourism
Authority
of
Thailand
1
0
0
0
0
0
0
0
0
Table
3
demonstrates
an
example
of
a
matrix
ğ´ğ´
constructed
from
word
count
by
sentence
of
a
Thai
travel
news
It
was
composed
of
98
words
and
9
sentences
This
matrix
ğ´ğ´
was
then
applied
with
the
LSA
and
NMF
The
sentence
vectors
were
calculated
from
the
term
weight
and
the
semantic
feature
vectors
from
Eq
1
for
LSA
and
Eq
2
for
NMF
sentences
from
all
concepts
The
Generic
Sentence
Relevance
score
for
NMF
also
collected
one
sentence
for
each
concept
the
same
as
Gong
Y
et
al
but
with
the
highest
score
calculated
by
Eq
5
As
multiple
important
sentences
could
be
selected
from
a
more
important
concept
Murray
G
et
al
outperformed
both
Gong
Y
et
al
and
the
GRS
method
6
EXPERIMENT
AND
RESULTS
6
1
Performance
Evaluations
Measure
7
We
evaluated
the
results
of
the
summarization
by
using
standard
accuracy
precision
recall
and
F1
score
21
These
measurements
quantify
the
differences
between
the
summary
from
human
and
the
experimental
methods
The
precision
shows
the
correctness
of
the
extracted
sentences
and
the
recall
reflects
the
number
of
good
sentences
missed
by
the
method
6
2
Experiment
Results
In
this
experimental
set
we
would
like
to
explore
how
the
different
sentence
selection
methods
the
Generic
Sentence
Relevance
score
and
K
means
clustering
affected
the
text
summarization
result
For
K
means
clustering
both
SVD
and
NMF
had
similar
summarization
efficiency
The
F1
score
of
SVD
with
K
means
clustering
was
0
83
0
72
and
0
62
for
the
compression
rate
of
20
30
and
40
For
the
NMF
with
K
means
clustering
the
F1
score
for
the
three
compression
rates
was
0
83
0
74
and
0
64
For
the
Generic
Sentence
Relevance
score
the
best
F1
score
for
the
compression
rate
of
20
30
and
40
was
0
86
0
78
and
0
68
respectively
and
the
best
F1
scores
for
all
compression
rates
were
from
the
approach
of
Murray
G
et
al
Figure
2
Thai
text
summarization
efficiency
of
5
models
Figure
2
shows
the
Thai
text
summarization
efficiency
of
5
models
1
NMF
with
GRS
2
NMF
with
K
means
3
SVD
with
sentence
score
by
Gong
Y
et
al
4
SVD
with
K
means
and
5
SVD
with
sentence
score
by
Murray
G
et
al
applied
to
400
Thai
travel
news
divided
into
5
sets
of
80
news
each
with
the
varied
compression
rates
of
20
30
and
40
From
this
experiment
the
best
model
based
on
keyword
score
for
Thai
travel
news
summarization
was
SVD
with
sentence
selection
by
Murray
G
et
al
This
model
with
the
compression
rate
of
20
got
the
highest
score
because
Murray
G
et
al
method
determined
the
number
of
sentences
to
be
extracted
from
each
concept
based
on
the
importance
of
that
concept
The
method
of
Gong
Y
et
al
on
the
other
hand
was
proposed
to
select
only
one
sentence
with
the
highest
score
from
each
concept
so
that
the
summary
would
include
CONCLUSIONS
In
this
paper
we
applied
several
text
summarization
methods
to
Thai
Travel
News
based
on
keyword
scored
in
Thai
language
by
extracting
the
most
relevant
sentences
from
the
original
document
We
compared
LSA
and
NMF
together
with
different
sentence
selection
methods
to
find
the
algorithm
suitable
with
this
paper
s
data
source
We
concluded
that
keyword
scored
calculation
by
LSA
with
sentence
selection
by
Generic
Sentence
Relevance
score
by
Murray
G
et
al
was
the
best
algorithm
while
the
best
compression
rate
of
all
models
was
20
for
summarizing
Thai
Travel
News
compared
with
humans
In
future
work
we
plan
to
perform
the
experiments
with
different
types
of
documents
and
improve
word
segmentation
of
compound
nouns
that
was
not
handled
by
Cutkum
8
ACKNOWLEDGMENTS
We
would
like
to
thank
the
department
of
computer
engineering
faculty
of
engineering
Chulalongkorn
University
for
providing
computing
facilities
