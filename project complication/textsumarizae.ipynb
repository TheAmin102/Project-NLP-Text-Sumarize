{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C46oF36Bg0V",
        "outputId": "2d6daa44-62ee-40b2-b87a-d2399a1b74d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install tensorflow\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuhz3v4zwIcn"
      },
      "outputs": [],
      "source": [
        "with open('paperThailand.txt') as f:\n",
        "    lines = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95urUhWR2rh7"
      },
      "outputs": [],
      "source": [
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaJzkGHn2uIN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4fFmkkg2wCF"
      },
      "outputs": [],
      "source": [
        "stopwords = list(STOP_WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOXYtmbn2x9x"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# For some OS you need to make a correctino installing the package separately using this command:\n",
        "# !pip3 install -U spacy\n",
        "# !python3 -m spacy download en_core_web_sm\n",
        "# run both command above to install a separate package from the spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehejsUuR2zfs"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KttOAXos272b",
        "outputId": "530e4132-cbab-40cb-a2f8-ade43b4f5ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', 'ABSTRACT', '\\n', 'In', 'recent', 'years', ',', 'people', 'are', 'seeking', 'for', 'a', 'solution', 'to', 'improve', 'text', '\\n', 'summarization', 'for', 'Thai', 'language', '.', 'Although', 'several', 'solutions', 'such', '\\n', 'as', 'PageRank', ',', 'Graph', 'Rank', ',', 'Latent', 'Semantic', 'Analysis', '(', 'LSA', ')', '\\n', 'models', ',', 'etc', '.', ',', 'have', 'been', 'proposed', ',', 'research', 'results', 'in', 'Thai', 'text', '\\n', 'summarization', 'were', 'restricted', 'due', 'to', 'limited', 'corpus', 'in', 'Thai', '\\n', 'language', 'with', 'complex', 'grammar', '.', 'This', 'paper', 'applied', 'a', 'text', '\\n', 'summarization', 'system', 'for', 'Thai', 'travel', 'news', 'based', 'on', 'keyword', '\\n', 'scored', 'in', 'Thai', 'language', 'by', 'extracting', 'the', 'most', 'relevant', 'sentences', '\\n', 'from', 'the', 'original', 'document', '.', 'We', 'compared', 'LSA', 'and', 'Non', '-', 'negative', '\\n', 'Matrix', 'Factorization', '(', 'NMF', ')', 'to', 'find', 'the', 'algorithm', 'that', 'is', 'suitable', '\\n', 'with', 'Thai', 'travel', 'news', '.', 'The', 'suitable', 'compression', 'rates', 'for', 'Generic', '\\n', 'Sentence', 'Relevance', 'score', '(', 'GRS', ')', 'and', 'K', '-', 'means', 'clustering', 'were', 'also', '\\n', 'evaluated', '.', 'From', 'these', 'experiments', ',', 'we', 'concluded', 'that', 'keyword', '\\n', 'scored', 'calculation', 'by', 'LSA', 'with', 'sentence', 'selection', 'by', 'GRS', 'is', 'the', '\\n', 'best', 'algorithm', 'for', 'summarizing', 'Thai', 'Travel', 'News', ',', 'compared', 'with', '\\n', 'human', 'with', 'the', 'best', 'compression', 'rate', 'of', '20', '%', '.', '\\n', 'CCS', 'Concepts', '\\n', '•', 'Information', 'systems', '➝', 'Information', 'retrieval', '➝', 'Retrieval', '\\n', 'tasks', 'and', 'goals', '➝', 'Summarization', '\\n', 'Keywords', '\\n', 'Text', 'summarization', ';', 'extractive', 'summarization', ';', 'non', '-', 'negative', '\\n', 'matrix', 'factorization', '\\n', '1', '.', 'INTRODUCTION', '\\n', 'Daily', 'newspaper', 'has', 'abundant', 'of', 'data', 'that', 'users', 'do', 'not', 'have', '\\n', 'enough', 'time', 'for', 'reading', 'them', '.', 'It', 'is', 'difficult', 'to', 'identify', 'the', 'relevant', '\\n', 'information', 'to', 'satisfy', 'the', 'information', 'needed', 'by', 'users', '.', 'Automatic', '\\n', 'summarization', 'can', 'reduce', 'the', 'problem', 'of', 'information', 'overloading', '\\n', 'and', 'it', 'has', 'been', 'proposed', 'previously', 'in', 'English', 'and', 'other', 'languages', '.', '\\n', 'However', ',', 'there', 'were', 'only', 'a', 'few', 'research', 'results', 'in', 'Thai', 'text', '\\n', 'summarization', 'due', 'to', 'the', 'lack', 'of', 'corpus', 'in', 'Thai', 'language', 'and', 'the', '\\n', 'complicated', 'grammar', '.', '\\n', 'Text', 'Summarization', '[', '1', ']', 'is', 'a', 'technique', 'for', 'summarizing', 'the', 'content', '\\n', 'of', 'the', 'documents', '.', 'It', 'consists', 'of', 'three', 'steps', ':', '1', ')', 'create', 'an', '\\n', 'intermediate', 'representation', 'of', 'the', 'input', 'text', ',', '2', ')', 'calculate', 'score', 'for', '\\n', 'the', 'sentences', 'based', 'on', 'the', 'concepts', ',', 'and', '3', ')', 'choose', 'important', '\\n', 'sentences', 'to', 'be', 'included', 'in', 'the', 'summary', '.', 'Text', 'summarization', 'can', '\\n', 'be', 'divided', 'into', '2', 'approaches', '.', 'The', 'first', 'approach', 'is', 'the', 'extractive', '\\n', 'summarization', ',', 'which', 'relies', 'on', 'a', 'method', 'for', 'extracting', 'words', 'and', '\\n', 'searching', 'for', 'keywords', 'from', 'the', 'original', 'document', '.', 'The', 'second', '\\n', 'approach', 'is', 'the', 'abstractive', 'summarization', ',', 'which', 'analyzes', 'words', '\\n', 'by', 'linguistic', 'principles', 'with', 'transcription', 'or', 'interpretation', 'from', 'the', '\\n', 'original', 'document', '.', 'This', 'approach', 'implies', 'more', 'effective', 'and', '\\n', 'accurate', 'summary', 'than', 'the', 'extractive', 'methods', '.', 'However', ',', 'with', 'the', '\\n', 'lack', 'of', 'Thai', 'corpus', ',', 'we', 'chose', 'to', 'apply', 'an', 'extractive', 'summarization', '\\n', 'method', 'for', 'Thai', 'text', 'summarization', '.', '\\n', 'This', 'research', 'focused', 'on', 'the', 'sentence', 'extraction', 'function', 'based', 'on', '\\n', 'keyword', 'score', 'calculation', 'then', 'selecting', 'important', 'sentences', 'based', '\\n', 'on', 'the', 'Generic', 'Sentence', 'Relevance', 'score', '(', 'GRS', ')', ',', 'calculated', 'from', '\\n', 'Latent', 'Semantic', 'Analysis', '(', 'LSA', ')', 'and', 'Non', '-', 'negative', 'Matrix', '\\n', 'Factorization', '(', 'NMF', ')', '.', 'We', 'also', 'tried', 'using', 'K', '-', 'means', 'clustering', 'for', '\\n', 'document', 'summarization', '.', 'In', 'this', 'experiment', ',', 'we', 'compared', '5', '\\n', 'models', 'for', '5', 'rounds', 'with', 'Thai', 'travel', 'news', 'using', 'the', 'compression', '\\n', 'rates', 'of', '20', '%', ',', '30', '%', 'and', '40', '%', 'and', 'reported', 'the', 'rate', 'and', 'method', 'that', '\\n', 'produced', 'the', 'best', 'result', 'from', 'the', 'experiment', '.', '\\n', '2', '.', 'RELATED', 'WORKS', '\\n', 'In', 'recent', 'years', ',', 'several', 'models', 'in', 'Thai', 'Text', 'summarization', 'have', '\\n', 'been', 'introduced', '.', 'Suwanno', ',', 'N.', 'et', 'al', '.', '[', '2', ']', 'proposed', 'a', 'Thai', 'text', '\\n', 'summarization', 'that', 'extracted', 'a', 'paragraph', 'from', 'a', 'document', 'based', '\\n', 'on', 'Thai', 'compound', 'nouns', ',', 'term', 'frequency', 'method', ',', 'and', 'headline', '\\n', 'score', 'for', 'generating', 'a', 'summary', '.', 'Chongsuntornsri', ',', 'A.', ',', 'et', 'al', '.', '[', '3', ']', '\\n', 'proposed', 'a', 'new', 'approach', 'for', 'Text', 'summarization', 'in', 'Thai', 'based', 'on', '\\n', 'content-', 'and', 'graph', '-', 'based', 'with', 'the', 'use', 'of', 'Topic', 'Sensitive', 'PageRank', '\\n', 'algorithm', 'for', 'summarizing', 'and', 'ranking', 'of', 'text', 'segments', '.', '\\n', 'Jaruskulchai', 'C.', ',', 'et', 'al', '.', '[', '4', ']', 'proposed', 'a', 'method', 'to', 'summarize', '\\n', 'documents', 'by', 'extracting', 'important', 'sentences', 'from', 'combining', 'the', '\\n', 'specific', 'properties', '(', 'Local', 'Property', ')', 'and', 'the', 'overall', 'properties', '\\n', '(', 'Global', 'Property', ')', 'of', 'the', 'sentences', '.', 'The', 'overall', 'properties', 'were', '\\n', 'based', 'on', 'the', 'relationship', 'between', 'sentences', 'in', 'the', 'document', '.', 'From', '\\n', 'their', 'experiments', ',', 'the', 'summarization', 'of', 'the', 'industrial', 'news', 'got', '\\n', '60', '%', 'precision', ',', '44', '%', 'recall', ',', 'and', '50.9', '%', 'F', '-', 'measure', ',', 'the', 'general', 'news', '\\n', 'got', 'the', '51.8', '%', 'precision', ',', '38.5', '%', 'recall', ',', 'and', '43.1', '%', 'F', '-', 'measure', 'while', '\\n', 'the', 'fashion', 'magazines', 'got', '53.0', '%', 'precision', ',', '33.0', '%', 'recall', ',', 'and', '\\n', '40.4', '%', 'F', '-', 'measure', '.', '\\n']\n"
          ]
        }
      ],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2y6WfuOJ28OQ",
        "outputId": "5fb27a6e-5eaf-4a31-fc58-909d649624f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNk7VDiI29-s"
      },
      "outputs": [],
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOh7L2ZD3AFD",
        "outputId": "6983aa8c-6b67-4333-da58-98e09ab69dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ABSTRACT': 1, 'recent': 2, 'years': 2, 'people': 1, 'seeking': 1, 'solution': 1, 'improve': 1, 'text': 8, 'summarization': 17, 'Thai': 16, 'language': 4, 'solutions': 1, 'PageRank': 2, 'Graph': 1, 'Rank': 1, 'Latent': 2, 'Semantic': 2, 'Analysis': 2, 'LSA': 4, 'models': 3, 'etc': 1, 'proposed': 5, 'research': 3, 'results': 2, 'restricted': 1, 'limited': 1, 'corpus': 3, 'complex': 1, 'grammar': 2, 'paper': 1, 'applied': 1, 'system': 1, 'travel': 3, 'news': 5, 'based': 8, 'keyword': 3, 'scored': 2, 'extracting': 3, 'relevant': 2, 'sentences': 7, 'original': 3, 'document': 6, 'compared': 3, 'Non': 2, 'negative': 3, 'Matrix': 2, 'Factorization': 2, 'NMF': 2, 'find': 1, 'algorithm': 3, 'suitable': 2, 'compression': 3, 'rates': 2, 'Generic': 2, 'Sentence': 2, 'Relevance': 2, 'score': 5, 'GRS': 3, 'K': 2, 'means': 2, 'clustering': 2, 'evaluated': 1, 'experiments': 2, 'concluded': 1, 'calculation': 2, 'sentence': 2, 'selection': 1, 'best': 3, 'summarizing': 3, 'Travel': 1, 'News': 1, 'human': 1, 'rate': 2, '20': 2, 'CCS': 1, 'Concepts': 1, '•': 1, 'Information': 2, 'systems': 1, '➝': 3, 'retrieval': 1, 'Retrieval': 1, 'tasks': 1, 'goals': 1, 'Summarization': 2, 'Keywords': 1, 'Text': 5, 'extractive': 4, 'non': 1, 'matrix': 1, 'factorization': 1, '1': 3, 'INTRODUCTION': 1, 'Daily': 1, 'newspaper': 1, 'abundant': 1, 'data': 1, 'users': 2, 'time': 1, 'reading': 1, 'difficult': 1, 'identify': 1, 'information': 3, 'satisfy': 1, 'needed': 1, 'Automatic': 1, 'reduce': 1, 'problem': 1, 'overloading': 1, 'previously': 1, 'English': 1, 'languages': 1, 'lack': 2, 'complicated': 1, 'technique': 1, 'content': 1, 'documents': 2, 'consists': 1, 'steps': 1, 'create': 1, 'intermediate': 1, 'representation': 1, 'input': 1, '2': 4, 'calculate': 1, 'concepts': 1, '3': 2, 'choose': 1, 'important': 3, 'included': 1, 'summary': 3, 'divided': 1, 'approaches': 1, 'approach': 4, 'relies': 1, 'method': 5, 'words': 2, 'searching': 1, 'keywords': 1, 'second': 1, 'abstractive': 1, 'analyzes': 1, 'linguistic': 1, 'principles': 1, 'transcription': 1, 'interpretation': 1, 'implies': 1, 'effective': 1, 'accurate': 1, 'methods': 1, 'chose': 1, 'apply': 1, 'focused': 1, 'extraction': 1, 'function': 1, 'selecting': 1, 'calculated': 1, 'tried': 1, 'experiment': 2, '5': 2, 'rounds': 1, '30': 1, '40': 1, 'reported': 1, 'produced': 1, 'result': 1, 'RELATED': 1, 'WORKS': 1, 'introduced': 1, 'Suwanno': 1, 'N.': 1, 'et': 3, 'al': 3, 'extracted': 1, 'paragraph': 1, 'compound': 1, 'nouns': 1, 'term': 1, 'frequency': 1, 'headline': 1, 'generating': 1, 'Chongsuntornsri': 1, 'A.': 1, 'new': 1, 'content-': 1, 'graph': 1, 'use': 1, 'Topic': 1, 'Sensitive': 1, 'ranking': 1, 'segments': 1, 'Jaruskulchai': 1, 'C.': 1, '4': 1, 'summarize': 1, 'combining': 1, 'specific': 1, 'properties': 3, 'Local': 1, 'Property': 2, 'overall': 2, 'Global': 1, 'relationship': 1, 'industrial': 1, 'got': 3, '60': 1, 'precision': 3, '44': 1, 'recall': 3, '50.9': 1, 'F': 3, 'measure': 3, 'general': 1, '51.8': 1, '38.5': 1, '43.1': 1, 'fashion': 1, 'magazines': 1, '53.0': 1, '33.0': 1, '40.4': 1}\n"
          ]
        }
      ],
      "source": [
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbRUtgdR3BuR"
      },
      "outputs": [],
      "source": [
        "max_frequency = max(word_frequencies.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hF7k2wP3DYR",
        "outputId": "aa809d5b-9d4f-41a0-f512-4448bd607c1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4q0KDAr3GAC"
      },
      "outputs": [],
      "source": [
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlBB0xdJ3INI",
        "outputId": "4f536368-21f2-4b9b-fbc3-3fbeed5c8e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ABSTRACT': 0.058823529411764705, 'recent': 0.11764705882352941, 'years': 0.11764705882352941, 'people': 0.058823529411764705, 'seeking': 0.058823529411764705, 'solution': 0.058823529411764705, 'improve': 0.058823529411764705, 'text': 0.47058823529411764, 'summarization': 1.0, 'Thai': 0.9411764705882353, 'language': 0.23529411764705882, 'solutions': 0.058823529411764705, 'PageRank': 0.11764705882352941, 'Graph': 0.058823529411764705, 'Rank': 0.058823529411764705, 'Latent': 0.11764705882352941, 'Semantic': 0.11764705882352941, 'Analysis': 0.11764705882352941, 'LSA': 0.23529411764705882, 'models': 0.17647058823529413, 'etc': 0.058823529411764705, 'proposed': 0.29411764705882354, 'research': 0.17647058823529413, 'results': 0.11764705882352941, 'restricted': 0.058823529411764705, 'limited': 0.058823529411764705, 'corpus': 0.17647058823529413, 'complex': 0.058823529411764705, 'grammar': 0.11764705882352941, 'paper': 0.058823529411764705, 'applied': 0.058823529411764705, 'system': 0.058823529411764705, 'travel': 0.17647058823529413, 'news': 0.29411764705882354, 'based': 0.47058823529411764, 'keyword': 0.17647058823529413, 'scored': 0.11764705882352941, 'extracting': 0.17647058823529413, 'relevant': 0.11764705882352941, 'sentences': 0.4117647058823529, 'original': 0.17647058823529413, 'document': 0.35294117647058826, 'compared': 0.17647058823529413, 'Non': 0.11764705882352941, 'negative': 0.17647058823529413, 'Matrix': 0.11764705882352941, 'Factorization': 0.11764705882352941, 'NMF': 0.11764705882352941, 'find': 0.058823529411764705, 'algorithm': 0.17647058823529413, 'suitable': 0.11764705882352941, 'compression': 0.17647058823529413, 'rates': 0.11764705882352941, 'Generic': 0.11764705882352941, 'Sentence': 0.11764705882352941, 'Relevance': 0.11764705882352941, 'score': 0.29411764705882354, 'GRS': 0.17647058823529413, 'K': 0.11764705882352941, 'means': 0.11764705882352941, 'clustering': 0.11764705882352941, 'evaluated': 0.058823529411764705, 'experiments': 0.11764705882352941, 'concluded': 0.058823529411764705, 'calculation': 0.11764705882352941, 'sentence': 0.11764705882352941, 'selection': 0.058823529411764705, 'best': 0.17647058823529413, 'summarizing': 0.17647058823529413, 'Travel': 0.058823529411764705, 'News': 0.058823529411764705, 'human': 0.058823529411764705, 'rate': 0.11764705882352941, '20': 0.11764705882352941, 'CCS': 0.058823529411764705, 'Concepts': 0.058823529411764705, '•': 0.058823529411764705, 'Information': 0.11764705882352941, 'systems': 0.058823529411764705, '➝': 0.17647058823529413, 'retrieval': 0.058823529411764705, 'Retrieval': 0.058823529411764705, 'tasks': 0.058823529411764705, 'goals': 0.058823529411764705, 'Summarization': 0.11764705882352941, 'Keywords': 0.058823529411764705, 'Text': 0.29411764705882354, 'extractive': 0.23529411764705882, 'non': 0.058823529411764705, 'matrix': 0.058823529411764705, 'factorization': 0.058823529411764705, '1': 0.17647058823529413, 'INTRODUCTION': 0.058823529411764705, 'Daily': 0.058823529411764705, 'newspaper': 0.058823529411764705, 'abundant': 0.058823529411764705, 'data': 0.058823529411764705, 'users': 0.11764705882352941, 'time': 0.058823529411764705, 'reading': 0.058823529411764705, 'difficult': 0.058823529411764705, 'identify': 0.058823529411764705, 'information': 0.17647058823529413, 'satisfy': 0.058823529411764705, 'needed': 0.058823529411764705, 'Automatic': 0.058823529411764705, 'reduce': 0.058823529411764705, 'problem': 0.058823529411764705, 'overloading': 0.058823529411764705, 'previously': 0.058823529411764705, 'English': 0.058823529411764705, 'languages': 0.058823529411764705, 'lack': 0.11764705882352941, 'complicated': 0.058823529411764705, 'technique': 0.058823529411764705, 'content': 0.058823529411764705, 'documents': 0.11764705882352941, 'consists': 0.058823529411764705, 'steps': 0.058823529411764705, 'create': 0.058823529411764705, 'intermediate': 0.058823529411764705, 'representation': 0.058823529411764705, 'input': 0.058823529411764705, '2': 0.23529411764705882, 'calculate': 0.058823529411764705, 'concepts': 0.058823529411764705, '3': 0.11764705882352941, 'choose': 0.058823529411764705, 'important': 0.17647058823529413, 'included': 0.058823529411764705, 'summary': 0.17647058823529413, 'divided': 0.058823529411764705, 'approaches': 0.058823529411764705, 'approach': 0.23529411764705882, 'relies': 0.058823529411764705, 'method': 0.29411764705882354, 'words': 0.11764705882352941, 'searching': 0.058823529411764705, 'keywords': 0.058823529411764705, 'second': 0.058823529411764705, 'abstractive': 0.058823529411764705, 'analyzes': 0.058823529411764705, 'linguistic': 0.058823529411764705, 'principles': 0.058823529411764705, 'transcription': 0.058823529411764705, 'interpretation': 0.058823529411764705, 'implies': 0.058823529411764705, 'effective': 0.058823529411764705, 'accurate': 0.058823529411764705, 'methods': 0.058823529411764705, 'chose': 0.058823529411764705, 'apply': 0.058823529411764705, 'focused': 0.058823529411764705, 'extraction': 0.058823529411764705, 'function': 0.058823529411764705, 'selecting': 0.058823529411764705, 'calculated': 0.058823529411764705, 'tried': 0.058823529411764705, 'experiment': 0.11764705882352941, '5': 0.11764705882352941, 'rounds': 0.058823529411764705, '30': 0.058823529411764705, '40': 0.058823529411764705, 'reported': 0.058823529411764705, 'produced': 0.058823529411764705, 'result': 0.058823529411764705, 'RELATED': 0.058823529411764705, 'WORKS': 0.058823529411764705, 'introduced': 0.058823529411764705, 'Suwanno': 0.058823529411764705, 'N.': 0.058823529411764705, 'et': 0.17647058823529413, 'al': 0.17647058823529413, 'extracted': 0.058823529411764705, 'paragraph': 0.058823529411764705, 'compound': 0.058823529411764705, 'nouns': 0.058823529411764705, 'term': 0.058823529411764705, 'frequency': 0.058823529411764705, 'headline': 0.058823529411764705, 'generating': 0.058823529411764705, 'Chongsuntornsri': 0.058823529411764705, 'A.': 0.058823529411764705, 'new': 0.058823529411764705, 'content-': 0.058823529411764705, 'graph': 0.058823529411764705, 'use': 0.058823529411764705, 'Topic': 0.058823529411764705, 'Sensitive': 0.058823529411764705, 'ranking': 0.058823529411764705, 'segments': 0.058823529411764705, 'Jaruskulchai': 0.058823529411764705, 'C.': 0.058823529411764705, '4': 0.058823529411764705, 'summarize': 0.058823529411764705, 'combining': 0.058823529411764705, 'specific': 0.058823529411764705, 'properties': 0.17647058823529413, 'Local': 0.058823529411764705, 'Property': 0.11764705882352941, 'overall': 0.11764705882352941, 'Global': 0.058823529411764705, 'relationship': 0.058823529411764705, 'industrial': 0.058823529411764705, 'got': 0.17647058823529413, '60': 0.058823529411764705, 'precision': 0.17647058823529413, '44': 0.058823529411764705, 'recall': 0.17647058823529413, '50.9': 0.058823529411764705, 'F': 0.17647058823529413, 'measure': 0.17647058823529413, 'general': 0.058823529411764705, '51.8': 0.058823529411764705, '38.5': 0.058823529411764705, '43.1': 0.058823529411764705, 'fashion': 0.058823529411764705, 'magazines': 0.058823529411764705, '53.0': 0.058823529411764705, '33.0': 0.058823529411764705, '40.4': 0.058823529411764705}\n"
          ]
        }
      ],
      "source": [
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyDY-NLw3J4b",
        "outputId": "b96554c6-de90-4a30-f4ce-c38a931b24a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "ABSTRACT\n",
            "In recent years, people are seeking for a solution to improve text\n",
            "summarization for Thai language., Although several solutions such\n",
            "as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
            "models, etc., have been proposed, research results in Thai text\n",
            "summarization were restricted due to limited corpus in Thai\n",
            "language with complex grammar., This paper applied a text\n",
            "summarization system for Thai travel news based on keyword\n",
            "scored in Thai language by extracting the most relevant sentences\n",
            "from the original document., We compared LSA and Non-negative\n",
            "Matrix Factorization (NMF) to find the algorithm that is suitable\n",
            "with Thai travel news., The suitable compression rates for Generic\n",
            "Sentence Relevance score (GRS) and K-means clustering were also\n",
            "evaluated., From these experiments, we concluded that keyword\n",
            "scored calculation by LSA with sentence selection by GRS is the\n",
            "best algorithm for summarizing Thai Travel News, compared with\n",
            "human with the best compression rate of 20%.\n",
            ", CCS Concepts\n",
            "• Information systems ➝ Information retrieval ➝ Retrieval\n",
            "tasks and goals➝ Summarization\n",
            "Keywords\n",
            "Text summarization; extractive summarization; non-negative\n",
            "matrix factorization\n",
            "1., INTRODUCTION\n",
            "Daily newspaper has abundant of data that users do not have\n",
            "enough time for reading them., It is difficult to identify the relevant\n",
            "information to satisfy the information needed by users., Automatic\n",
            "summarization can reduce the problem of information overloading\n",
            "and it has been proposed previously in English and other languages.\n",
            ", However, there were only a few research results in Thai text\n",
            "summarization due to the lack of corpus in Thai language and the\n",
            "complicated grammar.\n",
            ", Text Summarization, [1] is a technique for summarizing the content\n",
            "of the documents., It consists of three steps: 1) create an\n",
            "intermediate representation of the input text, 2) calculate score for\n",
            "the sentences based on the concepts, and 3) choose important\n",
            "sentences to be included in the summary., Text summarization can\n",
            "be divided into 2 approaches., The first approach is the extractive\n",
            "summarization, which relies on a method for extracting words and\n",
            "searching for keywords from the original document., The second\n",
            "approach is the abstractive summarization, which analyzes words\n",
            "by linguistic principles with transcription or interpretation from the\n",
            "original document., This approach implies more effective and\n",
            "accurate summary than the extractive methods., However, with the\n",
            "lack of Thai corpus, we chose to apply an extractive summarization\n",
            "method for Thai text summarization.\n",
            ", This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF)., We also tried using K-means clustering for\n",
            "document summarization., In this experiment, we compared 5\n",
            "models for 5 rounds with Thai travel news using the compression\n",
            "rates of 20%, 30% and 40% and reported the rate and method that\n",
            "produced the best result from the experiment.\n",
            ", 2. RELATED WORKS\n",
            ", In recent years, several models in Thai Text summarization have\n",
            "been introduced., Suwanno, N. et al., [2] proposed a Thai text\n",
            "summarization that extracted a paragraph from a document based\n",
            "on Thai compound nouns, term frequency method, and headline\n",
            "score for generating a summary., Chongsuntornsri, A., et al., [3]\n",
            "proposed a new approach for Text summarization in Thai based on\n",
            "content- and graph-based with the use of Topic Sensitive PageRank\n",
            "algorithm for summarizing and ranking of text segments.\n",
            ", Jaruskulchai C., et al., [4] proposed a method to summarize\n",
            "documents by extracting important sentences from combining the\n",
            "specific properties (Local Property) and the overall properties\n",
            "(Global Property) of the sentences., The overall properties were\n",
            "based on the relationship between sentences in the document., From\n",
            "their experiments, the summarization of the industrial news got\n",
            "60% precision, 44% recall, and 50.9% F-measure, the general news\n",
            "got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
            "the fashion magazines got 53.0% precision, 33.0% recall, and\n",
            "40.4% F-measure.\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPZdMgs_3KP2"
      },
      "outputs": [],
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8vLdWTa3MdB",
        "outputId": "3d70da4e-58ce-4244-d890-12a50bf865cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              " ABSTRACT\n",
              " In recent years, people are seeking for a solution to improve text\n",
              " summarization for Thai language.: 2.176470588235294,\n",
              " Although several solutions such\n",
              " as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
              " models, etc., have been proposed, research results in Thai text\n",
              " summarization were restricted due to limited corpus in Thai\n",
              " language with complex grammar.: 3.117647058823529,\n",
              " This paper applied a text\n",
              " summarization system for Thai travel news based on keyword\n",
              " scored in Thai language by extracting the most relevant sentences\n",
              " from the original document.: 4.352941176470588,\n",
              " We compared LSA and Non-negative\n",
              " Matrix Factorization (NMF) to find the algorithm that is suitable\n",
              " with Thai travel news.: 1.3529411764705883,\n",
              " The suitable compression rates for Generic\n",
              " Sentence Relevance score (GRS) and K-means clustering were also\n",
              " evaluated.: 1.1176470588235294,\n",
              " From these experiments, we concluded that keyword\n",
              " scored calculation by LSA with sentence selection by GRS is the\n",
              " best algorithm for summarizing Thai Travel News, compared with\n",
              " human with the best compression rate of 20%.: 2.5882352941176467,\n",
              " CCS Concepts\n",
              " • Information systems ➝ Information retrieval ➝ Retrieval\n",
              " tasks and goals➝ Summarization\n",
              " Keywords\n",
              " Text summarization; extractive summarization; non-negative\n",
              " matrix factorization\n",
              " 1.: 5.588235294117647,\n",
              " INTRODUCTION\n",
              " Daily newspaper has abundant of data that users do not have\n",
              " enough time for reading them.: 0.4117647058823529,\n",
              " It is difficult to identify the relevant\n",
              " information to satisfy the information needed by users.: 0.823529411764706,\n",
              " Automatic\n",
              " summarization can reduce the problem of information overloading\n",
              " and it has been proposed previously in English and other languages.: 1.7647058823529413,\n",
              " However, there were only a few research results in Thai text\n",
              " summarization due to the lack of corpus in Thai language and the\n",
              " complicated grammar.: 2.4705882352941173,\n",
              " Text Summarization: 1.4705882352941178,\n",
              " [1] is a technique for summarizing the content\n",
              " of the documents.: 0.5882352941176471,\n",
              " It consists of three steps: 1) create an\n",
              " intermediate representation of the input text, 2) calculate score for\n",
              " the sentences based on the concepts, and 3) choose important\n",
              " sentences to be included in the summary.: 3.5294117647058814,\n",
              " Text summarization can\n",
              " be divided into 2 approaches.: 1.823529411764706,\n",
              " The first approach is the extractive\n",
              " summarization, which relies on a method for extracting words and\n",
              " searching for keywords from the original document.: 2.7647058823529407,\n",
              " The second\n",
              " approach is the abstractive summarization, which analyzes words\n",
              " by linguistic principles with transcription or interpretation from the\n",
              " original document.: 2.294117647058824,\n",
              " This approach implies more effective and\n",
              " accurate summary than the extractive methods.: 0.8823529411764706,\n",
              " However, with the\n",
              " lack of Thai corpus, we chose to apply an extractive summarization\n",
              " method for Thai text summarization.: 3.411764705882353,\n",
              " This research focused on the sentence extraction function based on\n",
              " keyword score calculation then selecting important sentences based\n",
              " on the Generic Sentence Relevance score (GRS), calculated from\n",
              " Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
              " Factorization (NMF).: 3.470588235294117,\n",
              " We also tried using K-means clustering for\n",
              " document summarization.: 1.6470588235294117,\n",
              " In this experiment, we compared 5\n",
              " models for 5 rounds with Thai travel news using the compression\n",
              " rates of 20%, 30% and 40% and reported the rate and method that\n",
              " produced the best result from the experiment.: 2.6470588235294112,\n",
              " 2. RELATED WORKS: 0.23529411764705882,\n",
              " In recent years, several models in Thai Text summarization have\n",
              " been introduced.: 1.9411764705882353,\n",
              " Suwanno, N. et al.: 0.35294117647058826,\n",
              " [2] proposed a Thai text\n",
              " summarization that extracted a paragraph from a document based\n",
              " on Thai compound nouns, term frequency method, and headline\n",
              " score for generating a summary.: 4.058823529411763,\n",
              " Chongsuntornsri, A., et al.: 0.35294117647058826,\n",
              " [3]\n",
              " proposed a new approach for Text summarization in Thai based on\n",
              " content- and graph-based with the use of Topic Sensitive PageRank\n",
              " algorithm for summarizing and ranking of text segments.: 4.235294117647057,\n",
              " Jaruskulchai C., et al.: 0.35294117647058826,\n",
              " [4] proposed a method to summarize\n",
              " documents by extracting important sentences from combining the\n",
              " specific properties (Local Property) and the overall properties\n",
              " (Global Property) of the sentences.: 2.5882352941176467,\n",
              " The overall properties were\n",
              " based on the relationship between sentences in the document.: 1.5882352941176472,\n",
              " From\n",
              " their experiments, the summarization of the industrial news got\n",
              " 60% precision, 44% recall, and 50.9% F-measure, the general news\n",
              " got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
              " the fashion magazines got 53.0% precision, 33.0% recall, and\n",
              " 40.4% F-measure.: 4.588235294117644}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfZLsCg53wCp"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiRTSRGj3xg6",
        "outputId": "dcd4a9b3-173c-408a-f34d-eb9a827d0d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xw5ftgc3y_d"
      },
      "outputs": [],
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rBD_6w_30dv",
        "outputId": "9a8dc1f3-a9e5-491e-ac37-a5a10801c10f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[CCS Concepts\n",
              " • Information systems ➝ Information retrieval ➝ Retrieval\n",
              " tasks and goals➝ Summarization\n",
              " Keywords\n",
              " Text summarization; extractive summarization; non-negative\n",
              " matrix factorization\n",
              " 1.,\n",
              " From\n",
              " their experiments, the summarization of the industrial news got\n",
              " 60% precision, 44% recall, and 50.9% F-measure, the general news\n",
              " got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
              " the fashion magazines got 53.0% precision, 33.0% recall, and\n",
              " 40.4% F-measure.,\n",
              " This paper applied a text\n",
              " summarization system for Thai travel news based on keyword\n",
              " scored in Thai language by extracting the most relevant sentences\n",
              " from the original document.,\n",
              " [3]\n",
              " proposed a new approach for Text summarization in Thai based on\n",
              " content- and graph-based with the use of Topic Sensitive PageRank\n",
              " algorithm for summarizing and ranking of text segments.,\n",
              " [2] proposed a Thai text\n",
              " summarization that extracted a paragraph from a document based\n",
              " on Thai compound nouns, term frequency method, and headline\n",
              " score for generating a summary.,\n",
              " It consists of three steps: 1) create an\n",
              " intermediate representation of the input text, 2) calculate score for\n",
              " the sentences based on the concepts, and 3) choose important\n",
              " sentences to be included in the summary.,\n",
              " This research focused on the sentence extraction function based on\n",
              " keyword score calculation then selecting important sentences based\n",
              " on the Generic Sentence Relevance score (GRS), calculated from\n",
              " Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
              " Factorization (NMF).,\n",
              " However, with the\n",
              " lack of Thai corpus, we chose to apply an extractive summarization\n",
              " method for Thai text summarization.,\n",
              " Although several solutions such\n",
              " as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
              " models, etc., have been proposed, research results in Thai text\n",
              " summarization were restricted due to limited corpus in Thai\n",
              " language with complex grammar.]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuiWQWIB311O"
      },
      "outputs": [],
      "source": [
        "final_summary = [word.text for word in summary]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGs6IsSb33tk"
      },
      "outputs": [],
      "source": [
        "summary = ' '.join(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIpxcY4X35CS",
        "outputId": "9f092df7-8971-4898-c5cc-c578e05d900d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ABSTRACT\n",
            "In recent years, people are seeking for a solution to improve text\n",
            "summarization for Thai language. Although several solutions such\n",
            "as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
            "models, etc., have been proposed, research results in Thai text\n",
            "summarization were restricted due to limited corpus in Thai\n",
            "language with complex grammar. This paper applied a text\n",
            "summarization system for Thai travel news based on keyword\n",
            "scored in Thai language by extracting the most relevant sentences\n",
            "from the original document. We compared LSA and Non-negative\n",
            "Matrix Factorization (NMF) to find the algorithm that is suitable\n",
            "with Thai travel news. The suitable compression rates for Generic\n",
            "Sentence Relevance score (GRS) and K-means clustering were also\n",
            "evaluated. From these experiments, we concluded that keyword\n",
            "scored calculation by LSA with sentence selection by GRS is the\n",
            "best algorithm for summarizing Thai Travel News, compared with\n",
            "human with the best compression rate of 20%.\n",
            "CCS Concepts\n",
            "• Information systems ➝ Information retrieval ➝ Retrieval\n",
            "tasks and goals➝ Summarization\n",
            "Keywords\n",
            "Text summarization; extractive summarization; non-negative\n",
            "matrix factorization\n",
            "1. INTRODUCTION\n",
            "Daily newspaper has abundant of data that users do not have\n",
            "enough time for reading them. It is difficult to identify the relevant\n",
            "information to satisfy the information needed by users. Automatic\n",
            "summarization can reduce the problem of information overloading\n",
            "and it has been proposed previously in English and other languages.\n",
            "However, there were only a few research results in Thai text\n",
            "summarization due to the lack of corpus in Thai language and the\n",
            "complicated grammar.\n",
            "Text Summarization [1] is a technique for summarizing the content\n",
            "of the documents. It consists of three steps: 1) create an\n",
            "intermediate representation of the input text, 2) calculate score for\n",
            "the sentences based on the concepts, and 3) choose important\n",
            "sentences to be included in the summary. Text summarization can\n",
            "be divided into 2 approaches. The first approach is the extractive\n",
            "summarization, which relies on a method for extracting words and\n",
            "searching for keywords from the original document. The second\n",
            "approach is the abstractive summarization, which analyzes words\n",
            "by linguistic principles with transcription or interpretation from the\n",
            "original document. This approach implies more effective and\n",
            "accurate summary than the extractive methods. However, with the\n",
            "lack of Thai corpus, we chose to apply an extractive summarization\n",
            "method for Thai text summarization.\n",
            "This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF). We also tried using K-means clustering for\n",
            "document summarization. In this experiment, we compared 5\n",
            "models for 5 rounds with Thai travel news using the compression\n",
            "rates of 20%, 30% and 40% and reported the rate and method that\n",
            "produced the best result from the experiment.\n",
            "2. RELATED WORKS\n",
            "In recent years, several models in Thai Text summarization have\n",
            "been introduced. Suwanno, N. et al. [2] proposed a Thai text\n",
            "summarization that extracted a paragraph from a document based\n",
            "on Thai compound nouns, term frequency method, and headline\n",
            "score for generating a summary. Chongsuntornsri, A., et al. [3]\n",
            "proposed a new approach for Text summarization in Thai based on\n",
            "content- and graph-based with the use of Topic Sensitive PageRank\n",
            "algorithm for summarizing and ranking of text segments.\n",
            "Jaruskulchai C., et al. [4] proposed a method to summarize\n",
            "documents by extracting important sentences from combining the\n",
            "specific properties (Local Property) and the overall properties\n",
            "(Global Property) of the sentences. The overall properties were\n",
            "based on the relationship between sentences in the document. From\n",
            "their experiments, the summarization of the industrial news got\n",
            "60% precision, 44% recall, and 50.9% F-measure, the general news\n",
            "got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
            "the fashion magazines got 53.0% precision, 33.0% recall, and\n",
            "40.4% F-measure.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmbwOjsO36qg",
        "outputId": "3580101a-eb3c-4d1d-d5ab-fffa6c5da12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CCS Concepts\n",
            "• Information systems ➝ Information retrieval ➝ Retrieval\n",
            "tasks and goals➝ Summarization\n",
            "Keywords\n",
            "Text summarization; extractive summarization; non-negative\n",
            "matrix factorization\n",
            "1. From\n",
            "their experiments, the summarization of the industrial news got\n",
            "60% precision, 44% recall, and 50.9% F-measure, the general news\n",
            "got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
            "the fashion magazines got 53.0% precision, 33.0% recall, and\n",
            "40.4% F-measure.\n",
            " This paper applied a text\n",
            "summarization system for Thai travel news based on keyword\n",
            "scored in Thai language by extracting the most relevant sentences\n",
            "from the original document. [3]\n",
            "proposed a new approach for Text summarization in Thai based on\n",
            "content- and graph-based with the use of Topic Sensitive PageRank\n",
            "algorithm for summarizing and ranking of text segments.\n",
            " [2] proposed a Thai text\n",
            "summarization that extracted a paragraph from a document based\n",
            "on Thai compound nouns, term frequency method, and headline\n",
            "score for generating a summary. It consists of three steps: 1) create an\n",
            "intermediate representation of the input text, 2) calculate score for\n",
            "the sentences based on the concepts, and 3) choose important\n",
            "sentences to be included in the summary. This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF). However, with the\n",
            "lack of Thai corpus, we chose to apply an extractive summarization\n",
            "method for Thai text summarization.\n",
            " Although several solutions such\n",
            "as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
            "models, etc., have been proposed, research results in Thai text\n",
            "summarization were restricted due to limited corpus in Thai\n",
            "language with complex grammar.\n"
          ]
        }
      ],
      "source": [
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcZib3Q_rTh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    cleaned_text = re.sub('<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters\n",
        "    cleaned_text = re.sub('[^a-zA-Z0-9]', ' ', cleaned_text)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = [word.lower() for sent in sentences for word in word_tokenize(sent)]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return sentences, words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYr1m1cy_tza"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def score_sentences(sentences, words):\n",
        "    word_frequencies = Counter(words)\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for index, sent in enumerate(sentences):\n",
        "        for word in word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    if index not in sentence_scores:\n",
        "                        sentence_scores[index] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[index] += word_frequencies[word]\n",
        "\n",
        "    return sentence_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciawg8Fe_71O"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "def select_sentences(sentences, sentence_scores, num_sentences):\n",
        "    selected_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    summary = [sentences[index] for index in selected_sentences]\n",
        "    return ' '.join(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UeU9J-5vACUE",
        "outputId": "990696b1-b2d9-4d13-e57e-f6eefccc1231"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'CCS Concepts\\n• Information systems ➝ Information retrieval ➝ Retrieval\\ntasks and goals➝ Summarization\\nKeywords\\nText summarization; extractive summarization; non-negative\\nmatrix factorization\\n1. From\\ntheir experiments, the summarization of the industrial news got\\n60% precision, 44% recall, and 50.9% F-measure, the general news\\ngot the 51.8% precision, 38.5% recall, and 43.1% F-measure while\\nthe fashion magazines got 53.0% precision, 33.0% recall, and\\n40.4% F-measure.\\n This paper applied a text\\nsummarization system for Thai travel news based on keyword\\nscored in Thai language by extracting the most relevant sentences\\nfrom the original document. [3]\\nproposed a new approach for Text summarization in Thai based on\\ncontent- and graph-based with the use of Topic Sensitive PageRank\\nalgorithm for summarizing and ranking of text segments.\\n [2] proposed a Thai text\\nsummarization that extracted a paragraph from a document based\\non Thai compound nouns, term frequency method, and headline\\nscore for generating a summary. It consists of three steps: 1) create an\\nintermediate representation of the input text, 2) calculate score for\\nthe sentences based on the concepts, and 3) choose important\\nsentences to be included in the summary. This research focused on the sentence extraction function based on\\nkeyword score calculation then selecting important sentences based\\non the Generic Sentence Relevance score (GRS), calculated from\\nLatent Semantic Analysis (LSA) and Non-negative Matrix\\nFactorization (NMF). However, with the\\nlack of Thai corpus, we chose to apply an extractive summarization\\nmethod for Thai text summarization.\\n Although several solutions such\\nas PageRank, Graph Rank, Latent Semantic Analysis (LSA)\\nmodels, etc., have been proposed, research results in Thai text\\nsummarization were restricted due to limited corpus in Thai\\nlanguage with complex grammar.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2hRLgM1sAxUf",
        "outputId": "525e98bb-fa6b-4137-fc49-4c6ebb2af1b6"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-60050f6105dc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Training Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'one'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'two'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get word vector representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             self.train(\n\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[1;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[1;32m    492\u001b[0m             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[1;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    567\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 )\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not iterable"
          ]
        }
      ],
      "source": [
        "#import gensim\n",
        "\n",
        "# Training Word2Vec model\n",
        "#sentences = [['sentence', 'one'], ['sentence', 'two'], ...]\n",
        "#model = gensim.models.Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Get word vector representation\n",
        "#word_vector = model['word']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3buEXMQAyPG"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Build LSTM model\n",
        "#model = Sequential()\n",
        "#model.add(LSTM(128, input_shape=(max_sequence_length, embedding_dim)))\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Train the model\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.fit(X_train, y_train, batch_size=32, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n5iWT0EA3BF"
      },
      "outputs": [],
      "source": [
        "#from transformers import pipeline\n",
        "\n",
        "# Load BERT-based summarization model\n",
        "#summarizer = pipeline('summarization')\n",
        "\n",
        "# Summarize text using BERT\n",
        "#text = \"Your input text here\"\n",
        "#summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# Access summary\n",
        "#print(summary[0]['summary_text'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
