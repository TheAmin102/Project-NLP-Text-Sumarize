{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IJy6VR12jhxK"
      },
      "outputs": [],
      "source": [
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBUam9ePjtpd",
        "outputId": "4ad5a156-4cfe-4873-a7c9-5a454bf74725"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-07-05 15:27:24.355661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-05 15:27:25.422895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "Vqcv0edZjyY1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "v3x_ygkPj0uR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# For some OS you need to make a correctino installing the package separately using this command:\n",
        "# !pip3 install -U spacy\n",
        "# !python3 -m spacy download en_core_web_sm\n",
        "# run both command above to install a separate package from the spacy"
      ],
      "metadata": {
        "id": "EYiLBvztkAIC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "2RwaCAQKkCqz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ozz4GmkEdI",
        "outputId": "8ba83e8e-ec2e-4149-9418-bd20c0ab7c71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Extractive', 'Text', 'Summarization', 'for', 'Thai', 'Travel', 'News', '\\n', 'Based', 'on', 'Keyword', 'Scored', 'in', 'Thai', 'Language', '\\n', 'Sarunya', 'Nathonghor', '\\n\\n', 'Duangdao', 'Wichadakul', '\\n\\n', 'Department', 'of', 'Computer', 'Engineering', '\\n', 'Chulalongkorn', 'University', '\\n', 'Bangkok', ',', 'Thailand', '\\n\\n', 'Department', 'of', 'Computer', 'Engineering', '\\n', 'Chulalongkorn', 'University', '\\n', 'Bangkok', ',', 'Thailand', '\\n\\n', 'Sarunya.N@Student.Chula.ac.th', '\\n', 'ABSTRACT', '\\n\\n', 'In', 'recent', 'years', ',', 'people', 'are', 'seeking', 'for', 'a', 'solution', 'to', 'improve', 'text', '\\n', 'summarization', 'for', 'Thai', 'language', '.', 'Although', 'several', 'solutions', 'such', '\\n', 'as', 'PageRank', ',', 'Graph', 'Rank', ',', 'Latent', 'Semantic', 'Analysis', '(', 'LSA', ')', '\\n', 'models', ',', 'etc', '.', ',', 'have', 'been', 'proposed', ',', 'research', 'results', 'in', 'Thai', 'text', '\\n', 'summarization', 'were', 'restricted', 'due', 'to', 'limited', 'corpus', 'in', 'Thai', '\\n', 'language', 'with', 'complex', 'grammar', '.', 'This', 'paper', 'applied', 'a', 'text', '\\n', 'summarization', 'system', 'for', 'Thai', 'travel', 'news', 'based', 'on', 'keyword', '\\n', 'scored', 'in', 'Thai', 'language', 'by', 'extracting', 'the', 'most', 'relevant', 'sentences', '\\n', 'from', 'the', 'original', 'document', '.', 'We', 'compared', 'LSA', 'and', 'Non', '-', 'negative', '\\n', 'Matrix', 'Factorization', '(', 'NMF', ')', 'to', 'find', 'the', 'algorithm', 'that', 'is', 'suitable', '\\n', 'with', 'Thai', 'travel', 'news', '.', 'The', 'suitable', 'compression', 'rates', 'for', 'Generic', '\\n', 'Sentence', 'Relevance', 'score', '(', 'GRS', ')', 'and', 'K', '-', 'means', 'clustering', 'were', 'also', '\\n', 'evaluated', '.', 'From', 'these', 'experiments', ',', 'we', 'concluded', 'that', 'keyword', '\\n', 'scored', 'calculation', 'by', 'LSA', 'with', 'sentence', 'selection', 'by', 'GRS', 'is', 'the', '\\n', 'best', 'algorithm', 'for', 'summarizing', 'Thai', 'Travel', 'News', ',', 'compared', 'with', '\\n', 'human', 'with', 'the', 'best', 'compression', 'rate', 'of', '20', '%', '.', '\\n\\n', 'CCS', 'Concepts', '\\n\\n', 'â€¢', 'Information', 'systems', 'â', 'Information', 'retrieval', 'â', 'Retrieval', '\\n', 'tasks', 'and', 'goals', 'â', 'Summarization', '\\n\\n', 'Keywords', '\\n\\n', 'Text', 'summarization', ';', 'extractive', 'summarization', ';', 'non', '-', 'negative', '\\n', 'matrix', 'factorization', '\\n\\n', '1', '.', '\\n\\n', 'INTRODUCTION', '\\n\\n', 'Daily', 'newspaper', 'has', 'abundant', 'of', 'data', 'that', 'users', 'do', 'not', 'have', '\\n', 'enough', 'time', 'for', 'reading', 'them', '.', 'It', 'is', 'difficult', 'to', 'identify', 'the', 'relevant', '\\n', 'information', 'to', 'satisfy', 'the', 'information', 'needed', 'by', 'users', '.', 'Automatic', '\\n', 'summarization', 'can', 'reduce', 'the', 'problem', 'of', 'information', 'overloading', '\\n', 'and', 'it', 'has', 'been', 'proposed', 'previously', 'in', 'English', 'and', 'other', 'languages', '.', '\\n', 'However', ',', 'there', 'were', 'only', 'a', 'few', 'research', 'results', 'in', 'Thai', 'text', '\\n', 'summarization', 'due', 'to', 'the', 'lack', 'of', 'corpus', 'in', 'Thai', 'language', 'and', 'the', '\\n', 'complicated', 'grammar', '.', '\\n', 'Text', 'Summarization', '[', '1', ']', 'is', 'a', 'technique', 'for', 'summarizing', 'the', 'content', '\\n', 'of', 'the', 'documents', '.', 'It', 'consists', 'of', 'three', 'steps', ':', '1', ')', 'create', 'an', '\\n', 'intermediate', 'representation', 'of', 'the', 'input', 'text', ',', '2', ')', 'calculate', 'score', 'for', '\\n', 'the', 'sentences', 'based', 'on', 'the', 'concepts', ',', 'and', '3', ')', 'choose', 'important', '\\n', 'Permission', 'to', 'make', 'digital', 'or', 'hard', 'copies', 'of', 'all', 'or', 'part', 'of', 'this', 'work', 'for', '\\n', 'personal', 'or', 'classroom', 'use', 'is', 'granted', 'without', 'fee', 'provided', 'that', 'copies', 'are', '\\n', 'not', 'made', 'or', 'distributed', 'for', 'profit', 'or', 'commercial', 'advantage', 'and', 'that', 'copies', '\\n', 'bear', 'this', 'notice', 'and', 'the', 'full', 'citation', 'on', 'the', 'first', 'page', '.', 'Copyrights', 'for', '\\n', 'components', 'of', 'this', 'work', 'owned', 'by', 'others', 'than', 'ACM', 'must', 'be', 'honored', '.', '\\n', 'Abstracting', 'with', 'credit', 'is', 'permitted', '.', 'To', 'copy', 'otherwise', ',', 'or', 'republish', ',', 'to', '\\n', 'post', 'on', 'servers', 'or', 'to', 'redistribute', 'to', 'lists', ',', 'requires', 'prior', 'specific', 'permission', '\\n', 'and/or', 'a', 'fee', '.', 'Request', 'permissions', 'from', 'Permissions@acm.org', '.', '\\n', 'ITCC', '2020', ',', 'August', '12â€“14', ',', '2020', ',', 'Kuala', 'Lumpur', ',', 'Malaysia', '\\n', 'Â©', '2020', 'Association', 'for', 'Computing', 'Machinery', '.', '\\n', 'ACM', 'ISBN', '978', '-', '1', '-', '4503', '-', '7539', '-', '9/20/08', 'â€¦', '$15.00', '\\n\\n', 'DOI', ':', 'https://doi.org/10.1145/3417473.3417479', '\\n\\n', 'Duangdao.W@Chula.ac.th', '\\n\\n', 'sentences', 'to', 'be', 'included', 'in', 'the', 'summary', '.', 'Text', 'summarization', 'can', '\\n', 'be', 'divided', 'into', '2', 'approaches', '.', 'The', 'first', 'approach', 'is', 'the', 'extractive', '\\n', 'summarization', ',', 'which', 'relies', 'on', 'a', 'method', 'for', 'extracting', 'words', 'and', '\\n', 'searching', 'for', 'keywords', 'from', 'the', 'original', 'document', '.', 'The', 'second', '\\n', 'approach', 'is', 'the', 'abstractive', 'summarization', ',', 'which', 'analyzes', 'words', '\\n', 'by', 'linguistic', 'principles', 'with', 'transcription', 'or', 'interpretation', 'from', 'the', '\\n', 'original', 'document', '.', 'This', 'approach', 'implies', 'more', 'effective', 'and', '\\n', 'accurate', 'summary', 'than', 'the', 'extractive', 'methods', '.', 'However', ',', 'with', 'the', '\\n', 'lack', 'of', 'Thai', 'corpus', ',', 'we', 'chose', 'to', 'apply', 'an', 'extractive', 'summarization', '\\n', 'method', 'for', 'Thai', 'text', 'summarization', '.', '\\n', 'This', 'research', 'focused', 'on', 'the', 'sentence', 'extraction', 'function', 'based', 'on', '\\n', 'keyword', 'score', 'calculation', 'then', 'selecting', 'important', 'sentences', 'based', '\\n', 'on', 'the', 'Generic', 'Sentence', 'Relevance', 'score', '(', 'GRS', ')', ',', 'calculated', 'from', '\\n', 'Latent', 'Semantic', 'Analysis', '(', 'LSA', ')', 'and', 'Non', '-', 'negative', 'Matrix', '\\n', 'Factorization', '(', 'NMF', ')', '.', 'We', 'also', 'tried', 'using', 'K', '-', 'means', 'clustering', 'for', '\\n', 'document', 'summarization', '.', 'In', 'this', 'experiment', ',', 'we', 'compared', '5', '\\n', 'models', 'for', '5', 'rounds', 'with', 'Thai', 'travel', 'news', 'using', 'the', 'compression', '\\n', 'rates', 'of', '20', '%', ',', '30', '%', 'and', '40', '%', 'and', 'reported', 'the', 'rate', 'and', 'method', 'that', '\\n', 'produced', 'the', 'best', 'result', 'from', 'the', 'experiment', '.', '\\n\\n', '2', '.', '\\n\\n', 'RELATED', 'WORKS', '\\n\\n', 'In', 'recent', 'years', ',', 'several', 'models', 'in', 'Thai', 'Text', 'summarization', 'have', '\\n', 'been', 'introduced', '.', 'Suwanno', ',', 'N.', 'et', 'al', '.', '[', '2', ']', 'proposed', 'a', 'Thai', 'text', '\\n', 'summarization', 'that', 'extracted', 'a', 'paragraph', 'from', 'a', 'document', 'based', '\\n', 'on', 'Thai', 'compound', 'nouns', ',', 'term', 'frequency', 'method', ',', 'and', 'headline', '\\n', 'score', 'for', 'generating', 'a', 'summary', '.', 'Chongsuntornsri', ',', 'A.', ',', 'et', 'al', '.', '[', '3', ']', '\\n', 'proposed', 'a', 'new', 'approach', 'for', 'Text', 'summarization', 'in', 'Thai', 'based', 'on', '\\n', 'content-', 'and', 'graph', '-', 'based', 'with', 'the', 'use', 'of', 'Topic', 'Sensitive', 'PageRank', '\\n', 'algorithm', 'for', 'summarizing', 'and', 'ranking', 'of', 'text', 'segments', '.', '\\n', 'Jaruskulchai', 'C.', ',', 'et', 'al', '.', '[', '4', ']', 'proposed', 'a', 'method', 'to', 'summarize', '\\n', 'documents', 'by', 'extracting', 'important', 'sentences', 'from', 'combining', 'the', '\\n', 'specific', 'properties', '(', 'Local', 'Property', ')', 'and', 'the', 'overall', 'properties', '\\n', '(', 'Global', 'Property', ')', 'of', 'the', 'sentences', '.', 'The', 'overall', 'properties', 'were', '\\n', 'based', 'on', 'the', 'relationship', 'between', 'sentences', 'in', 'the', 'document', '.', 'From', '\\n', 'their', 'experiments', ',', 'the', 'summarization', 'of', 'the', 'industrial', 'news', 'got', '\\n', '60', '%', 'precision', ',', '44', '%', 'recall', ',', 'and', '50.9', '%', 'F', '-', 'measure', ',', 'the', 'general', 'news', '\\n', 'got', 'the', '51.8', '%', 'precision', ',', '38.5', '%', 'recall', ',', 'and', '43.1', '%', 'F', '-', 'measure', 'while', '\\n', 'the', 'fashion', 'magazines', 'got', '53.0', '%', 'precision', ',', '33.0', '%', 'recall', ',', 'and', '\\n', '40.4', '%', 'F', '-', 'measure', '.', '\\n', 'Mani', ',', 'I.', ',', 'et', 'al', '.', '[', '5', ']', 'proposed', 'techniques', 'of', 'text', 'summarization', 'by', '\\n', 'using', 'word', 'frequency', 'in', 'the', 'document', 'and', 'calculated', 'the', 'weight', 'of', '\\n', 'word', 'to', 'create', 'a', 'keyword', 'group', '.', 'They', 'then', 'calculated', 'the', 'cosine', '\\n', 'similarity', 'of', 'sentences', '.', 'The', 'researcher', 'used', 'A', '*', 'search', 'algorithm', 'to', '\\n', 'find', 'the', 'shortest', 'sequence', 'of', 'sentences', 'from', 'keyword', 'group', 'by', '\\n', 'topic', 'calculation', ',', 'sentence', 'segmentation', 'and', 'word', 'grouping', '.', 'The', '\\n', 'sequence', 'of', 'sentences', 'that', 'were', 'in', 'the', 'main', 'group', 'were', 'selected', 'as', '\\n', 'important', 'sentences', '.', 'Their', 'summarization', 'of', 'the', 'agricultural', 'news', '\\n', 'got', '68.57', '%', 'precision', ',', '51.95', '%', 'recall', 'and', '56.72', '%', 'F', '-', 'measure', '.', '\\n', 'Lee', ',', 'J.', ',', 'et', 'al', '.', '[', '6', ']', 'proposed', 'a', 'document', 'summarization', 'method', 'using', '\\n', 'Non', '-', 'negative', 'Matrix', 'Factorization', '(', 'NMF', ')', '.', 'They', 'compared', '\\n\\n\\x0c', 'between', 'Latent', 'Semantic', 'Analysis', '(', 'LSA', ')', 'and', 'NMF', 'to', 'find', 'the', '\\n', 'weight', 'of', 'each', 'word', 'and', 'calculated', 'the', 'summation', 'of', 'weights', '.', 'The', '\\n', 'important', 'sentences', 'were', 'ranked', 'and', 'selected', 'into', 'the', 'summary', '\\n', 'based', 'on', 'their', 'summed', 'weight', '.', 'Based', 'on', 'LSA', ',', 'they', 'found', 'many', '\\n', 'weights', 'with', 'zero', 'and', 'negative', 'values', '.', 'However', ',', 'when', 'applied', '\\n', 'NMF', ',', 'they', 'found', 'only', 'the', 'positive', 'values', 'and', 'the', 'scope', 'of', 'the', '\\n', 'semantic', 'features', 'â€™', 'meaning', 'was', 'narrow', '.', 'Therefore', ',', 'they', 'proposed', '\\n', 'that', 'NMF', 'provided', 'a', 'greater', 'possibility', 'for', 'extracting', 'important', '\\n', 'sentences', '.', '\\n\\n', '3', '.', '\\n\\n', 'PREPROCESSING', 'FOR', 'THAI', 'TEXT', '\\n\\n', 'The', 'first', 'step', 'for', 'working', 'with', 'Thai', 'Text', 'is', 'word', 'tokenization', '.', 'Even', '\\n', 'though', 'Thai', 'writing', 'system', 'has', 'no', 'delimiters', 'to', 'indicate', 'word', '\\n', 'boundaries', 'together', 'with', 'many', 'rules', 'for', 'word', 'segmentation', ',', 'several', '\\n', 'Thai', 'word', 'tokenization', 'programs', 'have', 'been', 'proposed', '.', 'Table', '1', '\\n', 'shows', 'F1', 'score', 'of', 'the', 'recent', 'programs', 'trained', 'and', 'tested', 'by', 'one', 'of', '\\n', 'our', 'laboratory', 'members', 'with', 'the', 'data', 'from', 'BEST2010', 'corpus', '[', '7', ']', '.', '\\n', 'Cutkum', '[', '8', ']', 'got', 'the', 'highest', 'F1', 'score', ',', 'hence', ',', 'we', 'used', 'Cutkum', 'for', 'this', '\\n', 'step', '.', '\\n', 'Table', '1', '.', 'Comparison', 'of', 'Thai', 'word', 'tokenization', 'programs', '\\n', 'Tools', '\\n\\n', 'F1', 'Score', '\\n\\n', 'Validate', '\\n', 'PyICU', '[', '9', ']', '\\n\\n', 'Article', '\\n', '100', '\\n', '0.6155', '\\n\\n', 'Encyclopedia', '\\n', '100', '\\n', '0.6932', '\\n\\n', 'News', '\\n', '100', '\\n', '0.5987', '\\n\\n', 'Novel', '\\n', '100', '\\n', '0.6800', '\\n\\n', 'Lexto', '[', '10', ']', '\\n\\n', '0.7267', '\\n\\n', '0.7709', '\\n\\n', '0.6994', '\\n\\n', '0.7701', '\\n\\n', 'Cutkum', '\\n', 'wordcutpy', '[', '11', ']', '\\n\\n', '0.9322', '\\n', '0.6212', '\\n\\n', '0.9299', '\\n', '0.6286', '\\n\\n', '0.8987', '\\n', '0.6571', '\\n\\n', '0.7140', '\\n', '0.6247', '\\n\\n', 'cunlp', '[', '12', ']', '\\n\\n', '0.6910', '\\n\\n', '0.6172', '\\n\\n', '0.5748', '\\n\\n', '0.0000', '\\n\\n', 'SWATH', '[', '13', ']', '\\n\\n', '0.6347', '\\n\\n', '0.6858', '\\n\\n', '0.6200', '\\n\\n', '0.6867', '\\n\\n', '3.1', '\\n\\n', 'Latent', 'Semantic', 'Analysis', '\\n\\n', 'Latent', 'Semantic', 'Analysis', '(', 'LSA', ')', '[', '14', ']', 'is', 'the', 'algorithm', ',', 'which', '\\n', 'reduces', 'the', 'dimensionality', 'of', 'term', 'document', '.', 'The', 'algorithm', '\\n', 'creates', 'a', 'matrix', 'by', 'using', 'word', 'frequency', ',', 'applies', 'the', 'singular', 'value', '\\n', 'decomposition', '(', 'SVD', ')', '[', '15', ']', ',', 'and', 'then', 'finds', 'closely', 'related', 'terms', 'and', '\\n', 'documents', '.', 'The', 'original', 'matrix', 'A', 'can', 'be', 'separated', 'into', 'three', '\\n', 'matrices', ',', 'where', 'U', 'is', 'the', 'm', 'x', 'r', '(', 'words', 'x', 'extracted', 'concept', ')', 'matrix', ',', '\\n', 'V', 'is', 'the', 'n', 'x', 'r', '(', 'sentences', 'x', 'extracted', 'concepts', ')', 'matrix', ',', 'and', 'Î£', 'is', 'the', '\\n', 'r', 'x', 'r', 'diagonal', 'matrix', ',', 'which', 'can', 'be', 'reconstructed', 'to', 'find', 'the', 'original', '\\n', 'matrix', 'A.', 'The', 'SVD', 'can', 'be', 'represented', 'in', 'Eq', '.', '(', '1', ')', '.', '\\n\\n', '3.2', '\\n\\n', 'A', 'â‰ˆ', 'ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰', 'ğ‘‡ğ‘‡', '\\n\\n', 'of', 'the', 'related', 'singular', 'value', 'over', 'the', 'sum', 'of', 'all', 'singular', 'values', ',', 'for', '\\n', 'each', 'concept', '.', '\\n\\n', '3.3', '\\n\\n', '(', '2', ')', '\\n\\n', 'A', '=', 'ğ‘Šğ‘Šğ‘Šğ‘Š', '\\n\\n', 'Factors', 'W', 'and', 'H', 'can', 'be', 'found', 'by', 'solving', 'the', 'optimization', 'problem', '\\n', 'as', 'follows', ',', 'whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘—', 'â‰¥', '0', ',', 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–', 'â‰¥', '0', '.', '\\n', 'ğ‘šğ‘š', '\\n\\n', 'ğ‘›ğ‘›', '\\n\\n', 'ğ‘Ÿğ‘Ÿ', '\\n\\n', 'ğ‘—ğ‘—=1', 'ğ‘–ğ‘–=1', '\\n\\n', 'ğ‘™ğ‘™=1', '\\n\\n', '2', '\\n\\n', 'ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š', 'ğ¹ğ¹(ğ‘Šğ‘Š', ',', 'ğ»ğ»', ')', '=', '||', 'ğ´ğ´', 'âˆ’', 'ğ‘Šğ‘Šğ‘Šğ‘Š', '||2ğ¹ğ¹', '=', 'ï¿½', 'ï¿½', 'ï¿½', 'ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘–', 'âˆ’', 'ï¿½', 'ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘–', 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–', 'ï¿½', '\\n\\n', '(', '3', ')', '\\n\\n', 'NMF', 'and', 'LSA', 'are', 'both', 'matrix', 'factorization', 'algorithms', '.', 'However', ',', '\\n', 'when', 'using', 'NMF', 'to', 'find', 'keywords', ',', 'NMF', 'will', 'return', 'the', 'keywords', '\\n', 'that', 'are', 'closely', 'related', 'because', 'its', 'components', 'have', 'only', 'nonnegative', 'values', '.', 'As', 'LSA', 'has', 'both', 'positive', 'and', 'negative', 'values', 'as', '\\n', 'well', 'as', 'some', 'zeroes', ',', 'it', 'gets', 'a', 'wider', 'distribution', '.', 'The', 'semantic', '\\n', 'feature', 'represents', 'a', 'concept', 'of', 'meaning', 'for', 'root', 'of', 'words', 'that', 'have', '\\n', 'a', 'relationship', '.', 'For', 'example', ',', 'man', ',', 'human', ',', 'male', 'and', 'adult', 'have', 'the', '\\n', 'same', 'semantic', ',', 'hence', 'their', 'semantic', 'values', 'are', 'close', '.', '\\n', 'In', 'this', 'paper', ',', 'we', 'applied', 'LSA', 'and', 'NMF', 'on', 'the', 'Thai', 'Travel', 'News', '\\n', 'dataset', 'for', 'calculating', 'the', 'semantic', 'weights', ',', 'which', 'represented', 'the', '\\n', 'relationship', 'between', 'sentences', 'and', 'words', 'in', 'order', 'to', 'select', 'the', '\\n', 'representative', 'sentences', 'for', 'summarization', '.', '\\n\\n', '3.4', 'Generic', 'document', 'summarization', 'by', '\\n', 'NMF', '\\n\\n', 'Lee', ',', 'J.', ',', 'et', 'al', '.', 'proposed', 'Eq', '.', '(', '4', ')', 'and', 'Eq', '.', '(', '5', ')', 'to', 'select', 'a', 'number', 'of', '\\n', 'sentences', 'based', 'on', 'NMF', ',', 'which', 'got', 'the', 'highest', 'semantic', 'weight', '\\n', 'values', ',', 'where', 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–', 'is', 'the', 'weight', 'of', 'the', 'topic', 'ğ‘–ğ‘–', 'in', 'the', 'sentence', 'ğ‘—ğ‘—.', '\\n', 'Generic', 'Relevance', 'of', 'jth', 'sentence', '\\n', 'ğ‘Ÿğ‘Ÿ', '\\n\\n', '(', '1', ')', '\\n\\n', '=', 'ï¿½', 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–', 'ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘–', ')', '\\n\\n', 'Document', 'summarization', 'using', 'LSA', '\\n\\n', 'Gong', ',', 'Y.', 'et', 'al', '.', '[', '16', ']', 'proposed', 'a', 'document', 'summarization', 'based', 'on', '\\n', 'SVD', 'matrices', '.', 'In', 'our', 'work', ',', 'after', 'applying', 'SVD', 'to', 'matrix', 'A', ',', 'ğ‘‰ğ‘‰', 'ğ‘‡ğ‘‡', '\\n', 'matrix', 'used', 'for', 'selecting', 'the', 'important', 'sentences', '.', 'The', 'cell', 'value', 'of', '\\n', 'the', 'matrix', 'shows', 'the', 'relationship', 'between', 'sentence', 'and', 'extracted', '\\n', 'concepts', '.', 'A', 'sentence', 'with', 'the', 'highest', 'cell', 'value', 'of', 'each', 'concept', '\\n', 'will', 'be', 'selected', 'into', 'the', 'summary', 'starting', 'from', 'the', 'most', 'important', '\\n', 'concept', '.', 'The', 'total', 'number', 'of', 'sentences', 'in', 'the', 'summary', 'will', 'be', '\\n', 'equal', 'to', 'the', 'number', 'all', 'detected', 'concepts', '.', '\\n', 'Murray', ',', 'G.', 'et', 'al', '.', '[', '17', ']', 'proposed', 'a', 'document', 'summarization', 'based', '\\n', 'on', 'SVD', 'matrices', 'using', 'ğ‘‰ğ‘‰', 'ğ‘‡ğ‘‡', 'and', 'Î£', 'matrices', 'for', 'sentence', 'selection', '.', '\\n', 'The', 'authors', 'proposed', 'that', 'more', 'than', 'one', 'sentence', 'could', 'be', '\\n', 'collected', 'from', 'the', 'more', 'important', 'concepts', '.', 'The', 'decision', 'of', 'how', '\\n', 'many', 'sentences', 'would', 'be', 'collected', 'from', 'each', 'concept', 'depending', '\\n', 'on', 'the', 'Î£', 'matrix', '.', 'The', 'value', 'was', 'decided', 'by', 'getting', 'the', 'percentage', '\\n\\n', 'Non', '-', 'negative', 'Matrix', 'Factorization', '\\n\\n', 'Non', '-', 'negative', 'Matrix', 'Factorization', '(', 'NMF', ')', 'is', 'a', 'method', 'of', 'matrix', '\\n', 'factorization', 'subject', 'to', 'the', 'non', '-', 'negative', 'constraint', '.', 'Lee', ',', 'J.', ',', 'et', 'al', '.', '\\n', 'proposed', 'the', 'model', 'based', 'on', 'NMF', 'for', 'document', 'summarization', '.', '\\n', 'NMF', 'decomposes', 'a', 'non', '-', 'negative', 'matrix', 'ğ´ğ´', 'âˆˆ', 'ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š', 'into', 'two', 'nonnegative', 'matrices', '.', 'The', 'first', 'matrix', 'ğ‘šğ‘š', 'x', 'ğ‘Ÿğ‘Ÿ', 'is', 'a', 'non', '-', 'negative', 'semantic', '\\n', 'feature', 'matrix', '(', 'NSFM', ')', ',', 'ğ‘Šğ‘Š', '.', 'The', 'second', 'matrix', 'ğ‘Ÿğ‘Ÿ', 'x', 'ğ‘›ğ‘›', 'is', 'a', 'nonnegative', 'semantic', 'variable', 'matrix', '(', 'NSVM', ')', ',', 'ğ»ğ».', 'So', ',', 'we', 'have', 'ğ‘Šğ‘Š', 'âˆˆ', '\\n', 'ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š', 'and', 'ğ»ğ»', 'âˆˆ', 'ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ', 'and', 'both', 'terms', 'are', 'non', '-', 'negative', 'as', 'shown', 'in', '\\n', 'Eq', '.', '(', '2', ')', 'and', 'Eq', '.', '(', '3', ')', '.', '\\n\\n', '(', '4', ')', '\\n\\n', 'ğ‘–ğ‘–=1', '\\n\\n', 'ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘–', ')', '=', '\\n\\n', 'âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1', 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–', '\\n', 'ğ‘Ÿğ‘Ÿ', '\\n', 'âˆ‘ğ‘ğ‘=1', 'âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1', 'ğ»ğ»ğ‘ğ‘ğ‘ğ‘', '\\n\\n', '(', '5', ')', '\\n\\n', 'The', 'ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘–', ')', 'is', 'the', 'relative', 'relevance', 'of', 'the', 'ith', 'semantic', 'feature', '\\n', '(', 'ğ‘Šğ‘Šğ‘–ğ‘–', ')', ',', 'where', 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–', 'is', 'the', 'weight', 'of', 'the', 'topic', 'ğ‘–ğ‘–', 'in', 'the', 'sentence', 'ğ‘ğ‘', 'and', '\\n', 'ğ»ğ»ğ‘ğ‘ğ‘ğ‘', 'is', 'the', 'weight', 'of', 'the', 'topic', 'ğ‘ğ‘', 'in', 'the', 'sentence', 'ğ‘ğ‘.', 'The', 'sentences', '\\n', 'can', 'be', 'ranked', 'by', 'Generic', 'Relevance', 'Sentence', 'scores', '.', 'Sentences', '\\n', 'with', 'the', 'maximum', 'score', 'will', 'be', 'selected', 'into', 'the', 'summary', '.', '\\n\\n', '3.5', '\\n\\n', 'Cosine', 'Similarity', '\\n\\n', 'Cosine', 'similarity', '[', '18', ']', 'is', 'a', 'widely', 'used', 'method', 'to', 'measure', 'the', '\\n', 'similarity', 'between', 'vectors', 'representing', 'the', 'documents', '.', 'The', 'result', 'of', '\\n', 'cosine', 'similarity', 'is', 'ranging', 'from', '0', 'to', '1', '.', 'If', 'it', 'is', 'closer', 'to', '1', ',', 'that', 'means', '\\n', 'both', 'vectors', 'are', 'similar', '.', 'Eq', '.', '(', '6', ')', 'and', 'Eq', '.', '(', '7', ')', 'represents', 'the', 'cosine', '\\n\\n\\x0c', 'similarity', 'equation', ',', 'where', 'cos(Î¸', ')', 'is', 'the', 'dot', 'product', 'between', 'vectors', '\\n', 'of', 'sentences', 'A', 'and', 'B', 'and', 'divided', 'by', 'the', 'product', 'of', 'the', 'two', 'vectors', \"'\", '\\n', 'lengths', '.', '\\n', 'In', 'this', 'paper', ',', 'we', 'deployed', 'cosine', 'similarity', 'to', 'measure', 'the', 'similarity', '\\n', 'of', 'sentences', 'in', 'K', '-', 'means', 'clustering', '.', '\\n\\n', 'Aâˆ™B', '\\n', '||A||', '||B||', '\\n', 'âˆ‘ni=1', 'Ai', 'Bi', '\\n\\n', 'Similarity(A', ',', 'B', ')', '=', 'cos(Î¸', ')', '=', '\\n\\n', '(', '7', ')', '\\n\\n', 'K', '-', 'means', 'Clustering', '\\n\\n', '15', '\\n\\n', '67', '\\n\\n', '7', '\\n', '7', '\\n\\n', '13', '\\n', '13', '\\n\\n', '55', '\\n', '38', '\\n\\n', 'Table', '2', 'shows', 'the', 'overall', 'number', 'of', 'sentences', 'of', 'news', 'within', 'each', '\\n', 'dataset', '.', 'The', 'average', 'numbers', 'of', 'sentences', 'per', 'news', 'of', 'the', '5', 'sets', '\\n', 'were', '21', ',', '16', ',', '15', ',', '13', 'and', '13', 'sentences', ',', 'respectively', '.', '\\n\\n', '5', '.', '\\n', 'PIPELINE', 'FOR', 'GENERATING', '\\n', 'SUMMARIES', '\\n', 'In', 'this', 'section', ',', 'we', 'demonstrate', 'our', 'pipeline', '(', 'Figure', '1', ')', 'used', 'for', 'text', '\\n', 'summarization', 'to', 'generate', 'a', 'summary', 'for', 'a', 'Thai', 'travel', 'news', '.', '\\n\\n', 'Word', '\\n\\n', 'S9', '\\n\\n', '6', '\\n\\n', 'Round', '4', '\\n', 'Round', '5', '\\n\\n', 'Table', '3', '.', 'Example', 'of', 'Word', 'by', 'Sentence', 'Matrix', 'A', '\\n', 'S8', '\\n\\n', 'Round', '3', '\\n\\n', 'Avg', '.', 'Number', '\\n', 'of', 'Sentences', '\\n\\n', 'S7', '\\n\\n', '21', '\\n', '16', '\\n\\n', 'Round', '1', '\\n', 'Round', '2', '\\n\\n', 'Min', '.', '\\n', 'Number', 'of', '\\n', 'Sentences', '\\n\\n', 'S6', '\\n\\n', '7', '\\n', '7', '\\n\\n', 'Max', '.', '\\n', 'Number', 'of', '\\n', 'Sentences', '\\n', '58', '\\n', '58', '\\n\\n', 'Dataset', '\\n\\n', 'S5', '\\n\\n', 'Table', '2', '.', 'Overall', 'Sentence', 'Language', 'of', 'each', 'Dataset', '\\n\\n', 'S4', '\\n\\n', 'DATA', 'PREPARATION', '\\n\\n', 'The', 'standard', 'data', 'sets', 'in', 'Thai', 'language', 'are', 'unavailable', 'for', '\\n', 'evaluating', 'text', 'summarization', 'system', '.', 'Therefore', ',', 'we', 'collected', '400', '\\n', 'Thai', 'travel', 'news', 'from', 'Thairath', 'and', 'Manager', 'online', 'newspapers', 'to', '\\n', 'be', 'used', 'as', 'datasets', 'for', 'our', 'experiments', '.', 'We', 'split', '400', 'travel', 'news', '\\n', 'into', '5', 'sets', 'of', '80', 'news', 'each', '.', 'We', 'then', 'evaluated', 'the', 'performance', 'of', '\\n', 'text', 'summarization', 'methods', 'which', 'were', 'LSA', 'and', 'NMF', 'by', '\\n', 'comparing', 'their', 'results', 'with', 'the', 'summaries', 'manually', 'curated', 'by', '\\n', 'two', 'experts', 'from', 'the', 'Faculty', 'of', 'Liberal', 'Arts', ',', 'Ubon', 'Ratchathani', '\\n', 'University', '.', '\\n\\n', 'The', 'open', '-', 'source', 'python', 'libraries', 'such', 'as', 'numpy', '[', '19', ']', 'and', 'sklearn', '\\n', '[', '20', ']', 'were', 'used', 'in', 'our', 'system', '.', 'We', 'converted', 'the', 'Thai', 'travel', 'news', '\\n', 'obtained', 'from', 'Thairath', 'and', 'Manager', 'online', 'newspapers', 'to', 'plain', '\\n', 'text', '.', 'Then', ',', 'the', 'sentences', 'of', 'each', 'news', 'were', 'segmented', 'by', 'human', '\\n', 'with', 'the', 'following', 'format', ':', 'Si', '=', 'â€˜', 'xxx', 'â€™', ',', 'where', 'Si', 'represents', 'the', 'order', '\\n', 'of', 'the', 'sentence', 'in', 'the', 'original', 'document', 'and', 'â€˜', 'xxx', 'â€™', 'represents', 'the', '\\n', 'content', 'of', 'that', 'sentence', '.', 'After', 'removing', 'stop', 'words', 'and', 'duplicate', '\\n', 'words', ',', 'we', 'built', 'a', 'document', 'term', 'matrix', 'or', 'matrix', 'A', 'then', 'applied', '\\n', 'SVD', 'and', 'NMF', 'to', 'the', 'matrix', '.', 'Then', ',', 'we', 'used', 'python', 'modules', '\\n', 'numpy.linalg.svd', 'to', 'calculate', 'SVD', 'and', 'sklearn.decomposition', 'to', '\\n', 'calculate', 'NMF', '.', 'For', 'sentence', 'selection', ',', 'we', 'used', 'Gong', ',', 'Y.', 'et', 'al', '.', 'and', '\\n', 'Murray', ',', 'G.', 'et', 'al', '.', 'approaches', 'for', 'calculating', 'weight', 'of', 'the', 'sentence', '\\n', 'scores', 'then', 'selected', 'sentences', 'with', 'the', 'highest', 'scores', 'into', 'the', '\\n', 'summary', '.', 'For', 'keyword', 'score', 'calculation', 'of', 'NMF', ',', 'we', 'calculated', '\\n', 'the', 'keyword', 'score', 'from', 'Eq', '.', '(', '5', ')', 'and', 'then', 'selected', 'the', 'sentence', 'with', '\\n', 'the', 'highest', 'score', 'from', 'each', 'concept', '.', 'The', 'python', 'module', '\\n', 'sklearn.cluster', 'was', 'used', 'for', 'K', '-', 'means', 'clustering', '.', 'The', 'selected', '\\n', 'sentences', 'from', 'all', 'approaches', 'were', 'in', 'the', 'same', 'order', 'as', 'the', 'original', '\\n', 'document', '.', 'In', 'this', 'paper', ',', 'we', 'performed', 'the', '20', '%', ',', '30', '%', 'and', '40', '%', '\\n', 'document', 'compression', '.', 'This', 'meant', '80', '%', ',', '70', '%', 'and', '60', '%', 'of', 'the', '\\n', 'sentences', 'will', 'be', 'selected', 'into', 'the', 'summary', '.', '\\n\\n', 'S3', '\\n\\n', '4', '.', '\\n\\n', 'Figure', '1', '.', 'Document', 'summarization', 'pipeline', 'based', 'on', 'LSA', '\\n', 'and', 'NMF', '\\n\\n', 'S2', '\\n\\n', 'For', 'sentence', 'selection', 'by', 'K', '-', 'means', 'clustering', ',', 'we', 'grouped', 'similar', '\\n', 'sentences', 'into', 'the', 'same', 'cluster', 'using', 'the', 'following', 'steps', ':', '\\n', '1', '.', 'Randomly', 'select', 'K', 'sentences', 'as', 'the', 'representative', 'of', 'K', '\\n', 'groups', '.', 'K', 'in', 'this', 'paper', 'is', 'the', 'number', 'of', 'sentences', 'that', '\\n', 'will', 'be', 'selected', 'into', 'the', 'summary', '.', '\\n', '2', '.', 'Calculate', 'centroid', 'of', 'each', 'group', 'by', 'using', 'the', 'value', 'of', '\\n', 'sentence', 'vector', 'from', 'V', 'matrix', 'for', 'LSA', 'and', 'ğ»ğ»ğ‘‡ğ‘‡', 'matrix', '\\n', 'for', 'NMF', '.', '\\n', '3', '.', 'Use', 'cosine', 'similarity', 'to', 'calculate', 'sentence', 'similarity', '\\n', 'between', 'a', 'sentence', 'and', 'the', 'centroid', 'of', 'each', 'group', '.', 'Then', '\\n', 'assign', 'that', 'sentence', 'to', 'the', 'group', 'with', 'the', 'highest', '\\n', 'similarity', '.', '\\n', '4', '.', 'Repeat', 'steps', '2', '-', '3', 'until', 'all', 'sentences', 'are', 'assigned', 'to', 'a', '\\n', 'group', ',', 'no', 'sentences', 'change', 'the', 'group', ',', 'or', 'the', 'similarity', '\\n', 'between', 'sentences', 'and', 'their', 'centroid', 'is', 'close', '.', '\\n', '5', '.', 'Select', 'a', 'sentence', 'with', 'the', 'maximum', 'similarity', 'score', 'with', '\\n', 'the', 'centroid', 'of', 'the', 'group', 'and', 'add', 'it', 'into', 'the', 'summary', '.', '\\n\\n', 'S1', '\\n\\n', '3.6', '\\n\\n', 'Aâˆ™B', '\\n', '=', '\\n', 'n', '\\n', 'n', '\\n', '||A||', '||B||', '\\n', 'ï¿½', 'âˆ‘i=1', '\\n', 'A2i', 'ï¿½', 'âˆ‘i=1', '\\n', 'Bi2', '\\n\\n', '(', '6', ')', '\\n\\n', 'Mr.', 'Yontas', '\\n', 'ak', '\\n\\n', '1', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', 'Supason', '\\n\\n', '1', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', 'Tourism', '\\n', 'Authority', '\\n', 'of', 'Thailand', '\\n\\n', '1', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', '0', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n', 'â€¦', '\\n\\n\\x0c', 'Table', '3', 'demonstrates', 'an', 'example', 'of', 'a', 'matrix', 'ğ´ğ´', ',', 'constructed', 'from', '\\n', 'word', 'count', 'by', 'sentence', 'of', 'a', 'Thai', 'travel', 'news', '.', 'It', 'was', 'composed', 'of', '\\n', '98', 'words', 'and', '9', 'sentences', '.', 'This', 'matrix', 'ğ´ğ´', 'was', 'then', 'applied', 'with', 'the', '\\n', 'LSA', 'and', 'NMF', '.', 'The', 'sentence', 'vectors', 'were', 'calculated', 'from', 'the', 'term', '\\n', 'weight', 'and', 'the', 'semantic', 'feature', 'vectors', 'from', 'Eq', '.', '(', '1', ')', 'for', 'LSA', 'and', '\\n', 'Eq', '.', '(', '2', ')', 'for', 'NMF', '.', '\\n\\n', 'sentences', 'from', 'all', 'concepts', '.', 'The', 'Generic', 'Sentence', 'Relevance', 'score', '\\n', 'for', 'NMF', 'also', 'collected', 'one', 'sentence', 'for', 'each', 'concept', ',', 'the', 'same', 'as', '\\n', 'Gong', ',', 'Y.', 'et', 'al', '.', 'but', 'with', 'the', 'highest', 'score', 'calculated', 'by', 'Eq', '.', '(', '5', ')', '.', 'As', '\\n', 'multiple', 'important', 'sentences', 'could', 'be', 'selected', 'from', 'a', 'more', '\\n', 'important', 'concept', ',', 'Murray', ',', 'G.', 'et', 'al', '.', 'outperformed', 'both', 'Gong', ',', 'Y.', 'et', '\\n', 'al', '.', 'and', 'the', 'GRS', 'method', '.', '\\n\\n', '6', '.', 'EXPERIMENT', 'AND', 'RESULTS', '\\n', '6.1', 'Performance', 'Evaluations', 'Measure', '\\n\\n', '7', '.', '\\n\\n', 'We', 'evaluated', 'the', 'results', 'of', 'the', 'summarization', 'by', 'using', 'standard', '\\n', 'accuracy', ',', 'precision', ',', 'recall', ',', 'and', 'F1', 'score', '[', '21', ']', '.', 'These', 'measurements', '\\n', 'quantify', 'the', 'differences', 'between', 'the', 'summary', 'from', 'human', 'and', 'the', '\\n', 'experimental', 'methods', '.', 'The', 'precision', 'shows', 'the', 'correctness', 'of', 'the', '\\n', 'extracted', 'sentences', 'and', 'the', 'recall', 'reflects', 'the', 'number', 'of', 'good', '\\n', 'sentences', 'missed', 'by', 'the', 'method', '.', '\\n\\n', '6', '.', '2', 'Experiment', 'Results', '\\n\\n', 'In', 'this', 'experimental', 'set', ',', 'we', 'would', 'like', 'to', 'explore', 'how', 'the', 'different', '\\n', 'sentence', 'selection', 'methods', ':', 'the', 'Generic', 'Sentence', 'Relevance', 'score', '\\n', 'and', 'K', '-', 'means', 'clustering', ',', 'affected', 'the', 'text', 'summarization', 'result', '.', '\\n', 'For', 'K', '-', 'means', 'clustering', ',', 'both', 'SVD', 'and', 'NMF', 'had', 'similar', '\\n', 'summarization', 'efficiency', '.', 'The', 'F1', 'score', 'of', 'SVD', 'with', 'K', '-', 'means', '\\n', 'clustering', 'was', '0.83', ',', '0.72', ',', 'and', '0.62', 'for', 'the', 'compression', 'rate', 'of', '20', '%', ',', '\\n', '30', '%', ',', 'and', '40', '%', '.', 'For', 'the', 'NMF', 'with', 'K', '-', 'means', 'clustering', ',', 'the', 'F1', 'score', '\\n', 'for', 'the', 'three', 'compression', 'rates', 'was', '0.83', ',', '0.74', 'and', '0.64', '.', '\\n', 'For', 'the', 'Generic', 'Sentence', 'Relevance', 'score', ',', 'the', 'best', 'F1', 'score', 'for', 'the', '\\n', 'compression', 'rate', 'of', '20', '%', ',', '30', '%', ',', 'and', '40', '%', 'was', '0.86', ',', '0.78', 'and', '0.68', '\\n', 'respectively', 'and', 'the', 'best', 'F1', 'scores', 'for', 'all', 'compression', 'rates', 'were', '\\n', 'from', 'the', 'approach', 'of', 'Murray', ',', 'G.', 'et', 'al', '.', '\\n\\n', 'Figure', '2', '.', 'Thai', 'text', 'summarization', 'efficiency', 'of', '5', 'models', '\\n', 'Figure', '2', 'shows', 'the', 'Thai', 'text', 'summarization', 'efficiency', 'of', '5', 'models', ':', '\\n', '(', '1', ')', 'NMF', 'with', 'GRS', ',', '(', '2', ')', 'NMF', 'with', 'K', '-', 'means', ',', '(', '3', ')', 'SVD', 'with', 'sentence', '\\n', 'score', 'by', 'Gong', ',', 'Y.', 'et', 'al', '.', ',', '(', '4', ')', 'SVD', 'with', 'K', '-', 'means', ',', 'and', '(', '5', ')', 'SVD', 'with', '\\n', 'sentence', 'score', 'by', 'Murray', ',', 'G.', 'et', 'al', '.', 'applied', 'to', '400', 'Thai', 'travel', 'news', ',', '\\n', 'divided', 'into', '5', 'sets', 'of', '80', 'news', 'each', ',', 'with', 'the', 'varied', 'compression', '\\n', 'rates', 'of', '20', '%', ',', '30', '%', 'and', '40', '%', '.', '\\n', 'From', 'this', 'experiment', ',', 'the', 'best', 'model', 'based', 'on', 'keyword', 'score', 'for', '\\n', 'Thai', 'travel', 'news', 'summarization', 'was', 'SVD', 'with', 'sentence', 'selection', '\\n', 'by', 'Murray', ',', 'G.', 'et', 'al', '.', 'This', 'model', 'with', 'the', 'compression', 'rate', 'of', '20', '%', '\\n', 'got', 'the', 'highest', 'score', 'because', 'Murray', 'G.', 'et', 'al', '.', 'method', 'determined', '\\n', 'the', 'number', 'of', 'sentences', 'to', 'be', 'extracted', 'from', 'each', 'concept', 'based', 'on', '\\n', 'the', 'importance', 'of', 'that', 'concept', '.', 'The', 'method', 'of', 'Gong', ',', 'Y.', 'et', 'al', '.', ',', 'on', '\\n', 'the', 'other', 'hand', 'was', 'proposed', 'to', 'select', 'only', 'one', 'sentence', 'with', 'the', '\\n', 'highest', 'score', 'from', 'each', 'concept', 'so', 'that', 'the', 'summary', 'would', 'include', '\\n\\n', 'CONCLUSIONS', '\\n\\n', 'In', 'this', 'paper', ',', 'we', 'applied', 'several', 'text', 'summarization', 'methods', 'to', '\\n', 'Thai', 'Travel', 'News', 'based', 'on', 'keyword', 'scored', 'in', 'Thai', 'language', 'by', '\\n', 'extracting', 'the', 'most', 'relevant', 'sentences', 'from', 'the', 'original', 'document', '.', '\\n', 'We', 'compared', 'LSA', 'and', 'NMF', 'together', 'with', 'different', 'sentence', '\\n', 'selection', 'methods', ',', 'to', 'find', 'the', 'algorithm', 'suitable', 'with', 'this', 'paper', \"'s\", '\\n', 'data', 'source', '.', 'We', 'concluded', 'that', 'keyword', 'scored', 'calculation', 'by', 'LSA', '\\n', 'with', 'sentence', 'selection', 'by', 'Generic', 'Sentence', 'Relevance', 'score', 'by', '\\n', 'Murray', ',', 'G.', 'et', 'al', '.', 'was', 'the', 'best', 'algorithm', 'while', 'the', 'best', 'compression', '\\n', 'rate', 'of', 'all', 'models', 'was', '20', '%', ',', 'for', 'summarizing', 'Thai', 'Travel', 'News', '\\n', 'compared', 'with', 'humans', '.', '\\n', 'In', 'future', 'work', ',', 'we', 'plan', 'to', 'perform', 'the', 'experiments', 'with', 'different', '\\n', 'types', 'of', 'documents', 'and', 'improve', 'word', 'segmentation', 'of', 'compound', '\\n', 'nouns', 'that', 'was', 'not', 'handled', 'by', 'Cutkum', '.', '\\n\\n', '8', '.', 'ACKNOWLEDGMENTS', '\\n\\n', 'We', 'would', 'like', 'to', 'thank', 'the', 'department', 'of', 'computer', 'engineering', ',', '\\n', 'faculty', 'of', 'engineering', ',', 'Chulalongkorn', 'University', 'for', 'providing', '\\n', 'computing', 'facilities', '.', '\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "owXJcM81kJfR",
        "outputId": "032a5900-fff5-4b94-a689-f93704ae8503"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1"
      ],
      "metadata": {
        "id": "PtOwiuTrkJ7J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCFsR0mmkMWE",
        "outputId": "f19abf54-2645-4968-f5bb-e2303441d1af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Extractive': 1, 'Text': 7, 'Summarization': 3, 'Thai': 35, 'Travel': 5, 'News': 6, 'Based': 2, 'Keyword': 1, 'Scored': 1, 'Language': 2, 'Sarunya': 1, 'Nathonghor': 1, '\\n\\n': 175, 'Duangdao': 1, 'Wichadakul': 1, 'Department': 2, 'Computer': 2, 'Engineering': 2, 'Chulalongkorn': 3, 'University': 4, 'Bangkok': 2, 'Thailand': 3, 'Sarunya.N@Student.Chula.ac.th': 1, 'ABSTRACT': 1, 'recent': 3, 'years': 2, 'people': 1, 'seeking': 1, 'solution': 1, 'improve': 2, 'text': 17, 'summarization': 37, 'language': 6, 'solutions': 1, 'PageRank': 2, 'Graph': 1, 'Rank': 1, 'Latent': 5, 'Semantic': 5, 'Analysis': 5, 'LSA': 18, 'models': 6, 'etc': 1, 'proposed': 15, 'research': 3, 'results': 4, 'restricted': 1, 'limited': 1, 'corpus': 4, 'complex': 1, 'grammar': 2, 'paper': 7, 'applied': 7, 'system': 4, 'travel': 10, 'news': 18, 'based': 17, 'keyword': 10, 'scored': 4, 'extracting': 5, 'relevant': 3, 'sentences': 43, 'original': 8, 'document': 18, 'compared': 6, 'Non': 5, 'negative': 12, 'Matrix': 6, 'Factorization': 5, 'NMF': 29, 'find': 6, 'algorithm': 8, 'suitable': 3, 'compression': 11, 'rates': 5, 'Generic': 9, 'Sentence': 9, 'Relevance': 8, 'score': 26, 'GRS': 5, 'K': 15, 'means': 13, 'clustering': 9, 'evaluated': 3, 'experiments': 4, 'concluded': 2, 'calculation': 5, 'sentence': 32, 'selection': 8, 'best': 8, 'summarizing': 4, 'human': 4, 'rate': 6, '20': 9, 'CCS': 1, 'Concepts': 1, 'â€¢': 1, 'Information': 2, 'systems': 1, 'â': 3, 'retrieval': 1, 'Retrieval': 1, 'tasks': 1, 'goals': 1, 'Keywords': 1, 'extractive': 4, 'non': 5, 'matrix': 25, 'factorization': 3, '1': 19, 'INTRODUCTION': 1, 'Daily': 1, 'newspaper': 1, 'abundant': 1, 'data': 4, 'users': 2, 'time': 1, 'reading': 1, 'difficult': 1, 'identify': 1, 'information': 3, 'satisfy': 1, 'needed': 1, 'Automatic': 1, 'reduce': 1, 'problem': 2, 'overloading': 1, 'previously': 1, 'English': 1, 'languages': 1, 'lack': 2, 'complicated': 1, 'technique': 1, 'content': 2, 'documents': 5, 'consists': 1, 'steps': 3, 'create': 2, 'intermediate': 1, 'representation': 1, 'input': 1, '2': 17, 'calculate': 4, 'concepts': 6, '3': 11, 'choose': 1, 'important': 11, 'Permission': 1, 'digital': 1, 'hard': 1, 'copies': 3, 'work': 4, 'personal': 1, 'classroom': 1, 'use': 2, 'granted': 1, 'fee': 2, 'provided': 2, 'distributed': 1, 'profit': 1, 'commercial': 1, 'advantage': 1, 'bear': 1, 'notice': 1, 'citation': 1, 'page': 1, 'Copyrights': 1, 'components': 2, 'owned': 1, 'ACM': 2, 'honored': 1, 'Abstracting': 1, 'credit': 1, 'permitted': 1, 'copy': 1, 'republish': 1, 'post': 1, 'servers': 1, 'redistribute': 1, 'lists': 1, 'requires': 1, 'prior': 1, 'specific': 2, 'permission': 1, 'and/or': 1, 'Request': 1, 'permissions': 1, 'Permissions@acm.org': 1, 'ITCC': 1, '2020': 3, 'August': 1, '12â€“14': 1, 'Kuala': 1, 'Lumpur': 1, 'Malaysia': 1, 'Â©': 1, 'Association': 1, 'Computing': 1, 'Machinery': 1, 'ISBN': 1, '978': 1, '4503': 1, '7539': 1, '9/20/08': 1, 'â€¦': 11, '$15.00': 1, 'DOI': 1, 'https://doi.org/10.1145/3417473.3417479': 1, 'Duangdao.W@Chula.ac.th': 1, 'included': 1, 'summary': 14, 'divided': 3, 'approaches': 3, 'approach': 5, 'relies': 1, 'method': 12, 'words': 8, 'searching': 1, 'keywords': 3, 'second': 2, 'abstractive': 1, 'analyzes': 1, 'linguistic': 1, 'principles': 1, 'transcription': 1, 'interpretation': 1, 'implies': 1, 'effective': 1, 'accurate': 1, 'methods': 6, 'chose': 1, 'apply': 1, 'focused': 1, 'extraction': 1, 'function': 1, 'selecting': 2, 'calculated': 7, 'tried': 1, 'experiment': 3, '5': 16, 'rounds': 1, '30': 5, '40': 5, 'reported': 1, 'produced': 1, 'result': 3, 'RELATED': 1, 'WORKS': 1, 'introduced': 1, 'Suwanno': 1, 'N.': 1, 'et': 21, 'al': 21, 'extracted': 6, 'paragraph': 1, 'compound': 2, 'nouns': 2, 'term': 4, 'frequency': 3, 'headline': 1, 'generating': 1, 'Chongsuntornsri': 1, 'A.': 2, 'new': 1, 'content-': 1, 'graph': 1, 'Topic': 1, 'Sensitive': 1, 'ranking': 1, 'segments': 1, 'Jaruskulchai': 1, 'C.': 1, '4': 7, 'summarize': 1, 'combining': 1, 'properties': 3, 'Local': 1, 'Property': 2, 'overall': 3, 'Global': 1, 'relationship': 4, 'industrial': 1, 'got': 7, '60': 2, 'precision': 6, '44': 1, 'recall': 6, '50.9': 1, 'F': 4, 'measure': 6, 'general': 1, '51.8': 1, '38.5': 1, '43.1': 1, 'fashion': 1, 'magazines': 1, '53.0': 1, '33.0': 1, '40.4': 1, 'Mani': 1, 'I.': 1, 'techniques': 1, 'word': 12, 'weight': 9, 'group': 9, 'cosine': 5, 'similarity': 12, 'researcher': 1, 'search': 1, 'shortest': 1, 'sequence': 2, 'topic': 4, 'segmentation': 3, 'grouping': 1, 'main': 1, 'selected': 10, 'agricultural': 1, '68.57': 1, '51.95': 1, '56.72': 1, 'Lee': 3, 'J.': 3, '6': 6, '\\n\\n\\x0c': 3, 'summation': 1, 'weights': 3, 'ranked': 2, 'summed': 1, 'found': 3, 'zero': 1, 'values': 7, 'positive': 2, 'scope': 1, 'semantic': 10, 'features': 1, 'â€™': 3, 'meaning': 2, 'narrow': 1, 'greater': 1, 'possibility': 1, 'PREPROCESSING': 1, 'THAI': 1, 'TEXT': 1, 'step': 2, 'working': 1, 'tokenization': 3, 'writing': 1, 'delimiters': 1, 'indicate': 1, 'boundaries': 1, 'rules': 1, 'programs': 3, 'Table': 6, 'shows': 5, 'F1': 8, 'trained': 1, 'tested': 1, 'laboratory': 1, 'members': 1, 'BEST2010': 1, '7': 8, 'Cutkum': 4, '8': 2, 'highest': 9, 'Comparison': 1, 'Tools': 1, 'Score': 1, 'Validate': 1, 'PyICU': 1, '9': 2, 'Article': 1, '100': 4, '0.6155': 1, 'Encyclopedia': 1, '0.6932': 1, '0.5987': 1, 'Novel': 1, '0.6800': 1, 'Lexto': 1, '10': 1, '0.7267': 1, '0.7709': 1, '0.6994': 1, '0.7701': 1, 'wordcutpy': 1, '11': 1, '0.9322': 1, '0.6212': 1, '0.9299': 1, '0.6286': 1, '0.8987': 1, '0.6571': 1, '0.7140': 1, '0.6247': 1, 'cunlp': 1, '12': 1, '0.6910': 1, '0.6172': 1, '0.5748': 1, '0.0000': 1, 'SWATH': 1, '13': 5, '0.6347': 1, '0.6858': 1, '0.6200': 1, '0.6867': 1, '3.1': 1, '14': 1, 'reduces': 1, 'dimensionality': 1, 'creates': 1, 'applies': 1, 'singular': 3, 'value': 6, 'decomposition': 1, 'SVD': 13, '15': 3, 'finds': 1, 'closely': 2, 'related': 3, 'terms': 2, 'separated': 1, 'matrices': 5, 'U': 1, 'm': 1, 'x': 7, 'r': 4, 'concept': 12, 'V': 2, 'n': 3, 'Î£': 3, 'diagonal': 1, 'reconstructed': 1, 'represented': 2, 'Eq': 11, '3.2': 1, 'â‰ˆ': 1, 'ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰': 1, 'ğ‘‡ğ‘‡': 3, 'sum': 1, '3.3': 1, 'ğ‘Šğ‘Šğ‘Šğ‘Š': 2, 'Factors': 1, 'W': 1, 'H': 1, 'solving': 1, 'optimization': 1, 'follows': 1, 'whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘—': 1, 'â‰¥': 2, '0': 27, 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–': 6, 'ğ‘šğ‘š': 2, 'ğ‘›ğ‘›': 2, 'ğ‘Ÿğ‘Ÿ': 5, 'ğ‘—ğ‘—=1': 1, 'ğ‘–ğ‘–=1': 2, 'ğ‘™ğ‘™=1': 1, 'ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š': 1, 'ğ¹ğ¹(ğ‘Šğ‘Š': 1, 'ğ»ğ»': 2, '||': 1, 'ğ´ğ´': 4, 'âˆ’': 2, '||2ğ¹ğ¹': 1, 'ï¿½': 8, 'ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘–': 1, 'ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘–': 1, 'algorithms': 1, 'return': 1, 'nonnegative': 3, 'zeroes': 1, 'gets': 1, 'wider': 1, 'distribution': 1, 'feature': 4, 'represents': 4, 'root': 1, 'example': 2, 'man': 1, 'male': 1, 'adult': 1, 'close': 2, 'dataset': 2, 'calculating': 2, 'order': 3, 'select': 4, 'representative': 2, '3.4': 1, 'number': 7, 'ğ‘–ğ‘–': 2, 'ğ‘—ğ‘—.': 1, 'jth': 1, 'ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘–': 3, 'Document': 2, 'Gong': 6, 'Y.': 6, '16': 3, 'applying': 1, 'ğ‘‰ğ‘‰': 2, 'cell': 2, 'starting': 1, 'total': 1, 'equal': 1, 'detected': 1, 'Murray': 8, 'G.': 8, '17': 1, 'authors': 1, 'collected': 4, 'decision': 1, 'depending': 1, 'decided': 1, 'getting': 1, 'percentage': 1, 'subject': 1, 'constraint': 1, 'model': 3, 'decomposes': 1, 'âˆˆ': 3, 'ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š': 2, 'NSFM': 1, 'ğ‘Šğ‘Š': 2, 'variable': 1, 'NSVM': 1, 'ğ»ğ».': 1, 'ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ': 1, 'shown': 1, 'âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1': 2, 'âˆ‘ğ‘ğ‘=1': 1, 'ğ»ğ»ğ‘ğ‘ğ‘ğ‘': 2, 'relative': 1, 'relevance': 1, 'ith': 1, 'ğ‘Šğ‘Šğ‘–ğ‘–': 1, 'ğ‘ğ‘': 1, 'ğ‘ğ‘': 1, 'ğ‘ğ‘.': 1, 'scores': 4, 'Sentences': 4, 'maximum': 2, '3.5': 1, 'Cosine': 2, 'Similarity': 1, '18': 1, 'widely': 1, 'vectors': 6, 'representing': 1, 'ranging': 1, 'closer': 1, 'similar': 3, 'equation': 1, 'cos(Î¸': 2, 'dot': 1, 'product': 2, 'B': 2, 'lengths': 1, 'deployed': 1, 'Aâˆ™B': 2, '||A||': 2, '||B||': 2, 'âˆ‘ni=1': 1, 'Ai': 1, 'Bi': 1, 'Similarity(A': 1, 'Clustering': 1, '67': 1, '55': 1, '38': 1, 'average': 1, 'numbers': 1, 'sets': 4, '21': 3, 'respectively': 2, 'PIPELINE': 1, 'GENERATING': 1, 'SUMMARIES': 1, 'section': 1, 'demonstrate': 1, 'pipeline': 2, 'Figure': 4, 'generate': 1, 'Word': 2, 'S9': 1, 'Round': 5, 'Example': 1, 'S8': 1, 'Avg': 1, 'Number': 3, 'S7': 1, 'Min': 1, 'S6': 1, 'Max': 1, '58': 2, 'Dataset': 2, 'S5': 1, 'Overall': 1, 'S4': 1, 'DATA': 1, 'PREPARATION': 1, 'standard': 2, 'unavailable': 1, 'evaluating': 1, '400': 3, 'Thairath': 2, 'Manager': 2, 'online': 2, 'newspapers': 2, 'datasets': 1, 'split': 1, '80': 3, 'performance': 1, 'comparing': 1, 'summaries': 1, 'manually': 1, 'curated': 1, 'experts': 1, 'Faculty': 1, 'Liberal': 1, 'Arts': 1, 'Ubon': 1, 'Ratchathani': 1, 'open': 1, 'source': 2, 'python': 3, 'libraries': 1, 'numpy': 1, '19': 1, 'sklearn': 1, 'converted': 1, 'obtained': 1, 'plain': 1, 'segmented': 1, 'following': 2, 'format': 1, 'Si': 2, 'â€˜': 2, 'xxx': 2, 'removing': 1, 'stop': 1, 'duplicate': 1, 'built': 1, 'modules': 1, 'numpy.linalg.svd': 1, 'sklearn.decomposition': 1, 'module': 1, 'sklearn.cluster': 1, 'performed': 1, 'meant': 1, '70': 1, 'S3': 1, 'S2': 1, 'grouped': 1, 'cluster': 1, 'Randomly': 1, 'groups': 1, 'Calculate': 1, 'centroid': 4, 'vector': 1, 'ğ»ğ»ğ‘‡ğ‘‡': 1, 'Use': 1, 'assign': 1, 'Repeat': 1, 'assigned': 1, 'change': 1, 'Select': 1, 'add': 1, 'S1': 1, '3.6': 1, 'âˆ‘i=1': 2, 'A2i': 1, 'Bi2': 1, 'Mr.': 1, 'Yontas': 1, 'ak': 1, 'Supason': 1, 'Tourism': 1, 'Authority': 1, 'demonstrates': 1, 'constructed': 1, 'count': 1, 'composed': 1, '98': 1, 'multiple': 1, 'outperformed': 1, 'EXPERIMENT': 1, 'RESULTS': 1, '6.1': 1, 'Performance': 1, 'Evaluations': 1, 'Measure': 1, 'accuracy': 1, 'measurements': 1, 'quantify': 1, 'differences': 1, 'experimental': 2, 'correctness': 1, 'reflects': 1, 'good': 1, 'missed': 1, 'Experiment': 1, 'Results': 1, 'set': 1, 'like': 2, 'explore': 1, 'different': 3, 'affected': 1, 'efficiency': 3, '0.83': 2, '0.72': 1, '0.62': 1, '0.74': 1, '0.64': 1, '0.86': 1, '0.78': 1, '0.68': 1, 'varied': 1, 'determined': 1, 'importance': 1, 'hand': 1, 'include': 1, 'CONCLUSIONS': 1, 'humans': 1, 'future': 1, 'plan': 1, 'perform': 1, 'types': 1, 'handled': 1, 'ACKNOWLEDGMENTS': 1, 'thank': 1, 'department': 1, 'computer': 1, 'engineering': 2, 'faculty': 1, 'providing': 1, 'computing': 1, 'facilities': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency = max(word_frequencies.values())"
      ],
      "metadata": {
        "id": "I7ixvA1BkOQO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe5vacsFocIl",
        "outputId": "d0e4272d-a846-4eed-e034-fa30f7c286a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency"
      ],
      "metadata": {
        "id": "N2-sI3cOoubS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9za-mtsouyF",
        "outputId": "d0690634-7975-4801-bfc1-100c6689cef1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Extractive': 0.005714285714285714, 'Text': 0.04, 'Summarization': 0.017142857142857144, 'Thai': 0.2, 'Travel': 0.02857142857142857, 'News': 0.03428571428571429, 'Based': 0.011428571428571429, 'Keyword': 0.005714285714285714, 'Scored': 0.005714285714285714, 'Language': 0.011428571428571429, 'Sarunya': 0.005714285714285714, 'Nathonghor': 0.005714285714285714, '\\n\\n': 1.0, 'Duangdao': 0.005714285714285714, 'Wichadakul': 0.005714285714285714, 'Department': 0.011428571428571429, 'Computer': 0.011428571428571429, 'Engineering': 0.011428571428571429, 'Chulalongkorn': 0.017142857142857144, 'University': 0.022857142857142857, 'Bangkok': 0.011428571428571429, 'Thailand': 0.017142857142857144, 'Sarunya.N@Student.Chula.ac.th': 0.005714285714285714, 'ABSTRACT': 0.005714285714285714, 'recent': 0.017142857142857144, 'years': 0.011428571428571429, 'people': 0.005714285714285714, 'seeking': 0.005714285714285714, 'solution': 0.005714285714285714, 'improve': 0.011428571428571429, 'text': 0.09714285714285714, 'summarization': 0.21142857142857144, 'language': 0.03428571428571429, 'solutions': 0.005714285714285714, 'PageRank': 0.011428571428571429, 'Graph': 0.005714285714285714, 'Rank': 0.005714285714285714, 'Latent': 0.02857142857142857, 'Semantic': 0.02857142857142857, 'Analysis': 0.02857142857142857, 'LSA': 0.10285714285714286, 'models': 0.03428571428571429, 'etc': 0.005714285714285714, 'proposed': 0.08571428571428572, 'research': 0.017142857142857144, 'results': 0.022857142857142857, 'restricted': 0.005714285714285714, 'limited': 0.005714285714285714, 'corpus': 0.022857142857142857, 'complex': 0.005714285714285714, 'grammar': 0.011428571428571429, 'paper': 0.04, 'applied': 0.04, 'system': 0.022857142857142857, 'travel': 0.05714285714285714, 'news': 0.10285714285714286, 'based': 0.09714285714285714, 'keyword': 0.05714285714285714, 'scored': 0.022857142857142857, 'extracting': 0.02857142857142857, 'relevant': 0.017142857142857144, 'sentences': 0.24571428571428572, 'original': 0.045714285714285714, 'document': 0.10285714285714286, 'compared': 0.03428571428571429, 'Non': 0.02857142857142857, 'negative': 0.06857142857142857, 'Matrix': 0.03428571428571429, 'Factorization': 0.02857142857142857, 'NMF': 0.1657142857142857, 'find': 0.03428571428571429, 'algorithm': 0.045714285714285714, 'suitable': 0.017142857142857144, 'compression': 0.06285714285714286, 'rates': 0.02857142857142857, 'Generic': 0.05142857142857143, 'Sentence': 0.05142857142857143, 'Relevance': 0.045714285714285714, 'score': 0.14857142857142858, 'GRS': 0.02857142857142857, 'K': 0.08571428571428572, 'means': 0.07428571428571429, 'clustering': 0.05142857142857143, 'evaluated': 0.017142857142857144, 'experiments': 0.022857142857142857, 'concluded': 0.011428571428571429, 'calculation': 0.02857142857142857, 'sentence': 0.18285714285714286, 'selection': 0.045714285714285714, 'best': 0.045714285714285714, 'summarizing': 0.022857142857142857, 'human': 0.022857142857142857, 'rate': 0.03428571428571429, '20': 0.05142857142857143, 'CCS': 0.005714285714285714, 'Concepts': 0.005714285714285714, 'â€¢': 0.005714285714285714, 'Information': 0.011428571428571429, 'systems': 0.005714285714285714, 'â': 0.017142857142857144, 'retrieval': 0.005714285714285714, 'Retrieval': 0.005714285714285714, 'tasks': 0.005714285714285714, 'goals': 0.005714285714285714, 'Keywords': 0.005714285714285714, 'extractive': 0.022857142857142857, 'non': 0.02857142857142857, 'matrix': 0.14285714285714285, 'factorization': 0.017142857142857144, '1': 0.10857142857142857, 'INTRODUCTION': 0.005714285714285714, 'Daily': 0.005714285714285714, 'newspaper': 0.005714285714285714, 'abundant': 0.005714285714285714, 'data': 0.022857142857142857, 'users': 0.011428571428571429, 'time': 0.005714285714285714, 'reading': 0.005714285714285714, 'difficult': 0.005714285714285714, 'identify': 0.005714285714285714, 'information': 0.017142857142857144, 'satisfy': 0.005714285714285714, 'needed': 0.005714285714285714, 'Automatic': 0.005714285714285714, 'reduce': 0.005714285714285714, 'problem': 0.011428571428571429, 'overloading': 0.005714285714285714, 'previously': 0.005714285714285714, 'English': 0.005714285714285714, 'languages': 0.005714285714285714, 'lack': 0.011428571428571429, 'complicated': 0.005714285714285714, 'technique': 0.005714285714285714, 'content': 0.011428571428571429, 'documents': 0.02857142857142857, 'consists': 0.005714285714285714, 'steps': 0.017142857142857144, 'create': 0.011428571428571429, 'intermediate': 0.005714285714285714, 'representation': 0.005714285714285714, 'input': 0.005714285714285714, '2': 0.09714285714285714, 'calculate': 0.022857142857142857, 'concepts': 0.03428571428571429, '3': 0.06285714285714286, 'choose': 0.005714285714285714, 'important': 0.06285714285714286, 'Permission': 0.005714285714285714, 'digital': 0.005714285714285714, 'hard': 0.005714285714285714, 'copies': 0.017142857142857144, 'work': 0.022857142857142857, 'personal': 0.005714285714285714, 'classroom': 0.005714285714285714, 'use': 0.011428571428571429, 'granted': 0.005714285714285714, 'fee': 0.011428571428571429, 'provided': 0.011428571428571429, 'distributed': 0.005714285714285714, 'profit': 0.005714285714285714, 'commercial': 0.005714285714285714, 'advantage': 0.005714285714285714, 'bear': 0.005714285714285714, 'notice': 0.005714285714285714, 'citation': 0.005714285714285714, 'page': 0.005714285714285714, 'Copyrights': 0.005714285714285714, 'components': 0.011428571428571429, 'owned': 0.005714285714285714, 'ACM': 0.011428571428571429, 'honored': 0.005714285714285714, 'Abstracting': 0.005714285714285714, 'credit': 0.005714285714285714, 'permitted': 0.005714285714285714, 'copy': 0.005714285714285714, 'republish': 0.005714285714285714, 'post': 0.005714285714285714, 'servers': 0.005714285714285714, 'redistribute': 0.005714285714285714, 'lists': 0.005714285714285714, 'requires': 0.005714285714285714, 'prior': 0.005714285714285714, 'specific': 0.011428571428571429, 'permission': 0.005714285714285714, 'and/or': 0.005714285714285714, 'Request': 0.005714285714285714, 'permissions': 0.005714285714285714, 'Permissions@acm.org': 0.005714285714285714, 'ITCC': 0.005714285714285714, '2020': 0.017142857142857144, 'August': 0.005714285714285714, '12â€“14': 0.005714285714285714, 'Kuala': 0.005714285714285714, 'Lumpur': 0.005714285714285714, 'Malaysia': 0.005714285714285714, 'Â©': 0.005714285714285714, 'Association': 0.005714285714285714, 'Computing': 0.005714285714285714, 'Machinery': 0.005714285714285714, 'ISBN': 0.005714285714285714, '978': 0.005714285714285714, '4503': 0.005714285714285714, '7539': 0.005714285714285714, '9/20/08': 0.005714285714285714, 'â€¦': 0.06285714285714286, '$15.00': 0.005714285714285714, 'DOI': 0.005714285714285714, 'https://doi.org/10.1145/3417473.3417479': 0.005714285714285714, 'Duangdao.W@Chula.ac.th': 0.005714285714285714, 'included': 0.005714285714285714, 'summary': 0.08, 'divided': 0.017142857142857144, 'approaches': 0.017142857142857144, 'approach': 0.02857142857142857, 'relies': 0.005714285714285714, 'method': 0.06857142857142857, 'words': 0.045714285714285714, 'searching': 0.005714285714285714, 'keywords': 0.017142857142857144, 'second': 0.011428571428571429, 'abstractive': 0.005714285714285714, 'analyzes': 0.005714285714285714, 'linguistic': 0.005714285714285714, 'principles': 0.005714285714285714, 'transcription': 0.005714285714285714, 'interpretation': 0.005714285714285714, 'implies': 0.005714285714285714, 'effective': 0.005714285714285714, 'accurate': 0.005714285714285714, 'methods': 0.03428571428571429, 'chose': 0.005714285714285714, 'apply': 0.005714285714285714, 'focused': 0.005714285714285714, 'extraction': 0.005714285714285714, 'function': 0.005714285714285714, 'selecting': 0.011428571428571429, 'calculated': 0.04, 'tried': 0.005714285714285714, 'experiment': 0.017142857142857144, '5': 0.09142857142857143, 'rounds': 0.005714285714285714, '30': 0.02857142857142857, '40': 0.02857142857142857, 'reported': 0.005714285714285714, 'produced': 0.005714285714285714, 'result': 0.017142857142857144, 'RELATED': 0.005714285714285714, 'WORKS': 0.005714285714285714, 'introduced': 0.005714285714285714, 'Suwanno': 0.005714285714285714, 'N.': 0.005714285714285714, 'et': 0.12, 'al': 0.12, 'extracted': 0.03428571428571429, 'paragraph': 0.005714285714285714, 'compound': 0.011428571428571429, 'nouns': 0.011428571428571429, 'term': 0.022857142857142857, 'frequency': 0.017142857142857144, 'headline': 0.005714285714285714, 'generating': 0.005714285714285714, 'Chongsuntornsri': 0.005714285714285714, 'A.': 0.011428571428571429, 'new': 0.005714285714285714, 'content-': 0.005714285714285714, 'graph': 0.005714285714285714, 'Topic': 0.005714285714285714, 'Sensitive': 0.005714285714285714, 'ranking': 0.005714285714285714, 'segments': 0.005714285714285714, 'Jaruskulchai': 0.005714285714285714, 'C.': 0.005714285714285714, '4': 0.04, 'summarize': 0.005714285714285714, 'combining': 0.005714285714285714, 'properties': 0.017142857142857144, 'Local': 0.005714285714285714, 'Property': 0.011428571428571429, 'overall': 0.017142857142857144, 'Global': 0.005714285714285714, 'relationship': 0.022857142857142857, 'industrial': 0.005714285714285714, 'got': 0.04, '60': 0.011428571428571429, 'precision': 0.03428571428571429, '44': 0.005714285714285714, 'recall': 0.03428571428571429, '50.9': 0.005714285714285714, 'F': 0.022857142857142857, 'measure': 0.03428571428571429, 'general': 0.005714285714285714, '51.8': 0.005714285714285714, '38.5': 0.005714285714285714, '43.1': 0.005714285714285714, 'fashion': 0.005714285714285714, 'magazines': 0.005714285714285714, '53.0': 0.005714285714285714, '33.0': 0.005714285714285714, '40.4': 0.005714285714285714, 'Mani': 0.005714285714285714, 'I.': 0.005714285714285714, 'techniques': 0.005714285714285714, 'word': 0.06857142857142857, 'weight': 0.05142857142857143, 'group': 0.05142857142857143, 'cosine': 0.02857142857142857, 'similarity': 0.06857142857142857, 'researcher': 0.005714285714285714, 'search': 0.005714285714285714, 'shortest': 0.005714285714285714, 'sequence': 0.011428571428571429, 'topic': 0.022857142857142857, 'segmentation': 0.017142857142857144, 'grouping': 0.005714285714285714, 'main': 0.005714285714285714, 'selected': 0.05714285714285714, 'agricultural': 0.005714285714285714, '68.57': 0.005714285714285714, '51.95': 0.005714285714285714, '56.72': 0.005714285714285714, 'Lee': 0.017142857142857144, 'J.': 0.017142857142857144, '6': 0.03428571428571429, '\\n\\n\\x0c': 0.017142857142857144, 'summation': 0.005714285714285714, 'weights': 0.017142857142857144, 'ranked': 0.011428571428571429, 'summed': 0.005714285714285714, 'found': 0.017142857142857144, 'zero': 0.005714285714285714, 'values': 0.04, 'positive': 0.011428571428571429, 'scope': 0.005714285714285714, 'semantic': 0.05714285714285714, 'features': 0.005714285714285714, 'â€™': 0.017142857142857144, 'meaning': 0.011428571428571429, 'narrow': 0.005714285714285714, 'greater': 0.005714285714285714, 'possibility': 0.005714285714285714, 'PREPROCESSING': 0.005714285714285714, 'THAI': 0.005714285714285714, 'TEXT': 0.005714285714285714, 'step': 0.011428571428571429, 'working': 0.005714285714285714, 'tokenization': 0.017142857142857144, 'writing': 0.005714285714285714, 'delimiters': 0.005714285714285714, 'indicate': 0.005714285714285714, 'boundaries': 0.005714285714285714, 'rules': 0.005714285714285714, 'programs': 0.017142857142857144, 'Table': 0.03428571428571429, 'shows': 0.02857142857142857, 'F1': 0.045714285714285714, 'trained': 0.005714285714285714, 'tested': 0.005714285714285714, 'laboratory': 0.005714285714285714, 'members': 0.005714285714285714, 'BEST2010': 0.005714285714285714, '7': 0.045714285714285714, 'Cutkum': 0.022857142857142857, '8': 0.011428571428571429, 'highest': 0.05142857142857143, 'Comparison': 0.005714285714285714, 'Tools': 0.005714285714285714, 'Score': 0.005714285714285714, 'Validate': 0.005714285714285714, 'PyICU': 0.005714285714285714, '9': 0.011428571428571429, 'Article': 0.005714285714285714, '100': 0.022857142857142857, '0.6155': 0.005714285714285714, 'Encyclopedia': 0.005714285714285714, '0.6932': 0.005714285714285714, '0.5987': 0.005714285714285714, 'Novel': 0.005714285714285714, '0.6800': 0.005714285714285714, 'Lexto': 0.005714285714285714, '10': 0.005714285714285714, '0.7267': 0.005714285714285714, '0.7709': 0.005714285714285714, '0.6994': 0.005714285714285714, '0.7701': 0.005714285714285714, 'wordcutpy': 0.005714285714285714, '11': 0.005714285714285714, '0.9322': 0.005714285714285714, '0.6212': 0.005714285714285714, '0.9299': 0.005714285714285714, '0.6286': 0.005714285714285714, '0.8987': 0.005714285714285714, '0.6571': 0.005714285714285714, '0.7140': 0.005714285714285714, '0.6247': 0.005714285714285714, 'cunlp': 0.005714285714285714, '12': 0.005714285714285714, '0.6910': 0.005714285714285714, '0.6172': 0.005714285714285714, '0.5748': 0.005714285714285714, '0.0000': 0.005714285714285714, 'SWATH': 0.005714285714285714, '13': 0.02857142857142857, '0.6347': 0.005714285714285714, '0.6858': 0.005714285714285714, '0.6200': 0.005714285714285714, '0.6867': 0.005714285714285714, '3.1': 0.005714285714285714, '14': 0.005714285714285714, 'reduces': 0.005714285714285714, 'dimensionality': 0.005714285714285714, 'creates': 0.005714285714285714, 'applies': 0.005714285714285714, 'singular': 0.017142857142857144, 'value': 0.03428571428571429, 'decomposition': 0.005714285714285714, 'SVD': 0.07428571428571429, '15': 0.017142857142857144, 'finds': 0.005714285714285714, 'closely': 0.011428571428571429, 'related': 0.017142857142857144, 'terms': 0.011428571428571429, 'separated': 0.005714285714285714, 'matrices': 0.02857142857142857, 'U': 0.005714285714285714, 'm': 0.005714285714285714, 'x': 0.04, 'r': 0.022857142857142857, 'concept': 0.06857142857142857, 'V': 0.011428571428571429, 'n': 0.017142857142857144, 'Î£': 0.017142857142857144, 'diagonal': 0.005714285714285714, 'reconstructed': 0.005714285714285714, 'represented': 0.011428571428571429, 'Eq': 0.06285714285714286, '3.2': 0.005714285714285714, 'â‰ˆ': 0.005714285714285714, 'ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰': 0.005714285714285714, 'ğ‘‡ğ‘‡': 0.017142857142857144, 'sum': 0.005714285714285714, '3.3': 0.005714285714285714, 'ğ‘Šğ‘Šğ‘Šğ‘Š': 0.011428571428571429, 'Factors': 0.005714285714285714, 'W': 0.005714285714285714, 'H': 0.005714285714285714, 'solving': 0.005714285714285714, 'optimization': 0.005714285714285714, 'follows': 0.005714285714285714, 'whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘—': 0.005714285714285714, 'â‰¥': 0.011428571428571429, '0': 0.15428571428571428, 'ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–': 0.03428571428571429, 'ğ‘šğ‘š': 0.011428571428571429, 'ğ‘›ğ‘›': 0.011428571428571429, 'ğ‘Ÿğ‘Ÿ': 0.02857142857142857, 'ğ‘—ğ‘—=1': 0.005714285714285714, 'ğ‘–ğ‘–=1': 0.011428571428571429, 'ğ‘™ğ‘™=1': 0.005714285714285714, 'ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š': 0.005714285714285714, 'ğ¹ğ¹(ğ‘Šğ‘Š': 0.005714285714285714, 'ğ»ğ»': 0.011428571428571429, '||': 0.005714285714285714, 'ğ´ğ´': 0.022857142857142857, 'âˆ’': 0.011428571428571429, '||2ğ¹ğ¹': 0.005714285714285714, 'ï¿½': 0.045714285714285714, 'ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘–': 0.005714285714285714, 'ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘–': 0.005714285714285714, 'algorithms': 0.005714285714285714, 'return': 0.005714285714285714, 'nonnegative': 0.017142857142857144, 'zeroes': 0.005714285714285714, 'gets': 0.005714285714285714, 'wider': 0.005714285714285714, 'distribution': 0.005714285714285714, 'feature': 0.022857142857142857, 'represents': 0.022857142857142857, 'root': 0.005714285714285714, 'example': 0.011428571428571429, 'man': 0.005714285714285714, 'male': 0.005714285714285714, 'adult': 0.005714285714285714, 'close': 0.011428571428571429, 'dataset': 0.011428571428571429, 'calculating': 0.011428571428571429, 'order': 0.017142857142857144, 'select': 0.022857142857142857, 'representative': 0.011428571428571429, '3.4': 0.005714285714285714, 'number': 0.04, 'ğ‘–ğ‘–': 0.011428571428571429, 'ğ‘—ğ‘—.': 0.005714285714285714, 'jth': 0.005714285714285714, 'ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘–': 0.017142857142857144, 'Document': 0.011428571428571429, 'Gong': 0.03428571428571429, 'Y.': 0.03428571428571429, '16': 0.017142857142857144, 'applying': 0.005714285714285714, 'ğ‘‰ğ‘‰': 0.011428571428571429, 'cell': 0.011428571428571429, 'starting': 0.005714285714285714, 'total': 0.005714285714285714, 'equal': 0.005714285714285714, 'detected': 0.005714285714285714, 'Murray': 0.045714285714285714, 'G.': 0.045714285714285714, '17': 0.005714285714285714, 'authors': 0.005714285714285714, 'collected': 0.022857142857142857, 'decision': 0.005714285714285714, 'depending': 0.005714285714285714, 'decided': 0.005714285714285714, 'getting': 0.005714285714285714, 'percentage': 0.005714285714285714, 'subject': 0.005714285714285714, 'constraint': 0.005714285714285714, 'model': 0.017142857142857144, 'decomposes': 0.005714285714285714, 'âˆˆ': 0.017142857142857144, 'ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š': 0.011428571428571429, 'NSFM': 0.005714285714285714, 'ğ‘Šğ‘Š': 0.011428571428571429, 'variable': 0.005714285714285714, 'NSVM': 0.005714285714285714, 'ğ»ğ».': 0.005714285714285714, 'ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ': 0.005714285714285714, 'shown': 0.005714285714285714, 'âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1': 0.011428571428571429, 'âˆ‘ğ‘ğ‘=1': 0.005714285714285714, 'ğ»ğ»ğ‘ğ‘ğ‘ğ‘': 0.011428571428571429, 'relative': 0.005714285714285714, 'relevance': 0.005714285714285714, 'ith': 0.005714285714285714, 'ğ‘Šğ‘Šğ‘–ğ‘–': 0.005714285714285714, 'ğ‘ğ‘': 0.005714285714285714, 'ğ‘ğ‘': 0.005714285714285714, 'ğ‘ğ‘.': 0.005714285714285714, 'scores': 0.022857142857142857, 'Sentences': 0.022857142857142857, 'maximum': 0.011428571428571429, '3.5': 0.005714285714285714, 'Cosine': 0.011428571428571429, 'Similarity': 0.005714285714285714, '18': 0.005714285714285714, 'widely': 0.005714285714285714, 'vectors': 0.03428571428571429, 'representing': 0.005714285714285714, 'ranging': 0.005714285714285714, 'closer': 0.005714285714285714, 'similar': 0.017142857142857144, 'equation': 0.005714285714285714, 'cos(Î¸': 0.011428571428571429, 'dot': 0.005714285714285714, 'product': 0.011428571428571429, 'B': 0.011428571428571429, 'lengths': 0.005714285714285714, 'deployed': 0.005714285714285714, 'Aâˆ™B': 0.011428571428571429, '||A||': 0.011428571428571429, '||B||': 0.011428571428571429, 'âˆ‘ni=1': 0.005714285714285714, 'Ai': 0.005714285714285714, 'Bi': 0.005714285714285714, 'Similarity(A': 0.005714285714285714, 'Clustering': 0.005714285714285714, '67': 0.005714285714285714, '55': 0.005714285714285714, '38': 0.005714285714285714, 'average': 0.005714285714285714, 'numbers': 0.005714285714285714, 'sets': 0.022857142857142857, '21': 0.017142857142857144, 'respectively': 0.011428571428571429, 'PIPELINE': 0.005714285714285714, 'GENERATING': 0.005714285714285714, 'SUMMARIES': 0.005714285714285714, 'section': 0.005714285714285714, 'demonstrate': 0.005714285714285714, 'pipeline': 0.011428571428571429, 'Figure': 0.022857142857142857, 'generate': 0.005714285714285714, 'Word': 0.011428571428571429, 'S9': 0.005714285714285714, 'Round': 0.02857142857142857, 'Example': 0.005714285714285714, 'S8': 0.005714285714285714, 'Avg': 0.005714285714285714, 'Number': 0.017142857142857144, 'S7': 0.005714285714285714, 'Min': 0.005714285714285714, 'S6': 0.005714285714285714, 'Max': 0.005714285714285714, '58': 0.011428571428571429, 'Dataset': 0.011428571428571429, 'S5': 0.005714285714285714, 'Overall': 0.005714285714285714, 'S4': 0.005714285714285714, 'DATA': 0.005714285714285714, 'PREPARATION': 0.005714285714285714, 'standard': 0.011428571428571429, 'unavailable': 0.005714285714285714, 'evaluating': 0.005714285714285714, '400': 0.017142857142857144, 'Thairath': 0.011428571428571429, 'Manager': 0.011428571428571429, 'online': 0.011428571428571429, 'newspapers': 0.011428571428571429, 'datasets': 0.005714285714285714, 'split': 0.005714285714285714, '80': 0.017142857142857144, 'performance': 0.005714285714285714, 'comparing': 0.005714285714285714, 'summaries': 0.005714285714285714, 'manually': 0.005714285714285714, 'curated': 0.005714285714285714, 'experts': 0.005714285714285714, 'Faculty': 0.005714285714285714, 'Liberal': 0.005714285714285714, 'Arts': 0.005714285714285714, 'Ubon': 0.005714285714285714, 'Ratchathani': 0.005714285714285714, 'open': 0.005714285714285714, 'source': 0.011428571428571429, 'python': 0.017142857142857144, 'libraries': 0.005714285714285714, 'numpy': 0.005714285714285714, '19': 0.005714285714285714, 'sklearn': 0.005714285714285714, 'converted': 0.005714285714285714, 'obtained': 0.005714285714285714, 'plain': 0.005714285714285714, 'segmented': 0.005714285714285714, 'following': 0.011428571428571429, 'format': 0.005714285714285714, 'Si': 0.011428571428571429, 'â€˜': 0.011428571428571429, 'xxx': 0.011428571428571429, 'removing': 0.005714285714285714, 'stop': 0.005714285714285714, 'duplicate': 0.005714285714285714, 'built': 0.005714285714285714, 'modules': 0.005714285714285714, 'numpy.linalg.svd': 0.005714285714285714, 'sklearn.decomposition': 0.005714285714285714, 'module': 0.005714285714285714, 'sklearn.cluster': 0.005714285714285714, 'performed': 0.005714285714285714, 'meant': 0.005714285714285714, '70': 0.005714285714285714, 'S3': 0.005714285714285714, 'S2': 0.005714285714285714, 'grouped': 0.005714285714285714, 'cluster': 0.005714285714285714, 'Randomly': 0.005714285714285714, 'groups': 0.005714285714285714, 'Calculate': 0.005714285714285714, 'centroid': 0.022857142857142857, 'vector': 0.005714285714285714, 'ğ»ğ»ğ‘‡ğ‘‡': 0.005714285714285714, 'Use': 0.005714285714285714, 'assign': 0.005714285714285714, 'Repeat': 0.005714285714285714, 'assigned': 0.005714285714285714, 'change': 0.005714285714285714, 'Select': 0.005714285714285714, 'add': 0.005714285714285714, 'S1': 0.005714285714285714, '3.6': 0.005714285714285714, 'âˆ‘i=1': 0.011428571428571429, 'A2i': 0.005714285714285714, 'Bi2': 0.005714285714285714, 'Mr.': 0.005714285714285714, 'Yontas': 0.005714285714285714, 'ak': 0.005714285714285714, 'Supason': 0.005714285714285714, 'Tourism': 0.005714285714285714, 'Authority': 0.005714285714285714, 'demonstrates': 0.005714285714285714, 'constructed': 0.005714285714285714, 'count': 0.005714285714285714, 'composed': 0.005714285714285714, '98': 0.005714285714285714, 'multiple': 0.005714285714285714, 'outperformed': 0.005714285714285714, 'EXPERIMENT': 0.005714285714285714, 'RESULTS': 0.005714285714285714, '6.1': 0.005714285714285714, 'Performance': 0.005714285714285714, 'Evaluations': 0.005714285714285714, 'Measure': 0.005714285714285714, 'accuracy': 0.005714285714285714, 'measurements': 0.005714285714285714, 'quantify': 0.005714285714285714, 'differences': 0.005714285714285714, 'experimental': 0.011428571428571429, 'correctness': 0.005714285714285714, 'reflects': 0.005714285714285714, 'good': 0.005714285714285714, 'missed': 0.005714285714285714, 'Experiment': 0.005714285714285714, 'Results': 0.005714285714285714, 'set': 0.005714285714285714, 'like': 0.011428571428571429, 'explore': 0.005714285714285714, 'different': 0.017142857142857144, 'affected': 0.005714285714285714, 'efficiency': 0.017142857142857144, '0.83': 0.011428571428571429, '0.72': 0.005714285714285714, '0.62': 0.005714285714285714, '0.74': 0.005714285714285714, '0.64': 0.005714285714285714, '0.86': 0.005714285714285714, '0.78': 0.005714285714285714, '0.68': 0.005714285714285714, 'varied': 0.005714285714285714, 'determined': 0.005714285714285714, 'importance': 0.005714285714285714, 'hand': 0.005714285714285714, 'include': 0.005714285714285714, 'CONCLUSIONS': 0.005714285714285714, 'humans': 0.005714285714285714, 'future': 0.005714285714285714, 'plan': 0.005714285714285714, 'perform': 0.005714285714285714, 'types': 0.005714285714285714, 'handled': 0.005714285714285714, 'ACKNOWLEDGMENTS': 0.005714285714285714, 'thank': 0.005714285714285714, 'department': 0.005714285714285714, 'computer': 0.005714285714285714, 'engineering': 0.011428571428571429, 'faculty': 0.005714285714285714, 'providing': 0.005714285714285714, 'computing': 0.005714285714285714, 'facilities': 0.005714285714285714}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-_T2sn6o1Sa",
        "outputId": "4d6e833d-6093-4c43-d47d-d339d08dbfc1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Extractive Text Summarization for Thai Travel News\n",
            "Based on Keyword Scored in Thai Language\n",
            "Sarunya Nathonghor\n",
            "\n",
            "Duangdao Wichadakul\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Sarunya.N@Student.Chula.ac.th\n",
            "ABSTRACT\n",
            "\n",
            "In recent years, people are seeking for a solution to improve text\n",
            "summarization for Thai language., Although several solutions such\n",
            "as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
            "models, etc., have been proposed, research results in Thai text\n",
            "summarization were restricted due to limited corpus in Thai\n",
            "language with complex grammar., This paper applied a text\n",
            "summarization system for Thai travel news based on keyword\n",
            "scored in Thai language by extracting the most relevant sentences\n",
            "from the original document., We compared LSA and Non-negative\n",
            "Matrix Factorization (NMF) to find the algorithm that is suitable\n",
            "with Thai travel news., The suitable compression rates for Generic\n",
            "Sentence Relevance score (GRS) and K-means clustering were also\n",
            "evaluated., From these experiments, we concluded that keyword\n",
            "scored calculation by LSA with sentence selection by GRS is the\n",
            "best algorithm for summarizing Thai Travel News, compared with\n",
            "human with the best compression rate of 20%.\n",
            "\n",
            ", CCS Concepts\n",
            "\n",
            "â€¢ Information systems â Information retrieval â Retrieval\n",
            "tasks and goalsâ Summarization\n",
            "\n",
            "Keywords\n",
            "\n",
            "Text summarization; extractive summarization; non-negative\n",
            "matrix factorization\n",
            "\n",
            "1.\n",
            "\n",
            "INTRODUCTION\n",
            "\n",
            "Daily newspaper has abundant of data that users do not have\n",
            "enough time for reading them., It is difficult to identify the relevant\n",
            "information to satisfy the information needed by users., Automatic\n",
            "summarization can reduce the problem of information overloading\n",
            "and it has been proposed previously in English and other languages.\n",
            ", However, there were only a few research results in Thai text\n",
            "summarization due to the lack of corpus in Thai language and the\n",
            "complicated grammar.\n",
            ", Text Summarization, [1] is a technique for summarizing the content\n",
            "of the documents., It consists of three steps: 1) create an\n",
            "intermediate representation of the input text, 2) calculate score for\n",
            "the sentences based on the concepts, and 3) choose important\n",
            "Permission to make digital or hard copies of all or part of this work for\n",
            "personal or classroom use is granted without fee provided that copies are\n",
            "not made or distributed for profit or commercial advantage and that copies\n",
            "bear this notice and the full citation on the first page., Copyrights for\n",
            "components of this work owned by others than ACM must be honored.\n",
            ", Abstracting with credit is permitted., To copy otherwise, or republish, to\n",
            "post on servers or to redistribute to lists, requires prior specific permission\n",
            "and/or a fee., Request permissions from Permissions@acm.org.\n",
            ", ITCC 2020, August 12â€“14, 2020, Kuala Lumpur, Malaysia\n",
            "Â© 2020 Association for Computing Machinery.\n",
            ", ACM ISBN 978-1-4503-7539-9/20/08â€¦$15.00\n",
            "\n",
            "DOI: https://doi.org/10.1145/3417473.3417479\n",
            "\n",
            "Duangdao.W@Chula.ac.th\n",
            "\n",
            "sentences to be included in the summary., Text summarization can\n",
            "be divided into 2 approaches., The first approach is the extractive\n",
            "summarization, which relies on a method for extracting words and\n",
            "searching for keywords from the original document., The second\n",
            "approach is the abstractive summarization, which analyzes words\n",
            "by linguistic principles with transcription or interpretation from the\n",
            "original document., This approach implies more effective and\n",
            "accurate summary than the extractive methods., However, with the\n",
            "lack of Thai corpus, we chose to apply an extractive summarization\n",
            "method for Thai text summarization.\n",
            ", This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF)., We also tried using K-means clustering for\n",
            "document summarization., In this experiment, we compared 5\n",
            "models for 5 rounds with Thai travel news using the compression\n",
            "rates of 20%, 30% and 40% and reported the rate and method that\n",
            "produced the best result from the experiment.\n",
            "\n",
            ", 2.\n",
            "\n",
            "RELATED WORKS\n",
            "\n",
            ", In recent years, several models in Thai Text summarization have\n",
            "been introduced., Suwanno, N. et al., [2] proposed a Thai text\n",
            "summarization that extracted a paragraph from a document based\n",
            "on Thai compound nouns, term frequency method, and headline\n",
            "score for generating a summary., Chongsuntornsri, A., et al., [3]\n",
            "proposed a new approach for Text summarization in Thai based on\n",
            "content- and graph-based with the use of Topic Sensitive PageRank\n",
            "algorithm for summarizing and ranking of text segments.\n",
            ", Jaruskulchai C., et al., [4] proposed a method to summarize\n",
            "documents by extracting important sentences from combining the\n",
            "specific properties (Local Property) and the overall properties\n",
            "(Global Property) of the sentences., The overall properties were\n",
            "based on the relationship between sentences in the document., From\n",
            "their experiments, the summarization of the industrial news got\n",
            "60% precision, 44% recall, and 50.9% F-measure, the general news\n",
            "got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
            "the fashion magazines got 53.0% precision, 33.0% recall, and\n",
            "40.4% F-measure.\n",
            ", Mani, I., et al., [5] proposed techniques of text summarization by\n",
            "using word frequency in the document and calculated the weight of\n",
            "word to create a keyword group., They then calculated the cosine\n",
            "similarity of sentences., The researcher used A* search algorithm to\n",
            "find the shortest sequence of sentences from keyword group by\n",
            "topic calculation, sentence segmentation and word grouping., The\n",
            "sequence of sentences that were in the main group were selected as\n",
            "important sentences., Their summarization of the agricultural news\n",
            "got 68.57% precision, 51.95% recall and 56.72% F-measure.\n",
            ", Lee, J., et al., [6] proposed a document summarization method using\n",
            "Non-negative Matrix Factorization (NMF)., They compared\n",
            "\n",
            "\fbetween Latent Semantic Analysis (LSA) and NMF to find the\n",
            "weight of each word and calculated the summation of weights., The\n",
            "important sentences were ranked and selected into the summary\n",
            "based on their summed weight., Based on LSA, they found many\n",
            "weights with zero and negative values., However, when applied\n",
            "NMF, they found only the positive values and the scope of the\n",
            "semantic featuresâ€™ meaning was narrow., Therefore, they proposed\n",
            "that NMF provided a greater possibility for extracting important\n",
            "sentences.\n",
            "\n",
            ", 3.\n",
            "\n",
            "PREPROCESSING FOR THAI TEXT\n",
            "\n",
            "The first step for working with Thai Text is word tokenization., Even\n",
            "though Thai writing system has no delimiters to indicate word\n",
            "boundaries together with many rules for word segmentation, several\n",
            "Thai word tokenization programs have been proposed., Table 1\n",
            "shows F1 score of the recent programs trained and tested by one of\n",
            "our laboratory members with the data from BEST2010 corpus, [7].\n",
            ", Cutkum [8] got the highest F1 score, hence, we used Cutkum for this\n",
            "step.\n",
            ", Table 1., Comparison of Thai word tokenization programs\n",
            "Tools\n",
            "\n",
            "F1 Score\n",
            "\n",
            "Validate\n",
            "PyICU, [9]\n",
            "\n",
            "Article\n",
            "100\n",
            "0.6155\n",
            "\n",
            "Encyclopedia\n",
            "100\n",
            "0.6932\n",
            "\n",
            "News\n",
            "100\n",
            "0.5987\n",
            "\n",
            "Novel\n",
            "100\n",
            "0.6800\n",
            "\n",
            "Lexto, [10]\n",
            "\n",
            "0.7267\n",
            "\n",
            "0.7709\n",
            "\n",
            "0.6994\n",
            "\n",
            "0.7701\n",
            "\n",
            "Cutkum\n",
            "wordcutpy [11]\n",
            "\n",
            "0.9322\n",
            "0.6212\n",
            "\n",
            "0.9299\n",
            "0.6286\n",
            "\n",
            "0.8987\n",
            "0.6571\n",
            "\n",
            "0.7140\n",
            "0.6247\n",
            "\n",
            "cunlp [12]\n",
            "\n",
            "0.6910\n",
            "\n",
            "0.6172\n",
            "\n",
            "0.5748\n",
            "\n",
            "0.0000\n",
            "\n",
            "SWATH, [13]\n",
            "\n",
            "0.6347\n",
            "\n",
            "0.6858\n",
            "\n",
            "0.6200\n",
            "\n",
            "0.6867\n",
            "\n",
            "3.1\n",
            "\n",
            "Latent Semantic Analysis\n",
            "\n",
            "Latent Semantic Analysis (LSA), [14] is the algorithm, which\n",
            "reduces the dimensionality of term document., The algorithm\n",
            "creates a matrix by using word frequency, applies the singular value\n",
            "decomposition (SVD), [15], and then finds closely related terms and\n",
            "documents., The original matrix A can be separated into three\n",
            "matrices, where U is the m x r (words x extracted concept) matrix,\n",
            "V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\n",
            "r x r diagonal matrix, which can be reconstructed to find the original\n",
            "matrix A. The SVD can be represented in Eq., (1).\n",
            "\n",
            ", 3.2\n",
            "\n",
            "A â‰ˆ ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
            "\n",
            "of the related singular value over the sum of all singular values, for\n",
            "each concept.\n",
            "\n",
            ", 3.3\n",
            "\n",
            "(2)\n",
            "\n",
            "A = ğ‘Šğ‘Šğ‘Šğ‘Š\n",
            "\n",
            "Factors W and H can be found by solving the optimization problem\n",
            "as follows, whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— â‰¥ 0, ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– â‰¥ 0.\n",
            ", ğ‘šğ‘š\n",
            "\n",
            "ğ‘›ğ‘›\n",
            "\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "ğ‘—ğ‘—=1 ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘™ğ‘™=1\n",
            "\n",
            "2\n",
            "\n",
            "ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ğ¹ğ¹(ğ‘Šğ‘Š, ğ»ğ») = || ğ´ğ´ âˆ’ ğ‘Šğ‘Šğ‘Šğ‘Š ||2ğ¹ğ¹, = ï¿½ ï¿½ ï¿½ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘– âˆ’ ï¿½ ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘– ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ï¿½\n",
            "\n",
            "(3)\n",
            "\n",
            "NMF and LSA are both matrix factorization algorithms., However,\n",
            "when using NMF to find keywords, NMF will return the keywords\n",
            "that are closely related because its components have only nonnegative values., As LSA has both positive and negative values as\n",
            "well as some zeroes, it gets a wider distribution., The semantic\n",
            "feature represents a concept of meaning for root of words that have\n",
            "a relationship., For example, man, human, male and adult have the\n",
            "same semantic, hence their semantic values are close.\n",
            ", In this paper, we applied LSA and NMF on the Thai Travel News\n",
            "dataset for calculating the semantic weights, which represented the\n",
            "relationship between sentences and words in order to select the\n",
            "representative sentences for summarization.\n",
            "\n",
            ", 3.4 Generic document summarization by\n",
            "NMF\n",
            "\n",
            "Lee, J., et al. proposed Eq., (4) and Eq., (5) to select a number of\n",
            "sentences based on NMF, which got the highest semantic weight\n",
            "values, where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘—ğ‘—.\n",
            "Generic Relevance of jth sentence\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "(1)\n",
            "\n",
            "= ï¿½ ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– )\n",
            "\n",
            "Document summarization using LSA\n",
            "\n",
            "Gong, Y. et al., [16] proposed a document summarization based on\n",
            "SVD matrices., In our work, after applying SVD to matrix A, ğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
            "matrix used for selecting the important sentences., The cell value of\n",
            "the matrix shows the relationship between sentence and extracted\n",
            "concepts., A sentence with the highest cell value of each concept\n",
            "will be selected into the summary starting from the most important\n",
            "concept., The total number of sentences in the summary will be\n",
            "equal to the number all detected concepts.\n",
            ", Murray, G. et al., [17] proposed a document summarization based\n",
            "on SVD matrices using ğ‘‰ğ‘‰ ğ‘‡ğ‘‡ and Î£ matrices for sentence selection.\n",
            ", The authors proposed that more than one sentence could be\n",
            "collected from the more important concepts., The decision of how\n",
            "many sentences would be collected from each concept depending\n",
            "on the Î£ matrix., The value was decided by getting the percentage\n",
            "\n",
            "Non-negative Matrix Factorization\n",
            "\n",
            "Non-negative Matrix Factorization (NMF) is a method of matrix\n",
            "factorization subject to the non-negative constraint., Lee, J., et al.\n",
            "proposed the model based on NMF for document summarization.\n",
            ", NMF decomposes a non-negative matrix ğ´ğ´ âˆˆ ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š into two nonnegative matrices., The first matrix ğ‘šğ‘š x ğ‘Ÿğ‘Ÿ is a non-negative semantic\n",
            "feature matrix (NSFM), ğ‘Šğ‘Š ., The second matrix ğ‘Ÿğ‘Ÿ x ğ‘›ğ‘› is a nonnegative semantic variable matrix (NSVM), ğ»ğ»., So, we have ğ‘Šğ‘Š âˆˆ\n",
            "ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š and ğ»ğ» âˆˆ ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ and both terms are non-negative as shown in\n",
            "Eq., (2) and Eq., (3).\n",
            "\n",
            ", (4)\n",
            "\n",
            "ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ), =\n",
            "\n",
            "âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "âˆ‘ğ‘ğ‘=1 âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘\n",
            "\n",
            "(5)\n",
            "\n",
            ", The ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) is the relative relevance of the ith semantic feature\n",
            "(ğ‘Šğ‘Šğ‘–ğ‘– ), where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘ğ‘ and\n",
            "ğ»ğ»ğ‘ğ‘ğ‘ğ‘ is the weight of the topic ğ‘ğ‘ in the sentence ğ‘ğ‘., The sentences\n",
            "can be ranked by Generic Relevance Sentence scores., Sentences\n",
            "with the maximum score will be selected into the summary.\n",
            "\n",
            ", 3.5\n",
            "\n",
            "Cosine Similarity\n",
            "\n",
            "Cosine similarity [18] is a widely used method to measure the\n",
            "similarity between vectors representing the documents., The result of\n",
            "cosine similarity is ranging from 0 to 1., If it is closer to 1, that means\n",
            "both vectors are similar., Eq., (6) and Eq., (7) represents the cosine\n",
            "\n",
            "\fsimilarity equation, where cos(Î¸) is the dot product between vectors\n",
            "of sentences A and B and divided by the product of the two vectors'\n",
            "lengths.\n",
            ", In this paper, we deployed cosine similarity to measure the similarity\n",
            "of sentences in K-means clustering.\n",
            "\n",
            ", Aâˆ™B\n",
            "||A|| ||B||\n",
            "âˆ‘ni=1, Ai Bi\n",
            "\n",
            "Similarity(A, B) = cos(Î¸) =\n",
            "\n",
            "(7)\n",
            "\n",
            "K-means Clustering\n",
            "\n",
            "15\n",
            "\n",
            "67\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "13\n",
            "13\n",
            "\n",
            "55\n",
            "38\n",
            "\n",
            "Table 2 shows the overall number of sentences of news within each\n",
            "dataset., The average numbers of sentences per news of the 5 sets\n",
            "were 21, 16, 15, 13 and 13 sentences, respectively.\n",
            "\n",
            ", 5.\n",
            "PIPELINE FOR GENERATING\n",
            "SUMMARIES\n",
            ", In this section, we demonstrate our pipeline (Figure 1) used for text\n",
            "summarization to generate a summary for a Thai travel news.\n",
            "\n",
            ", Word\n",
            "\n",
            "S9\n",
            "\n",
            "6\n",
            "\n",
            "Round 4\n",
            "Round 5\n",
            "\n",
            "Table 3., Example of Word by Sentence Matrix A\n",
            "S8\n",
            "\n",
            "Round 3\n",
            "\n",
            "Avg. Number\n",
            "of Sentences\n",
            "\n",
            "S7\n",
            "\n",
            "21\n",
            "16\n",
            "\n",
            "Round 1\n",
            "Round 2\n",
            "\n",
            "Min.\n",
            "Number of\n",
            "Sentences\n",
            "\n",
            "S6\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "Max.\n",
            ", Number of\n",
            "Sentences\n",
            "58\n",
            "58\n",
            "\n",
            "Dataset\n",
            "\n",
            "S5\n",
            "\n",
            "Table 2., Overall Sentence Language of each Dataset\n",
            "\n",
            "S4\n",
            "\n",
            "DATA PREPARATION\n",
            "\n",
            ", The standard data sets in Thai language are unavailable for\n",
            "evaluating text summarization system., Therefore, we collected 400\n",
            "Thai travel news from Thairath and Manager online newspapers to\n",
            "be used as datasets for our experiments., We split 400 travel news\n",
            "into 5 sets of 80 news each., We then evaluated the performance of\n",
            "text summarization methods which were LSA and NMF by\n",
            "comparing their results with the summaries manually curated by\n",
            "two experts from the Faculty of Liberal Arts, Ubon Ratchathani\n",
            "University.\n",
            "\n",
            ", The open-source python libraries such as numpy [19] and sklearn\n",
            "[20] were used in our system., We converted the Thai travel news\n",
            "obtained from Thairath and Manager online newspapers to plain\n",
            "text., Then, the sentences of each news were segmented by human\n",
            "with the following format:, Si = â€˜xxxâ€™, where Si represents the order\n",
            "of the sentence in the original document and â€˜xxxâ€™ represents the\n",
            "content of that sentence., After removing stop words and duplicate\n",
            "words, we built a document term matrix or matrix A then applied\n",
            "SVD and NMF to the matrix., Then, we used python modules\n",
            "numpy.linalg.svd to calculate SVD and sklearn.decomposition to\n",
            "calculate NMF., For sentence selection, we used Gong, Y. et al. and\n",
            "Murray, G. et al. approaches for calculating weight of the sentence\n",
            "scores then selected sentences with the highest scores into the\n",
            "summary., For keyword score calculation of NMF, we calculated\n",
            "the keyword score from Eq., (5) and then selected the sentence with\n",
            "the highest score from each concept., The python module\n",
            "sklearn.cluster was used for K-means clustering., The selected\n",
            "sentences from all approaches were in the same order as the original\n",
            "document., In this paper, we performed the 20%, 30% and 40%\n",
            "document compression., This meant 80%, 70% and 60% of the\n",
            "sentences will be selected into the summary.\n",
            "\n",
            ", S3\n",
            "\n",
            "4.\n",
            "\n",
            "Figure 1., Document summarization pipeline based on LSA\n",
            "and NMF\n",
            "\n",
            "S2\n",
            "\n",
            "For sentence selection by K-means clustering, we grouped similar\n",
            "sentences into the same cluster using the following steps:\n",
            "1., Randomly select K sentences as the representative of K\n",
            "groups., K in this paper is the number of sentences that\n",
            "will be selected into the summary.\n",
            ", 2. Calculate centroid of each group by using the value of\n",
            "sentence vector from V matrix for LSA and ğ»ğ»ğ‘‡ğ‘‡ matrix\n",
            "for NMF.\n",
            ", 3. Use cosine similarity to calculate sentence similarity\n",
            "between a sentence and the centroid of each group., Then\n",
            "assign that sentence to the group with the highest\n",
            "similarity.\n",
            ", 4. Repeat steps 2-3 until all sentences are assigned to a\n",
            "group, no sentences change the group, or the similarity\n",
            "between sentences and their centroid is close.\n",
            ", 5. Select a sentence with the maximum similarity score with\n",
            "the centroid of the group and add it into the summary.\n",
            "\n",
            ", S1\n",
            "\n",
            "3.6\n",
            "\n",
            "Aâˆ™B\n",
            "=\n",
            "n\n",
            "n\n",
            "||A|| ||B||\n",
            "ï¿½âˆ‘i=1\n",
            "A2i ï¿½âˆ‘i=1\n",
            "Bi2\n",
            "\n",
            "(6)\n",
            "\n",
            "Mr.Yontas\n",
            "ak\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Supason\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Tourism\n",
            "Authority\n",
            "of Thailand\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "\fTable 3 demonstrates an example of a matrix ğ´ğ´, constructed from\n",
            "word count by sentence of a Thai travel news., It was composed of\n",
            "98 words and 9 sentences., This matrix ğ´ğ´ was then applied with the\n",
            "LSA and NMF., The sentence vectors were calculated from the term\n",
            "weight and the semantic feature vectors from Eq., (1) for LSA and\n",
            "Eq., (2) for NMF.\n",
            "\n",
            "sentences from all concepts., The Generic Sentence Relevance score\n",
            "for NMF also collected one sentence for each concept, the same as\n",
            "Gong, Y. et al., but with the highest score calculated by Eq., (5)., As\n",
            "multiple important sentences could be selected from a more\n",
            "important concept, Murray, G. et al. outperformed both Gong, Y. et\n",
            "al. and the GRS method.\n",
            "\n",
            ", 6. EXPERIMENT AND RESULTS\n",
            "6.1 Performance Evaluations Measure\n",
            "\n",
            "7.\n",
            "\n",
            ", We evaluated the results of the summarization by using standard\n",
            "accuracy, precision, recall, and F1 score, [21]., These measurements\n",
            "quantify the differences between the summary from human and the\n",
            "experimental methods., The precision shows the correctness of the\n",
            "extracted sentences and the recall reflects the number of good\n",
            "sentences missed by the method.\n",
            "\n",
            ", 6. 2 Experiment Results\n",
            "\n",
            ", In this experimental set, we would like to explore how the different\n",
            "sentence selection methods: the Generic Sentence Relevance score\n",
            "and K-means clustering, affected the text summarization result.\n",
            ", For K-means clustering, both SVD and NMF had similar\n",
            "summarization efficiency., The F1 score of SVD with K-means\n",
            "clustering was 0.83, 0.72, and 0.62 for the compression rate of 20%,\n",
            "30%, and 40%., For the NMF with K-means clustering, the F1 score\n",
            "for the three compression rates was 0.83, 0.74 and 0.64.\n",
            ", For the Generic Sentence Relevance score, the best F1 score for the\n",
            "compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\n",
            "respectively and the best F1 scores for all compression rates were\n",
            "from the approach of Murray, G. et al.\n",
            "\n",
            "Figure 2., Thai text summarization efficiency of 5 models\n",
            "Figure 2 shows the Thai text summarization efficiency of 5 models:\n",
            "(1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\n",
            "score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\n",
            "sentence score by Murray, G. et al. applied to 400 Thai travel news,\n",
            "divided into 5 sets of 80 news each, with the varied compression\n",
            "rates of 20%, 30% and 40%.\n",
            ", From this experiment, the best model based on keyword score for\n",
            "Thai travel news summarization was SVD with sentence selection\n",
            "by Murray, G. et al., This model with the compression rate of 20%\n",
            "got the highest score because Murray G. et al. method determined\n",
            "the number of sentences to be extracted from each concept based on\n",
            "the importance of that concept., The method of Gong, Y. et al., on\n",
            "the other hand was proposed to select only one sentence with the\n",
            "highest score from each concept so that the summary would include\n",
            "\n",
            "CONCLUSIONS\n",
            "\n",
            "In this paper, we applied several text summarization methods to\n",
            "Thai Travel News based on keyword scored in Thai language by\n",
            "extracting the most relevant sentences from the original document.\n",
            ", We compared LSA and NMF together with different sentence\n",
            "selection methods, to find the algorithm suitable with this paper's\n",
            "data source., We concluded that keyword scored calculation by LSA\n",
            "with sentence selection by Generic Sentence Relevance score by\n",
            "Murray, G. et al. was the best algorithm while the best compression\n",
            "rate of all models was 20%, for summarizing Thai Travel News\n",
            "compared with humans.\n",
            ", In future work, we plan to perform the experiments with different\n",
            "types of documents and improve word segmentation of compound\n",
            "nouns that was not handled by Cutkum.\n",
            "\n",
            ", 8. ACKNOWLEDGMENTS\n",
            "\n",
            "We would like to thank the department of computer engineering,\n",
            "faculty of engineering, Chulalongkorn University for providing\n",
            "computing facilities.\n",
            "\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]"
      ],
      "metadata": {
        "id": "Dn9RQs5So4Ed"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCimHub2o4jW",
        "outputId": "2517cf67-fc0c-4fd8-dc96-d50bc712a23c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Extractive Text Summarization for Thai Travel News\n",
              " Based on Keyword Scored in Thai Language\n",
              " Sarunya Nathonghor\n",
              " \n",
              " Duangdao Wichadakul\n",
              " \n",
              " Department of Computer Engineering\n",
              " Chulalongkorn University\n",
              " Bangkok, Thailand\n",
              " \n",
              " Department of Computer Engineering\n",
              " Chulalongkorn University\n",
              " Bangkok, Thailand\n",
              " \n",
              " Sarunya.N@Student.Chula.ac.th\n",
              " ABSTRACT\n",
              " \n",
              " In recent years, people are seeking for a solution to improve text\n",
              " summarization for Thai language.: 6.148571428571429,\n",
              " Although several solutions such\n",
              " as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
              " models, etc., have been proposed, research results in Thai text\n",
              " summarization were restricted due to limited corpus in Thai\n",
              " language with complex grammar.: 0.6285714285714284,\n",
              " This paper applied a text\n",
              " summarization system for Thai travel news based on keyword\n",
              " scored in Thai language by extracting the most relevant sentences\n",
              " from the original document.: 1.2228571428571429,\n",
              " We compared LSA and Non-negative\n",
              " Matrix Factorization (NMF) to find the algorithm that is suitable\n",
              " with Thai travel news.: 0.5485714285714286,\n",
              " The suitable compression rates for Generic\n",
              " Sentence Relevance score (GRS) and K-means clustering were also\n",
              " evaluated.: 0.5885714285714285,\n",
              " From these experiments, we concluded that keyword\n",
              " scored calculation by LSA with sentence selection by GRS is the\n",
              " best algorithm for summarizing Thai Travel News, compared with\n",
              " human with the best compression rate of 20%.\n",
              " : 1.897142857142857,\n",
              " CCS Concepts\n",
              " \n",
              " â€¢ Information systems â Information retrieval â Retrieval\n",
              " tasks and goalsâ Summarization\n",
              " \n",
              " Keywords\n",
              " \n",
              " Text summarization; extractive summarization; non-negative\n",
              " matrix factorization\n",
              " \n",
              " 1.\n",
              " \n",
              " INTRODUCTION\n",
              " \n",
              " Daily newspaper has abundant of data that users do not have\n",
              " enough time for reading them.: 7.348571428571428,\n",
              " It is difficult to identify the relevant\n",
              " information to satisfy the information needed by users.: 0.08571428571428572,\n",
              " Automatic\n",
              " summarization can reduce the problem of information overloading\n",
              " and it has been proposed previously in English and other languages.: 0.34857142857142864,\n",
              " However, there were only a few research results in Thai text\n",
              " summarization due to the lack of corpus in Thai language and the\n",
              " complicated grammar.: 0.4342857142857144,\n",
              " Text Summarization: 0.3085714285714286,\n",
              " [1] is a technique for summarizing the content\n",
              " of the documents.: 0.17714285714285716,\n",
              " It consists of three steps: 1) create an\n",
              " intermediate representation of the input text, 2) calculate score for\n",
              " the sentences based on the concepts, and 3) choose important\n",
              " Permission to make digital or hard copies of all or part of this work for\n",
              " personal or classroom use is granted without fee provided that copies are\n",
              " not made or distributed for profit or commercial advantage and that copies\n",
              " bear this notice and the full citation on the first page.: 1.2228571428571438,\n",
              " Copyrights for\n",
              " components of this work owned by others than ACM must be honored.: 0.045714285714285714,\n",
              " Abstracting with credit is permitted.: 0.011428571428571429,\n",
              " To copy otherwise, or republish, to\n",
              " post on servers or to redistribute to lists, requires prior specific permission\n",
              " and/or a fee.: 0.08,\n",
              " Request permissions from Permissions@acm.org.: 0.005714285714285714,\n",
              " ITCC 2020, August 12â€“14, 2020, Kuala Lumpur, Malaysia\n",
              " Â© 2020 Association for Computing Machinery.: 0.06857142857142857,\n",
              " ACM ISBN 978-1-4503-7539-9/20/08â€¦$15.00\n",
              " \n",
              " DOI: https://doi.org/10.1145/3417473.3417479\n",
              " \n",
              " Duangdao.W@Chula.ac.th\n",
              " \n",
              " sentences to be included in the summary.: 3.537142857142857,\n",
              " Text summarization can\n",
              " be divided into 2 approaches.: 0.44,\n",
              " The first approach is the extractive\n",
              " summarization, which relies on a method for extracting words and\n",
              " searching for keywords from the original document.: 0.5828571428571429,\n",
              " The second\n",
              " approach is the abstractive summarization, which analyzes words\n",
              " by linguistic principles with transcription or interpretation from the\n",
              " original document.: 0.4800000000000001,\n",
              " This approach implies more effective and\n",
              " accurate summary than the extractive methods.: 0.18285714285714286,\n",
              " However, with the\n",
              " lack of Thai corpus, we chose to apply an extractive summarization\n",
              " method for Thai text summarization.: 0.6571428571428571,\n",
              " This research focused on the sentence extraction function based on\n",
              " keyword score calculation then selecting important sentences based\n",
              " on the Generic Sentence Relevance score (GRS), calculated from\n",
              " Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
              " Factorization (NMF).: 1.657142857142857,\n",
              " We also tried using K-means clustering for\n",
              " document summarization.: 0.44571428571428573,\n",
              " In this experiment, we compared 5\n",
              " models for 5 rounds with Thai travel news using the compression\n",
              " rates of 20%, 30% and 40% and reported the rate and method that\n",
              " produced the best result from the experiment.\n",
              " : 1.8285714285714285,\n",
              " 2.\n",
              " \n",
              " RELATED WORKS\n",
              " : 2.1142857142857143,\n",
              " In recent years, several models in Thai Text summarization have\n",
              " been introduced.: 0.37714285714285717,\n",
              " Suwanno, N. et al.: 0.24,\n",
              " [2] proposed a Thai text\n",
              " summarization that extracted a paragraph from a document based\n",
              " on Thai compound nouns, term frequency method, and headline\n",
              " score for generating a summary.: 1.102857142857143,\n",
              " Chongsuntornsri, A., et al.: 0.24,\n",
              " [3]\n",
              " proposed a new approach for Text summarization in Thai based on\n",
              " content- and graph-based with the use of Topic Sensitive PageRank\n",
              " algorithm for summarizing and ranking of text segments.: 0.9085714285714285,\n",
              " Jaruskulchai C., et al.: 0.24,\n",
              " [4] proposed a method to summarize\n",
              " documents by extracting important sentences from combining the\n",
              " specific properties (Local Property) and the overall properties\n",
              " (Global Property) of the sentences.: 0.8800000000000001,\n",
              " The overall properties were\n",
              " based on the relationship between sentences in the document.: 0.5028571428571429,\n",
              " From\n",
              " their experiments, the summarization of the industrial news got\n",
              " 60% precision, 44% recall, and 50.9% F-measure, the general news\n",
              " got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
              " the fashion magazines got 53.0% precision, 33.0% recall, and\n",
              " 40.4% F-measure.: 0.9485714285714282,\n",
              " Mani, I., et al.: 0.24,\n",
              " [5] proposed techniques of text summarization by\n",
              " using word frequency in the document and calculated the weight of\n",
              " word to create a keyword group.: 0.9600000000000001,\n",
              " They then calculated the cosine\n",
              " similarity of sentences.: 0.3828571428571429,\n",
              " The researcher used A* search algorithm to\n",
              " find the shortest sequence of sentences from keyword group by\n",
              " topic calculation, sentence segmentation and word grouping.: 0.7885714285714286,\n",
              " The\n",
              " sequence of sentences that were in the main group were selected as\n",
              " important sentences.: 0.6800000000000002,\n",
              " Their summarization of the agricultural news\n",
              " got 68.57% precision, 51.95% recall and 56.72% F-measure.: 0.4800000000000001,\n",
              " Lee, J., et al.: 0.24,\n",
              " [6] proposed a document summarization method using\n",
              " Non-negative Matrix Factorization (NMF).: 0.76,\n",
              " They compared\n",
              " \n",
              " \n",
              " between Latent Semantic Analysis (LSA) and NMF to find the\n",
              " weight of each word and calculated the summation of weights.: 0.3257142857142857,\n",
              " The\n",
              " important sentences were ranked and selected into the summary\n",
              " based on their summed weight.: 0.6114285714285714,\n",
              " Based on LSA, they found many\n",
              " weights with zero and negative values.: 0.24571428571428575,\n",
              " However, when applied\n",
              " NMF, they found only the positive values and the scope of the\n",
              " semantic featuresâ€™ meaning was narrow.: 0.21142857142857147,\n",
              " Therefore, they proposed\n",
              " that NMF provided a greater possibility for extracting important\n",
              " sentences.\n",
              " : 1.4457142857142857,\n",
              " 3.\n",
              " \n",
              " PREPROCESSING FOR THAI TEXT\n",
              " \n",
              " The first step for working with Thai Text is word tokenization.: 2.3600000000000003,\n",
              " Even\n",
              " though Thai writing system has no delimiters to indicate word\n",
              " boundaries together with many rules for word segmentation, several\n",
              " Thai word tokenization programs have been proposed.: 0.39428571428571424,\n",
              " Table 1\n",
              " shows F1 score of the recent programs trained and tested by one of\n",
              " our laboratory members with the data from BEST2010 corpus: 0.38857142857142857,\n",
              " [7].: 0.045714285714285714,\n",
              " Cutkum [8] got the highest F1 score, hence, we used Cutkum for this\n",
              " step.: 0.2628571428571429,\n",
              " Table 1.: 0.10857142857142857,\n",
              " Comparison of Thai word tokenization programs\n",
              " Tools\n",
              " \n",
              " F1 Score\n",
              " \n",
              " Validate\n",
              " PyICU: 2.2514285714285713,\n",
              " [9]\n",
              " \n",
              " Article\n",
              " 100\n",
              " 0.6155\n",
              " \n",
              " Encyclopedia\n",
              " 100\n",
              " 0.6932\n",
              " \n",
              " News\n",
              " 100\n",
              " 0.5987\n",
              " \n",
              " Novel\n",
              " 100\n",
              " 0.6800\n",
              " \n",
              " Lexto: 5.228571428571428,\n",
              " [10]\n",
              " \n",
              " 0.7267\n",
              " \n",
              " 0.7709\n",
              " \n",
              " 0.6994\n",
              " \n",
              " 0.7701\n",
              " \n",
              " Cutkum\n",
              " wordcutpy [11]\n",
              " \n",
              " 0.9322\n",
              " 0.6212\n",
              " \n",
              " 0.9299\n",
              " 0.6286\n",
              " \n",
              " 0.8987\n",
              " 0.6571\n",
              " \n",
              " 0.7140\n",
              " 0.6247\n",
              " \n",
              " cunlp [12]\n",
              " \n",
              " 0.6910\n",
              " \n",
              " 0.6172\n",
              " \n",
              " 0.5748\n",
              " \n",
              " 0.0000\n",
              " \n",
              " SWATH: 15.119999999999994,\n",
              " [13]\n",
              " \n",
              " 0.6347\n",
              " \n",
              " 0.6858\n",
              " \n",
              " 0.6200\n",
              " \n",
              " 0.6867\n",
              " \n",
              " 3.1\n",
              " \n",
              " Latent Semantic Analysis\n",
              " \n",
              " Latent Semantic Analysis (LSA): 7.171428571428571,\n",
              " [14] is the algorithm, which\n",
              " reduces the dimensionality of term document.: 0.18857142857142856,\n",
              " The algorithm\n",
              " creates a matrix by using word frequency, applies the singular value\n",
              " decomposition (SVD): 0.34285714285714286,\n",
              " [15], and then finds closely related terms and\n",
              " documents.: 0.09142857142857143,\n",
              " The original matrix A can be separated into three\n",
              " matrices, where U is the m x r (words x extracted concept) matrix,\n",
              " V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\n",
              " r x r diagonal matrix, which can be reconstructed to find the original\n",
              " matrix A. The SVD can be represented in Eq.: 1.6742857142857144,\n",
              " (1).\n",
              " : 1.1085714285714285,\n",
              " 3.2\n",
              " \n",
              " A â‰ˆ ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
              " \n",
              " of the related singular value over the sum of all singular values, for\n",
              " each concept.\n",
              " : 3.2342857142857153,\n",
              " 3.3\n",
              " \n",
              " (2)\n",
              " \n",
              " A = ğ‘Šğ‘Šğ‘Šğ‘Š\n",
              " \n",
              " Factors W and H can be found by solving the optimization problem\n",
              " as follows, whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— â‰¥ 0, ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– â‰¥ 0.: 3.531428571428573,\n",
              " ğ‘šğ‘š\n",
              " \n",
              " ğ‘›ğ‘›\n",
              " \n",
              " ğ‘Ÿğ‘Ÿ\n",
              " \n",
              " ğ‘—ğ‘—=1 ğ‘–ğ‘–=1\n",
              " \n",
              " ğ‘™ğ‘™=1\n",
              " \n",
              " 2\n",
              " \n",
              " ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ğ¹ğ¹(ğ‘Šğ‘Š, ğ»ğ») = || ğ´ğ´ âˆ’ ğ‘Šğ‘Šğ‘Šğ‘Š ||2ğ¹ğ¹: 6.25142857142857,\n",
              " = ï¿½ ï¿½ ï¿½ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘– âˆ’ ï¿½ ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘– ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ï¿½\n",
              " \n",
              " (3)\n",
              " \n",
              " NMF and LSA are both matrix factorization algorithms.: 2.5142857142857147,\n",
              " However,\n",
              " when using NMF to find keywords, NMF will return the keywords\n",
              " that are closely related because its components have only nonnegative values.: 0.17142857142857143,\n",
              " As LSA has both positive and negative values as\n",
              " well as some zeroes, it gets a wider distribution.: 0.1428571428571429,\n",
              " The semantic\n",
              " feature represents a concept of meaning for root of words that have\n",
              " a relationship.: 0.2571428571428572,\n",
              " For example, man, human, male and adult have the\n",
              " same semantic, hence their semantic values are close.: 0.21714285714285714,\n",
              " In this paper, we applied LSA and NMF on the Thai Travel News\n",
              " dataset for calculating the semantic weights, which represented the\n",
              " relationship between sentences and words in order to select the\n",
              " representative sentences for summarization.\n",
              " : 2.1714285714285717,\n",
              " 3.4 Generic document summarization by\n",
              " NMF\n",
              " \n",
              " Lee, J., et al. proposed Eq.: 1.6457142857142857,\n",
              " (4) and Eq.: 0.04,\n",
              " (5) to select a number of\n",
              " sentences based on NMF, which got the highest semantic weight\n",
              " values, where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘—ğ‘—.\n",
              " Generic Relevance of jth sentence\n",
              " ğ‘Ÿğ‘Ÿ\n",
              " \n",
              " (1)\n",
              " \n",
              " = ï¿½ ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– )\n",
              " \n",
              " Document summarization using LSA\n",
              " \n",
              " Gong, Y. et al.: 6.02857142857143,\n",
              " [16] proposed a document summarization based on\n",
              " SVD matrices.: 0.5428571428571428,\n",
              " In our work, after applying SVD to matrix A, ğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
              " matrix used for selecting the important sentences.: 0.6628571428571428,\n",
              " The cell value of\n",
              " the matrix shows the relationship between sentence and extracted\n",
              " concepts.: 0.49142857142857144,\n",
              " A sentence with the highest cell value of each concept\n",
              " will be selected into the summary starting from the most important\n",
              " concept.: 0.622857142857143,\n",
              " The total number of sentences in the summary will be\n",
              " equal to the number all detected concepts.: 0.4571428571428572,\n",
              " Murray, G. et al.: 0.24,\n",
              " [17] proposed a document summarization based\n",
              " on SVD matrices using ğ‘‰ğ‘‰ ğ‘‡ğ‘‡ and Î£ matrices for sentence selection.: 0.8171428571428572,\n",
              " The authors proposed that more than one sentence could be\n",
              " collected from the more important concepts.: 0.3942857142857143,\n",
              " The decision of how\n",
              " many sentences would be collected from each concept depending\n",
              " on the Î£ matrix.: 0.49142857142857144,\n",
              " The value was decided by getting the percentage\n",
              " \n",
              " Non-negative Matrix Factorization\n",
              " \n",
              " Non-negative Matrix Factorization (NMF) is a method of matrix\n",
              " factorization subject to the non-negative constraint.: 2.9028571428571426,\n",
              " Lee, J., et al.\n",
              " proposed the model based on NMF for document summarization.: 0.7542857142857142,\n",
              " NMF decomposes a non-negative matrix ğ´ğ´ âˆˆ ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š into two nonnegative matrices.: 0.34285714285714286,\n",
              " The first matrix ğ‘šğ‘š x ğ‘Ÿğ‘Ÿ is a non-negative semantic\n",
              " feature matrix (NSFM), ğ‘Šğ‘Š .: 0.5542857142857144,\n",
              " The second matrix ğ‘Ÿğ‘Ÿ x ğ‘›ğ‘› is a nonnegative semantic variable matrix (NSVM), ğ»ğ».: 0.4628571428571429,\n",
              " So, we have ğ‘Šğ‘Š âˆˆ\n",
              " ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š and ğ»ğ» âˆˆ ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ and both terms are non-negative as shown in\n",
              " Eq.: 0.18857142857142858,\n",
              " (2) and Eq.: 0.09714285714285714,\n",
              " (3).\n",
              " : 1.062857142857143,\n",
              " (4)\n",
              " \n",
              " ğ‘–ğ‘–=1\n",
              " \n",
              " ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ): 2.068571428571429,\n",
              " =\n",
              " \n",
              " âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–\n",
              " ğ‘Ÿğ‘Ÿ\n",
              " âˆ‘ğ‘ğ‘=1 âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘\n",
              " \n",
              " (5)\n",
              " : 3.194285714285714,\n",
              " The ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) is the relative relevance of the ith semantic feature\n",
              " (ğ‘Šğ‘Šğ‘–ğ‘– ), where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘ğ‘ and\n",
              " ğ»ğ»ğ‘ğ‘ğ‘ğ‘ is the weight of the topic ğ‘ğ‘ in the sentence ğ‘ğ‘.: 0.7085714285714285,\n",
              " The sentences\n",
              " can be ranked by Generic Relevance Sentence scores.: 0.4685714285714286,\n",
              " Sentences\n",
              " with the maximum score will be selected into the summary.\n",
              " : 1.542857142857143,\n",
              " 3.5\n",
              " \n",
              " Cosine Similarity\n",
              " \n",
              " Cosine similarity [18] is a widely used method to measure the\n",
              " similarity between vectors representing the documents.: 2.451428571428571,\n",
              " The result of\n",
              " cosine similarity is ranging from 0 to 1.: 0.3828571428571429,\n",
              " If it is closer to 1, that means\n",
              " both vectors are similar.: 0.24,\n",
              " (6) and Eq.: 0.03428571428571429,\n",
              " (7) represents the cosine\n",
              " \n",
              " \n",
              " similarity equation, where cos(Î¸) is the dot product between vectors\n",
              " of sentences A and B and divided by the product of the two vectors'\n",
              " lengths.: 0.5657142857142857,\n",
              " In this paper, we deployed cosine similarity to measure the similarity\n",
              " of sentences in K-means clustering.\n",
              " : 1.617142857142857,\n",
              " Aâˆ™B\n",
              " ||A|| ||B||\n",
              " âˆ‘ni=1: 0.005714285714285714,\n",
              " Ai Bi\n",
              " \n",
              " Similarity(A, B) = cos(Î¸) =\n",
              " \n",
              " (7)\n",
              " \n",
              " K-means Clustering\n",
              " \n",
              " 15\n",
              " \n",
              " 67\n",
              " \n",
              " 7\n",
              " 7\n",
              " \n",
              " 13\n",
              " 13\n",
              " \n",
              " 55\n",
              " 38\n",
              " \n",
              " Table 2 shows the overall number of sentences of news within each\n",
              " dataset.: 9.908571428571426,\n",
              " The average numbers of sentences per news of the 5 sets\n",
              " were 21, 16, 15, 13 and 13 sentences, respectively.\n",
              " : 1.8399999999999999,\n",
              " 5.\n",
              " PIPELINE FOR GENERATING\n",
              " SUMMARIES: 0.11428571428571428,\n",
              " In this section, we demonstrate our pipeline (Figure 1) used for text\n",
              " summarization to generate a summary for a Thai travel news.\n",
              " : 1.6857142857142857,\n",
              " Word\n",
              " \n",
              " S9\n",
              " \n",
              " 6\n",
              " \n",
              " Round 4\n",
              " Round 5\n",
              " \n",
              " Table 3.: 4.297142857142858,\n",
              " Example of Word by Sentence Matrix A\n",
              " S8\n",
              " \n",
              " Round 3\n",
              " \n",
              " Avg. Number\n",
              " of Sentences\n",
              " \n",
              " S7\n",
              " \n",
              " 21\n",
              " 16\n",
              " \n",
              " Round 1\n",
              " Round 2\n",
              " \n",
              " Min.\n",
              " Number of\n",
              " Sentences\n",
              " \n",
              " S6\n",
              " \n",
              " 7\n",
              " 7\n",
              " \n",
              " Max.: 10.371428571428572,\n",
              " Number of\n",
              " Sentences\n",
              " 58\n",
              " 58\n",
              " \n",
              " Dataset\n",
              " \n",
              " S5\n",
              " \n",
              " Table 2.: 3.4171428571428573,\n",
              " Overall Sentence Language of each Dataset\n",
              " \n",
              " S4\n",
              " \n",
              " DATA PREPARATION\n",
              " : 3.2685714285714282,\n",
              " The standard data sets in Thai language are unavailable for\n",
              " evaluating text summarization system.: 0.43428571428571433,\n",
              " Therefore, we collected 400\n",
              " Thai travel news from Thairath and Manager online newspapers to\n",
              " be used as datasets for our experiments.: 0.25142857142857145,\n",
              " We split 400 travel news\n",
              " into 5 sets of 80 news each.: 0.41714285714285715,\n",
              " We then evaluated the performance of\n",
              " text summarization methods which were LSA and NMF by\n",
              " comparing their results with the summaries manually curated by\n",
              " two experts from the Faculty of Liberal Arts, Ubon Ratchathani\n",
              " University.\n",
              " : 1.422857142857143,\n",
              " The open-source python libraries such as numpy [19] and sklearn\n",
              " [20] were used in our system.: 0.13142857142857142,\n",
              " We converted the Thai travel news\n",
              " obtained from Thairath and Manager online newspapers to plain\n",
              " text.: 0.29714285714285715,\n",
              " Then, the sentences of each news were segmented by human\n",
              " with the following format:: 0.39428571428571435,\n",
              " Si = â€˜xxxâ€™, where Si represents the order\n",
              " of the sentence in the original document and â€˜xxxâ€™ represents the\n",
              " content of that sentence.: 0.6685714285714286,\n",
              " After removing stop words and duplicate\n",
              " words, we built a document term matrix or matrix A then applied\n",
              " SVD and NMF to the matrix.: 0.7085714285714286,\n",
              " Then, we used python modules\n",
              " numpy.linalg.svd to calculate SVD and sklearn.decomposition to\n",
              " calculate NMF.: 0.08,\n",
              " For sentence selection, we used Gong, Y. et al. and\n",
              " Murray, G. et al. approaches for calculating weight of the sentence\n",
              " scores then selected sentences with the highest scores into the\n",
              " summary.: 1.4514285714285715,\n",
              " For keyword score calculation of NMF, we calculated\n",
              " the keyword score from Eq.: 0.48000000000000004,\n",
              " (5) and then selected the sentence with\n",
              " the highest score from each concept.: 0.6,\n",
              " The python module\n",
              " sklearn.cluster was used for K-means clustering.: 0.15428571428571428,\n",
              " The selected\n",
              " sentences from all approaches were in the same order as the original\n",
              " document.: 0.4857142857142857,\n",
              " In this paper, we performed the 20%, 30% and 40%\n",
              " document compression.: 0.32000000000000006,\n",
              " This meant 80%, 70% and 60% of the\n",
              " sentences will be selected into the summary.\n",
              " : 1.4228571428571428,\n",
              " S3\n",
              " \n",
              " 4.\n",
              " \n",
              " Figure 1.: 2.1485714285714286,\n",
              " Document summarization pipeline based on LSA\n",
              " and NMF\n",
              " \n",
              " S2\n",
              " \n",
              " For sentence selection by K-means clustering, we grouped similar\n",
              " sentences into the same cluster using the following steps:\n",
              " 1.: 3.188571428571429,\n",
              " Randomly select K sentences as the representative of K\n",
              " groups.: 0.28571428571428575,\n",
              " K in this paper is the number of sentences that\n",
              " will be selected into the summary.: 0.4628571428571429,\n",
              " 2. Calculate centroid of each group by using the value of\n",
              " sentence vector from V matrix for LSA and ğ»ğ»ğ‘‡ğ‘‡ matrix\n",
              " for NMF.: 0.7085714285714286,\n",
              " 3. Use cosine similarity to calculate sentence similarity\n",
              " between a sentence and the centroid of each group.: 0.7028571428571428,\n",
              " Then\n",
              " assign that sentence to the group with the highest\n",
              " similarity.: 0.36,\n",
              " 4. Repeat steps 2-3 until all sentences are assigned to a\n",
              " group, no sentences change the group, or the similarity\n",
              " between sentences and their centroid is close.: 1.1714285714285715,\n",
              " 5. Select a sentence with the maximum similarity score with\n",
              " the centroid of the group and add it into the summary.\n",
              " : 1.6857142857142857,\n",
              " S1\n",
              " \n",
              " 3.6\n",
              " \n",
              " Aâˆ™B\n",
              " =\n",
              " n\n",
              " n\n",
              " ||A|| ||B||\n",
              " ï¿½âˆ‘i=1\n",
              " A2i ï¿½âˆ‘i=1\n",
              " Bi2\n",
              " \n",
              " (6)\n",
              " \n",
              " Mr.Yontas\n",
              " ak\n",
              " \n",
              " 1\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " Supason\n",
              " \n",
              " 1\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " Tourism\n",
              " Authority\n",
              " of Thailand\n",
              " \n",
              " 1\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " \n",
              " Table 3 demonstrates an example of a matrix ğ´ğ´, constructed from\n",
              " word count by sentence of a Thai travel news.: 48.53714285714282,\n",
              " It was composed of\n",
              " 98 words and 9 sentences.: 0.3142857142857143,\n",
              " This matrix ğ´ğ´ was then applied with the\n",
              " LSA and NMF.: 0.2057142857142857,\n",
              " The sentence vectors were calculated from the term\n",
              " weight and the semantic feature vectors from Eq.: 0.44571428571428573,\n",
              " (1) for LSA and\n",
              " Eq.: 0.10857142857142857,\n",
              " (2) for NMF.\n",
              " \n",
              " sentences from all concepts.: 1.3771428571428572,\n",
              " The Generic Sentence Relevance score\n",
              " for NMF also collected one sentence for each concept, the same as\n",
              " Gong, Y. et al.: 0.8514285714285715,\n",
              " but with the highest score calculated by Eq.: 0.24000000000000002,\n",
              " (5).: 0.09142857142857143,\n",
              " As\n",
              " multiple important sentences could be selected from a more\n",
              " important concept, Murray, G. et al. outperformed both Gong, Y. et\n",
              " al. and the GRS method.\n",
              " : 2.057142857142857,\n",
              " 6. EXPERIMENT AND RESULTS\n",
              " 6.1 Performance Evaluations Measure\n",
              " \n",
              " 7.\n",
              " : 2.165714285714286,\n",
              " We evaluated the results of the summarization by using standard\n",
              " accuracy, precision, recall, and F1 score: 0.4857142857142858,\n",
              " [21].: 0.017142857142857144,\n",
              " These measurements\n",
              " quantify the differences between the summary from human and the\n",
              " experimental methods.: 0.1657142857142857,\n",
              " The precision shows the correctness of the\n",
              " extracted sentences and the recall reflects the number of good\n",
              " sentences missed by the method.\n",
              " : 1.7542857142857144,\n",
              " 6. 2 Experiment Results\n",
              " : 1.1714285714285715,\n",
              " In this experimental set, we would like to explore how the different\n",
              " sentence selection methods: the Generic Sentence Relevance score\n",
              " and K-means clustering, affected the text summarization result.: 1.1085714285714285,\n",
              " For K-means clustering, both SVD and NMF had similar\n",
              " summarization efficiency.: 0.37142857142857144,\n",
              " The F1 score of SVD with K-means\n",
              " clustering was 0.83, 0.72, and 0.62 for the compression rate of 20%,\n",
              " 30%, and 40%.: 0.502857142857143,\n",
              " For the NMF with K-means clustering, the F1 score\n",
              " for the three compression rates was 0.83, 0.74 and 0.64.: 0.3885714285714287,\n",
              " For the Generic Sentence Relevance score, the best F1 score for the\n",
              " compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\n",
              " respectively and the best F1 scores for all compression rates were\n",
              " from the approach of Murray, G. et al.\n",
              " \n",
              " Figure 2.: 2.2914285714285714,\n",
              " Thai text summarization efficiency of 5 models\n",
              " Figure 2 shows the Thai text summarization efficiency of 5 models:\n",
              " (1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\n",
              " score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\n",
              " sentence score by Murray, G. et al. applied to 400 Thai travel news,\n",
              " divided into 5 sets of 80 news each, with the varied compression\n",
              " rates of 20%, 30% and 40%.: 3.394285714285715,\n",
              " From this experiment, the best model based on keyword score for\n",
              " Thai travel news summarization was SVD with sentence selection\n",
              " by Murray, G. et al.: 1.2228571428571429,\n",
              " This model with the compression rate of 20%\n",
              " got the highest score because Murray G. et al. method determined\n",
              " the number of sentences to be extracted from each concept based on\n",
              " the importance of that concept.: 1.28,\n",
              " The method of Gong, Y. et al., on\n",
              " the other hand was proposed to select only one sentence with the\n",
              " highest score from each concept so that the summary would include\n",
              " \n",
              " CONCLUSIONS\n",
              " \n",
              " In this paper, we applied several text summarization methods to\n",
              " Thai Travel News based on keyword scored in Thai language by\n",
              " extracting the most relevant sentences from the original document.: 4.194285714285713,\n",
              " We compared LSA and NMF together with different sentence\n",
              " selection methods, to find the algorithm suitable with this paper's\n",
              " data source.: 0.48571428571428577,\n",
              " We concluded that keyword scored calculation by LSA\n",
              " with sentence selection by Generic Sentence Relevance score by\n",
              " Murray, G. et al. was the best algorithm while the best compression\n",
              " rate of all models was 20%, for summarizing Thai Travel News\n",
              " compared with humans.: 1.4685714285714286,\n",
              " In future work, we plan to perform the experiments with different\n",
              " types of documents and improve word segmentation of compound\n",
              " nouns that was not handled by Cutkum.\n",
              " : 1.24,\n",
              " 8. ACKNOWLEDGMENTS\n",
              " \n",
              " We would like to thank the department of computer engineering,\n",
              " faculty of engineering, Chulalongkorn University for providing\n",
              " computing facilities.\n",
              " : 2.085714285714286}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "ThRxqAyVo6PT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKKlQL4Ro8bU",
        "outputId": "254fb0dc-ce63-491c-81df-b6624d03fc44"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)"
      ],
      "metadata": {
        "id": "mq62FJhlpoAN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpUXroatpqDc",
        "outputId": "4c5885ac-833c-4e65-d1cb-1d3c6b18c953"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[S1\n",
              " \n",
              " 3.6\n",
              " \n",
              " Aâˆ™B\n",
              " =\n",
              " n\n",
              " n\n",
              " ||A|| ||B||\n",
              " ï¿½âˆ‘i=1\n",
              " A2i ï¿½âˆ‘i=1\n",
              " Bi2\n",
              " \n",
              " (6)\n",
              " \n",
              " Mr.Yontas\n",
              " ak\n",
              " \n",
              " 1\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " Supason\n",
              " \n",
              " 1\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " Tourism\n",
              " Authority\n",
              " of Thailand\n",
              " \n",
              " 1\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " 0\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " â€¦\n",
              " \n",
              " \n",
              " Table 3 demonstrates an example of a matrix ğ´ğ´, constructed from\n",
              " word count by sentence of a Thai travel news.,\n",
              " [10]\n",
              " \n",
              " 0.7267\n",
              " \n",
              " 0.7709\n",
              " \n",
              " 0.6994\n",
              " \n",
              " 0.7701\n",
              " \n",
              " Cutkum\n",
              " wordcutpy [11]\n",
              " \n",
              " 0.9322\n",
              " 0.6212\n",
              " \n",
              " 0.9299\n",
              " 0.6286\n",
              " \n",
              " 0.8987\n",
              " 0.6571\n",
              " \n",
              " 0.7140\n",
              " 0.6247\n",
              " \n",
              " cunlp [12]\n",
              " \n",
              " 0.6910\n",
              " \n",
              " 0.6172\n",
              " \n",
              " 0.5748\n",
              " \n",
              " 0.0000\n",
              " \n",
              " SWATH,\n",
              " Example of Word by Sentence Matrix A\n",
              " S8\n",
              " \n",
              " Round 3\n",
              " \n",
              " Avg. Number\n",
              " of Sentences\n",
              " \n",
              " S7\n",
              " \n",
              " 21\n",
              " 16\n",
              " \n",
              " Round 1\n",
              " Round 2\n",
              " \n",
              " Min.\n",
              " Number of\n",
              " Sentences\n",
              " \n",
              " S6\n",
              " \n",
              " 7\n",
              " 7\n",
              " \n",
              " Max.,\n",
              " Ai Bi\n",
              " \n",
              " Similarity(A, B) = cos(Î¸) =\n",
              " \n",
              " (7)\n",
              " \n",
              " K-means Clustering\n",
              " \n",
              " 15\n",
              " \n",
              " 67\n",
              " \n",
              " 7\n",
              " 7\n",
              " \n",
              " 13\n",
              " 13\n",
              " \n",
              " 55\n",
              " 38\n",
              " \n",
              " Table 2 shows the overall number of sentences of news within each\n",
              " dataset.,\n",
              " CCS Concepts\n",
              " \n",
              " â€¢ Information systems â Information retrieval â Retrieval\n",
              " tasks and goalsâ Summarization\n",
              " \n",
              " Keywords\n",
              " \n",
              " Text summarization; extractive summarization; non-negative\n",
              " matrix factorization\n",
              " \n",
              " 1.\n",
              " \n",
              " INTRODUCTION\n",
              " \n",
              " Daily newspaper has abundant of data that users do not have\n",
              " enough time for reading them.,\n",
              " [13]\n",
              " \n",
              " 0.6347\n",
              " \n",
              " 0.6858\n",
              " \n",
              " 0.6200\n",
              " \n",
              " 0.6867\n",
              " \n",
              " 3.1\n",
              " \n",
              " Latent Semantic Analysis\n",
              " \n",
              " Latent Semantic Analysis (LSA),\n",
              " ğ‘šğ‘š\n",
              " \n",
              " ğ‘›ğ‘›\n",
              " \n",
              " ğ‘Ÿğ‘Ÿ\n",
              " \n",
              " ğ‘—ğ‘—=1 ğ‘–ğ‘–=1\n",
              " \n",
              " ğ‘™ğ‘™=1\n",
              " \n",
              " 2\n",
              " \n",
              " ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ğ¹ğ¹(ğ‘Šğ‘Š, ğ»ğ») = || ğ´ğ´ âˆ’ ğ‘Šğ‘Šğ‘Šğ‘Š ||2ğ¹ğ¹,\n",
              " Extractive Text Summarization for Thai Travel News\n",
              " Based on Keyword Scored in Thai Language\n",
              " Sarunya Nathonghor\n",
              " \n",
              " Duangdao Wichadakul\n",
              " \n",
              " Department of Computer Engineering\n",
              " Chulalongkorn University\n",
              " Bangkok, Thailand\n",
              " \n",
              " Department of Computer Engineering\n",
              " Chulalongkorn University\n",
              " Bangkok, Thailand\n",
              " \n",
              " Sarunya.N@Student.Chula.ac.th\n",
              " ABSTRACT\n",
              " \n",
              " In recent years, people are seeking for a solution to improve text\n",
              " summarization for Thai language.,\n",
              " (5) to select a number of\n",
              " sentences based on NMF, which got the highest semantic weight\n",
              " values, where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘—ğ‘—.\n",
              " Generic Relevance of jth sentence\n",
              " ğ‘Ÿğ‘Ÿ\n",
              " \n",
              " (1)\n",
              " \n",
              " = ï¿½ ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– )\n",
              " \n",
              " Document summarization using LSA\n",
              " \n",
              " Gong, Y. et al.,\n",
              " [9]\n",
              " \n",
              " Article\n",
              " 100\n",
              " 0.6155\n",
              " \n",
              " Encyclopedia\n",
              " 100\n",
              " 0.6932\n",
              " \n",
              " News\n",
              " 100\n",
              " 0.5987\n",
              " \n",
              " Novel\n",
              " 100\n",
              " 0.6800\n",
              " \n",
              " Lexto,\n",
              " Word\n",
              " \n",
              " S9\n",
              " \n",
              " 6\n",
              " \n",
              " Round 4\n",
              " Round 5\n",
              " \n",
              " Table 3.,\n",
              " The method of Gong, Y. et al., on\n",
              " the other hand was proposed to select only one sentence with the\n",
              " highest score from each concept so that the summary would include\n",
              " \n",
              " CONCLUSIONS\n",
              " \n",
              " In this paper, we applied several text summarization methods to\n",
              " Thai Travel News based on keyword scored in Thai language by\n",
              " extracting the most relevant sentences from the original document.,\n",
              " ACM ISBN 978-1-4503-7539-9/20/08â€¦$15.00\n",
              " \n",
              " DOI: https://doi.org/10.1145/3417473.3417479\n",
              " \n",
              " Duangdao.W@Chula.ac.th\n",
              " \n",
              " sentences to be included in the summary.,\n",
              " 3.3\n",
              " \n",
              " (2)\n",
              " \n",
              " A = ğ‘Šğ‘Šğ‘Šğ‘Š\n",
              " \n",
              " Factors W and H can be found by solving the optimization problem\n",
              " as follows, whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— â‰¥ 0, ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– â‰¥ 0.,\n",
              " Number of\n",
              " Sentences\n",
              " 58\n",
              " 58\n",
              " \n",
              " Dataset\n",
              " \n",
              " S5\n",
              " \n",
              " Table 2.,\n",
              " Thai text summarization efficiency of 5 models\n",
              " Figure 2 shows the Thai text summarization efficiency of 5 models:\n",
              " (1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\n",
              " score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\n",
              " sentence score by Murray, G. et al. applied to 400 Thai travel news,\n",
              " divided into 5 sets of 80 news each, with the varied compression\n",
              " rates of 20%, 30% and 40%.,\n",
              " Overall Sentence Language of each Dataset\n",
              " \n",
              " S4\n",
              " \n",
              " DATA PREPARATION\n",
              " ,\n",
              " 3.2\n",
              " \n",
              " A â‰ˆ ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
              " \n",
              " of the related singular value over the sum of all singular values, for\n",
              " each concept.\n",
              " ,\n",
              " =\n",
              " \n",
              " âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–\n",
              " ğ‘Ÿğ‘Ÿ\n",
              " âˆ‘ğ‘ğ‘=1 âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘\n",
              " \n",
              " (5)\n",
              " ,\n",
              " Document summarization pipeline based on LSA\n",
              " and NMF\n",
              " \n",
              " S2\n",
              " \n",
              " For sentence selection by K-means clustering, we grouped similar\n",
              " sentences into the same cluster using the following steps:\n",
              " 1.,\n",
              " The value was decided by getting the percentage\n",
              " \n",
              " Non-negative Matrix Factorization\n",
              " \n",
              " Non-negative Matrix Factorization (NMF) is a method of matrix\n",
              " factorization subject to the non-negative constraint.,\n",
              " = ï¿½ ï¿½ ï¿½ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘– âˆ’ ï¿½ ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘– ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ï¿½\n",
              " \n",
              " (3)\n",
              " \n",
              " NMF and LSA are both matrix factorization algorithms.,\n",
              " 3.5\n",
              " \n",
              " Cosine Similarity\n",
              " \n",
              " Cosine similarity [18] is a widely used method to measure the\n",
              " similarity between vectors representing the documents.,\n",
              " 3.\n",
              " \n",
              " PREPROCESSING FOR THAI TEXT\n",
              " \n",
              " The first step for working with Thai Text is word tokenization.,\n",
              " For the Generic Sentence Relevance score, the best F1 score for the\n",
              " compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\n",
              " respectively and the best F1 scores for all compression rates were\n",
              " from the approach of Murray, G. et al.\n",
              " \n",
              " Figure 2.,\n",
              " Comparison of Thai word tokenization programs\n",
              " Tools\n",
              " \n",
              " F1 Score\n",
              " \n",
              " Validate\n",
              " PyICU,\n",
              " In this paper, we applied LSA and NMF on the Thai Travel News\n",
              " dataset for calculating the semantic weights, which represented the\n",
              " relationship between sentences and words in order to select the\n",
              " representative sentences for summarization.\n",
              " ,\n",
              " 6. EXPERIMENT AND RESULTS\n",
              " 6.1 Performance Evaluations Measure\n",
              " \n",
              " 7.\n",
              " ,\n",
              " S3\n",
              " \n",
              " 4.\n",
              " \n",
              " Figure 1.,\n",
              " 2.\n",
              " \n",
              " RELATED WORKS\n",
              " ,\n",
              " 8. ACKNOWLEDGMENTS\n",
              " \n",
              " We would like to thank the department of computer engineering,\n",
              " faculty of engineering, Chulalongkorn University for providing\n",
              " computing facilities.\n",
              " ,\n",
              " (4)\n",
              " \n",
              " ğ‘–ğ‘–=1\n",
              " \n",
              " ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ),\n",
              " As\n",
              " multiple important sentences could be selected from a more\n",
              " important concept, Murray, G. et al. outperformed both Gong, Y. et\n",
              " al. and the GRS method.\n",
              " ,\n",
              " From these experiments, we concluded that keyword\n",
              " scored calculation by LSA with sentence selection by GRS is the\n",
              " best algorithm for summarizing Thai Travel News, compared with\n",
              " human with the best compression rate of 20%.\n",
              " ,\n",
              " The average numbers of sentences per news of the 5 sets\n",
              " were 21, 16, 15, 13 and 13 sentences, respectively.\n",
              " ,\n",
              " In this experiment, we compared 5\n",
              " models for 5 rounds with Thai travel news using the compression\n",
              " rates of 20%, 30% and 40% and reported the rate and method that\n",
              " produced the best result from the experiment.\n",
              " ,\n",
              " The precision shows the correctness of the\n",
              " extracted sentences and the recall reflects the number of good\n",
              " sentences missed by the method.\n",
              " ,\n",
              " In this section, we demonstrate our pipeline (Figure 1) used for text\n",
              " summarization to generate a summary for a Thai travel news.\n",
              " ,\n",
              " 5. Select a sentence with the maximum similarity score with\n",
              " the centroid of the group and add it into the summary.\n",
              " ,\n",
              " The original matrix A can be separated into three\n",
              " matrices, where U is the m x r (words x extracted concept) matrix,\n",
              " V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\n",
              " r x r diagonal matrix, which can be reconstructed to find the original\n",
              " matrix A. The SVD can be represented in Eq.,\n",
              " This research focused on the sentence extraction function based on\n",
              " keyword score calculation then selecting important sentences based\n",
              " on the Generic Sentence Relevance score (GRS), calculated from\n",
              " Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
              " Factorization (NMF).,\n",
              " 3.4 Generic document summarization by\n",
              " NMF\n",
              " \n",
              " Lee, J., et al. proposed Eq.,\n",
              " In this paper, we deployed cosine similarity to measure the similarity\n",
              " of sentences in K-means clustering.\n",
              " ,\n",
              " Sentences\n",
              " with the maximum score will be selected into the summary.\n",
              " ,\n",
              " We concluded that keyword scored calculation by LSA\n",
              " with sentence selection by Generic Sentence Relevance score by\n",
              " Murray, G. et al. was the best algorithm while the best compression\n",
              " rate of all models was 20%, for summarizing Thai Travel News\n",
              " compared with humans.,\n",
              " For sentence selection, we used Gong, Y. et al. and\n",
              " Murray, G. et al. approaches for calculating weight of the sentence\n",
              " scores then selected sentences with the highest scores into the\n",
              " summary.,\n",
              " Therefore, they proposed\n",
              " that NMF provided a greater possibility for extracting important\n",
              " sentences.\n",
              " ,\n",
              " We then evaluated the performance of\n",
              " text summarization methods which were LSA and NMF by\n",
              " comparing their results with the summaries manually curated by\n",
              " two experts from the Faculty of Liberal Arts, Ubon Ratchathani\n",
              " University.\n",
              " ,\n",
              " This meant 80%, 70% and 60% of the\n",
              " sentences will be selected into the summary.\n",
              " ,\n",
              " (2) for NMF.\n",
              " \n",
              " sentences from all concepts.,\n",
              " This model with the compression rate of 20%\n",
              " got the highest score because Murray G. et al. method determined\n",
              " the number of sentences to be extracted from each concept based on\n",
              " the importance of that concept.]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_summary = [word.text for word in summary]"
      ],
      "metadata": {
        "id": "IUqyL4Ddpr28"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ' '.join(final_summary)"
      ],
      "metadata": {
        "id": "Wy4jaUf3ptqN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceN2d8YopvPW",
        "outputId": "b5c612a0-3d0e-42c5-fcd8-5b92f6e39331"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive Text Summarization for Thai Travel News\n",
            "Based on Keyword Scored in Thai Language\n",
            "Sarunya Nathonghor\n",
            "\n",
            "Duangdao Wichadakul\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Sarunya.N@Student.Chula.ac.th\n",
            "ABSTRACT\n",
            "\n",
            "In recent years, people are seeking for a solution to improve text\n",
            "summarization for Thai language. Although several solutions such\n",
            "as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
            "models, etc., have been proposed, research results in Thai text\n",
            "summarization were restricted due to limited corpus in Thai\n",
            "language with complex grammar. This paper applied a text\n",
            "summarization system for Thai travel news based on keyword\n",
            "scored in Thai language by extracting the most relevant sentences\n",
            "from the original document. We compared LSA and Non-negative\n",
            "Matrix Factorization (NMF) to find the algorithm that is suitable\n",
            "with Thai travel news. The suitable compression rates for Generic\n",
            "Sentence Relevance score (GRS) and K-means clustering were also\n",
            "evaluated. From these experiments, we concluded that keyword\n",
            "scored calculation by LSA with sentence selection by GRS is the\n",
            "best algorithm for summarizing Thai Travel News, compared with\n",
            "human with the best compression rate of 20%.\n",
            "\n",
            "CCS Concepts\n",
            "\n",
            "â€¢ Information systems â Information retrieval â Retrieval\n",
            "tasks and goalsâ Summarization\n",
            "\n",
            "Keywords\n",
            "\n",
            "Text summarization; extractive summarization; non-negative\n",
            "matrix factorization\n",
            "\n",
            "1.\n",
            "\n",
            "INTRODUCTION\n",
            "\n",
            "Daily newspaper has abundant of data that users do not have\n",
            "enough time for reading them. It is difficult to identify the relevant\n",
            "information to satisfy the information needed by users. Automatic\n",
            "summarization can reduce the problem of information overloading\n",
            "and it has been proposed previously in English and other languages.\n",
            "However, there were only a few research results in Thai text\n",
            "summarization due to the lack of corpus in Thai language and the\n",
            "complicated grammar.\n",
            "Text Summarization [1] is a technique for summarizing the content\n",
            "of the documents. It consists of three steps: 1) create an\n",
            "intermediate representation of the input text, 2) calculate score for\n",
            "the sentences based on the concepts, and 3) choose important\n",
            "Permission to make digital or hard copies of all or part of this work for\n",
            "personal or classroom use is granted without fee provided that copies are\n",
            "not made or distributed for profit or commercial advantage and that copies\n",
            "bear this notice and the full citation on the first page. Copyrights for\n",
            "components of this work owned by others than ACM must be honored.\n",
            "Abstracting with credit is permitted. To copy otherwise, or republish, to\n",
            "post on servers or to redistribute to lists, requires prior specific permission\n",
            "and/or a fee. Request permissions from Permissions@acm.org.\n",
            "ITCC 2020, August 12â€“14, 2020, Kuala Lumpur, Malaysia\n",
            "Â© 2020 Association for Computing Machinery.\n",
            "ACM ISBN 978-1-4503-7539-9/20/08â€¦$15.00\n",
            "\n",
            "DOI: https://doi.org/10.1145/3417473.3417479\n",
            "\n",
            "Duangdao.W@Chula.ac.th\n",
            "\n",
            "sentences to be included in the summary. Text summarization can\n",
            "be divided into 2 approaches. The first approach is the extractive\n",
            "summarization, which relies on a method for extracting words and\n",
            "searching for keywords from the original document. The second\n",
            "approach is the abstractive summarization, which analyzes words\n",
            "by linguistic principles with transcription or interpretation from the\n",
            "original document. This approach implies more effective and\n",
            "accurate summary than the extractive methods. However, with the\n",
            "lack of Thai corpus, we chose to apply an extractive summarization\n",
            "method for Thai text summarization.\n",
            "This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF). We also tried using K-means clustering for\n",
            "document summarization. In this experiment, we compared 5\n",
            "models for 5 rounds with Thai travel news using the compression\n",
            "rates of 20%, 30% and 40% and reported the rate and method that\n",
            "produced the best result from the experiment.\n",
            "\n",
            "2.\n",
            "\n",
            "RELATED WORKS\n",
            "\n",
            "In recent years, several models in Thai Text summarization have\n",
            "been introduced. Suwanno, N. et al. [2] proposed a Thai text\n",
            "summarization that extracted a paragraph from a document based\n",
            "on Thai compound nouns, term frequency method, and headline\n",
            "score for generating a summary. Chongsuntornsri, A., et al. [3]\n",
            "proposed a new approach for Text summarization in Thai based on\n",
            "content- and graph-based with the use of Topic Sensitive PageRank\n",
            "algorithm for summarizing and ranking of text segments.\n",
            "Jaruskulchai C., et al. [4] proposed a method to summarize\n",
            "documents by extracting important sentences from combining the\n",
            "specific properties (Local Property) and the overall properties\n",
            "(Global Property) of the sentences. The overall properties were\n",
            "based on the relationship between sentences in the document. From\n",
            "their experiments, the summarization of the industrial news got\n",
            "60% precision, 44% recall, and 50.9% F-measure, the general news\n",
            "got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
            "the fashion magazines got 53.0% precision, 33.0% recall, and\n",
            "40.4% F-measure.\n",
            "Mani, I., et al. [5] proposed techniques of text summarization by\n",
            "using word frequency in the document and calculated the weight of\n",
            "word to create a keyword group. They then calculated the cosine\n",
            "similarity of sentences. The researcher used A* search algorithm to\n",
            "find the shortest sequence of sentences from keyword group by\n",
            "topic calculation, sentence segmentation and word grouping. The\n",
            "sequence of sentences that were in the main group were selected as\n",
            "important sentences. Their summarization of the agricultural news\n",
            "got 68.57% precision, 51.95% recall and 56.72% F-measure.\n",
            "Lee, J., et al. [6] proposed a document summarization method using\n",
            "Non-negative Matrix Factorization (NMF). They compared\n",
            "\n",
            "\fbetween Latent Semantic Analysis (LSA) and NMF to find the\n",
            "weight of each word and calculated the summation of weights. The\n",
            "important sentences were ranked and selected into the summary\n",
            "based on their summed weight. Based on LSA, they found many\n",
            "weights with zero and negative values. However, when applied\n",
            "NMF, they found only the positive values and the scope of the\n",
            "semantic featuresâ€™ meaning was narrow. Therefore, they proposed\n",
            "that NMF provided a greater possibility for extracting important\n",
            "sentences.\n",
            "\n",
            "3.\n",
            "\n",
            "PREPROCESSING FOR THAI TEXT\n",
            "\n",
            "The first step for working with Thai Text is word tokenization. Even\n",
            "though Thai writing system has no delimiters to indicate word\n",
            "boundaries together with many rules for word segmentation, several\n",
            "Thai word tokenization programs have been proposed. Table 1\n",
            "shows F1 score of the recent programs trained and tested by one of\n",
            "our laboratory members with the data from BEST2010 corpus [7].\n",
            "Cutkum [8] got the highest F1 score, hence, we used Cutkum for this\n",
            "step.\n",
            "Table 1. Comparison of Thai word tokenization programs\n",
            "Tools\n",
            "\n",
            "F1 Score\n",
            "\n",
            "Validate\n",
            "PyICU [9]\n",
            "\n",
            "Article\n",
            "100\n",
            "0.6155\n",
            "\n",
            "Encyclopedia\n",
            "100\n",
            "0.6932\n",
            "\n",
            "News\n",
            "100\n",
            "0.5987\n",
            "\n",
            "Novel\n",
            "100\n",
            "0.6800\n",
            "\n",
            "Lexto [10]\n",
            "\n",
            "0.7267\n",
            "\n",
            "0.7709\n",
            "\n",
            "0.6994\n",
            "\n",
            "0.7701\n",
            "\n",
            "Cutkum\n",
            "wordcutpy [11]\n",
            "\n",
            "0.9322\n",
            "0.6212\n",
            "\n",
            "0.9299\n",
            "0.6286\n",
            "\n",
            "0.8987\n",
            "0.6571\n",
            "\n",
            "0.7140\n",
            "0.6247\n",
            "\n",
            "cunlp [12]\n",
            "\n",
            "0.6910\n",
            "\n",
            "0.6172\n",
            "\n",
            "0.5748\n",
            "\n",
            "0.0000\n",
            "\n",
            "SWATH [13]\n",
            "\n",
            "0.6347\n",
            "\n",
            "0.6858\n",
            "\n",
            "0.6200\n",
            "\n",
            "0.6867\n",
            "\n",
            "3.1\n",
            "\n",
            "Latent Semantic Analysis\n",
            "\n",
            "Latent Semantic Analysis (LSA) [14] is the algorithm, which\n",
            "reduces the dimensionality of term document. The algorithm\n",
            "creates a matrix by using word frequency, applies the singular value\n",
            "decomposition (SVD) [15], and then finds closely related terms and\n",
            "documents. The original matrix A can be separated into three\n",
            "matrices, where U is the m x r (words x extracted concept) matrix,\n",
            "V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\n",
            "r x r diagonal matrix, which can be reconstructed to find the original\n",
            "matrix A. The SVD can be represented in Eq. (1).\n",
            "\n",
            "3.2\n",
            "\n",
            "A â‰ˆ ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
            "\n",
            "of the related singular value over the sum of all singular values, for\n",
            "each concept.\n",
            "\n",
            "3.3\n",
            "\n",
            "(2)\n",
            "\n",
            "A = ğ‘Šğ‘Šğ‘Šğ‘Š\n",
            "\n",
            "Factors W and H can be found by solving the optimization problem\n",
            "as follows, whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— â‰¥ 0, ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– â‰¥ 0.\n",
            "ğ‘šğ‘š\n",
            "\n",
            "ğ‘›ğ‘›\n",
            "\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "ğ‘—ğ‘—=1 ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘™ğ‘™=1\n",
            "\n",
            "2\n",
            "\n",
            "ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ğ¹ğ¹(ğ‘Šğ‘Š, ğ»ğ») = || ğ´ğ´ âˆ’ ğ‘Šğ‘Šğ‘Šğ‘Š ||2ğ¹ğ¹ = ï¿½ ï¿½ ï¿½ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘– âˆ’ ï¿½ ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘– ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ï¿½\n",
            "\n",
            "(3)\n",
            "\n",
            "NMF and LSA are both matrix factorization algorithms. However,\n",
            "when using NMF to find keywords, NMF will return the keywords\n",
            "that are closely related because its components have only nonnegative values. As LSA has both positive and negative values as\n",
            "well as some zeroes, it gets a wider distribution. The semantic\n",
            "feature represents a concept of meaning for root of words that have\n",
            "a relationship. For example, man, human, male and adult have the\n",
            "same semantic, hence their semantic values are close.\n",
            "In this paper, we applied LSA and NMF on the Thai Travel News\n",
            "dataset for calculating the semantic weights, which represented the\n",
            "relationship between sentences and words in order to select the\n",
            "representative sentences for summarization.\n",
            "\n",
            "3.4 Generic document summarization by\n",
            "NMF\n",
            "\n",
            "Lee, J., et al. proposed Eq. (4) and Eq. (5) to select a number of\n",
            "sentences based on NMF, which got the highest semantic weight\n",
            "values, where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘—ğ‘—.\n",
            "Generic Relevance of jth sentence\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "(1)\n",
            "\n",
            "= ï¿½ ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– )\n",
            "\n",
            "Document summarization using LSA\n",
            "\n",
            "Gong, Y. et al. [16] proposed a document summarization based on\n",
            "SVD matrices. In our work, after applying SVD to matrix A, ğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
            "matrix used for selecting the important sentences. The cell value of\n",
            "the matrix shows the relationship between sentence and extracted\n",
            "concepts. A sentence with the highest cell value of each concept\n",
            "will be selected into the summary starting from the most important\n",
            "concept. The total number of sentences in the summary will be\n",
            "equal to the number all detected concepts.\n",
            "Murray, G. et al. [17] proposed a document summarization based\n",
            "on SVD matrices using ğ‘‰ğ‘‰ ğ‘‡ğ‘‡ and Î£ matrices for sentence selection.\n",
            "The authors proposed that more than one sentence could be\n",
            "collected from the more important concepts. The decision of how\n",
            "many sentences would be collected from each concept depending\n",
            "on the Î£ matrix. The value was decided by getting the percentage\n",
            "\n",
            "Non-negative Matrix Factorization\n",
            "\n",
            "Non-negative Matrix Factorization (NMF) is a method of matrix\n",
            "factorization subject to the non-negative constraint. Lee, J., et al.\n",
            "proposed the model based on NMF for document summarization.\n",
            "NMF decomposes a non-negative matrix ğ´ğ´ âˆˆ ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š into two nonnegative matrices. The first matrix ğ‘šğ‘š x ğ‘Ÿğ‘Ÿ is a non-negative semantic\n",
            "feature matrix (NSFM), ğ‘Šğ‘Š . The second matrix ğ‘Ÿğ‘Ÿ x ğ‘›ğ‘› is a nonnegative semantic variable matrix (NSVM), ğ»ğ». So, we have ğ‘Šğ‘Š âˆˆ\n",
            "ğ‘…ğ‘…ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š and ğ»ğ» âˆˆ ğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ and both terms are non-negative as shown in\n",
            "Eq. (2) and Eq. (3).\n",
            "\n",
            "(4)\n",
            "\n",
            "ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) =\n",
            "\n",
            "âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "âˆ‘ğ‘ğ‘=1 âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘\n",
            "\n",
            "(5)\n",
            "\n",
            "The ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) is the relative relevance of the ith semantic feature\n",
            "(ğ‘Šğ‘Šğ‘–ğ‘– ), where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘ğ‘ and\n",
            "ğ»ğ»ğ‘ğ‘ğ‘ğ‘ is the weight of the topic ğ‘ğ‘ in the sentence ğ‘ğ‘. The sentences\n",
            "can be ranked by Generic Relevance Sentence scores. Sentences\n",
            "with the maximum score will be selected into the summary.\n",
            "\n",
            "3.5\n",
            "\n",
            "Cosine Similarity\n",
            "\n",
            "Cosine similarity [18] is a widely used method to measure the\n",
            "similarity between vectors representing the documents. The result of\n",
            "cosine similarity is ranging from 0 to 1. If it is closer to 1, that means\n",
            "both vectors are similar. Eq. (6) and Eq. (7) represents the cosine\n",
            "\n",
            "\fsimilarity equation, where cos(Î¸) is the dot product between vectors\n",
            "of sentences A and B and divided by the product of the two vectors'\n",
            "lengths.\n",
            "In this paper, we deployed cosine similarity to measure the similarity\n",
            "of sentences in K-means clustering.\n",
            "\n",
            "Aâˆ™B\n",
            "||A|| ||B||\n",
            "âˆ‘ni=1 Ai Bi\n",
            "\n",
            "Similarity(A, B) = cos(Î¸) =\n",
            "\n",
            "(7)\n",
            "\n",
            "K-means Clustering\n",
            "\n",
            "15\n",
            "\n",
            "67\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "13\n",
            "13\n",
            "\n",
            "55\n",
            "38\n",
            "\n",
            "Table 2 shows the overall number of sentences of news within each\n",
            "dataset. The average numbers of sentences per news of the 5 sets\n",
            "were 21, 16, 15, 13 and 13 sentences, respectively.\n",
            "\n",
            "5.\n",
            "PIPELINE FOR GENERATING\n",
            "SUMMARIES\n",
            "In this section, we demonstrate our pipeline (Figure 1) used for text\n",
            "summarization to generate a summary for a Thai travel news.\n",
            "\n",
            "Word\n",
            "\n",
            "S9\n",
            "\n",
            "6\n",
            "\n",
            "Round 4\n",
            "Round 5\n",
            "\n",
            "Table 3. Example of Word by Sentence Matrix A\n",
            "S8\n",
            "\n",
            "Round 3\n",
            "\n",
            "Avg. Number\n",
            "of Sentences\n",
            "\n",
            "S7\n",
            "\n",
            "21\n",
            "16\n",
            "\n",
            "Round 1\n",
            "Round 2\n",
            "\n",
            "Min.\n",
            "Number of\n",
            "Sentences\n",
            "\n",
            "S6\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "Max.\n",
            "Number of\n",
            "Sentences\n",
            "58\n",
            "58\n",
            "\n",
            "Dataset\n",
            "\n",
            "S5\n",
            "\n",
            "Table 2. Overall Sentence Language of each Dataset\n",
            "\n",
            "S4\n",
            "\n",
            "DATA PREPARATION\n",
            "\n",
            "The standard data sets in Thai language are unavailable for\n",
            "evaluating text summarization system. Therefore, we collected 400\n",
            "Thai travel news from Thairath and Manager online newspapers to\n",
            "be used as datasets for our experiments. We split 400 travel news\n",
            "into 5 sets of 80 news each. We then evaluated the performance of\n",
            "text summarization methods which were LSA and NMF by\n",
            "comparing their results with the summaries manually curated by\n",
            "two experts from the Faculty of Liberal Arts, Ubon Ratchathani\n",
            "University.\n",
            "\n",
            "The open-source python libraries such as numpy [19] and sklearn\n",
            "[20] were used in our system. We converted the Thai travel news\n",
            "obtained from Thairath and Manager online newspapers to plain\n",
            "text. Then, the sentences of each news were segmented by human\n",
            "with the following format: Si = â€˜xxxâ€™, where Si represents the order\n",
            "of the sentence in the original document and â€˜xxxâ€™ represents the\n",
            "content of that sentence. After removing stop words and duplicate\n",
            "words, we built a document term matrix or matrix A then applied\n",
            "SVD and NMF to the matrix. Then, we used python modules\n",
            "numpy.linalg.svd to calculate SVD and sklearn.decomposition to\n",
            "calculate NMF. For sentence selection, we used Gong, Y. et al. and\n",
            "Murray, G. et al. approaches for calculating weight of the sentence\n",
            "scores then selected sentences with the highest scores into the\n",
            "summary. For keyword score calculation of NMF, we calculated\n",
            "the keyword score from Eq. (5) and then selected the sentence with\n",
            "the highest score from each concept. The python module\n",
            "sklearn.cluster was used for K-means clustering. The selected\n",
            "sentences from all approaches were in the same order as the original\n",
            "document. In this paper, we performed the 20%, 30% and 40%\n",
            "document compression. This meant 80%, 70% and 60% of the\n",
            "sentences will be selected into the summary.\n",
            "\n",
            "S3\n",
            "\n",
            "4.\n",
            "\n",
            "Figure 1. Document summarization pipeline based on LSA\n",
            "and NMF\n",
            "\n",
            "S2\n",
            "\n",
            "For sentence selection by K-means clustering, we grouped similar\n",
            "sentences into the same cluster using the following steps:\n",
            "1. Randomly select K sentences as the representative of K\n",
            "groups. K in this paper is the number of sentences that\n",
            "will be selected into the summary.\n",
            "2. Calculate centroid of each group by using the value of\n",
            "sentence vector from V matrix for LSA and ğ»ğ»ğ‘‡ğ‘‡ matrix\n",
            "for NMF.\n",
            "3. Use cosine similarity to calculate sentence similarity\n",
            "between a sentence and the centroid of each group. Then\n",
            "assign that sentence to the group with the highest\n",
            "similarity.\n",
            "4. Repeat steps 2-3 until all sentences are assigned to a\n",
            "group, no sentences change the group, or the similarity\n",
            "between sentences and their centroid is close.\n",
            "5. Select a sentence with the maximum similarity score with\n",
            "the centroid of the group and add it into the summary.\n",
            "\n",
            "S1\n",
            "\n",
            "3.6\n",
            "\n",
            "Aâˆ™B\n",
            "=\n",
            "n\n",
            "n\n",
            "||A|| ||B||\n",
            "ï¿½âˆ‘i=1\n",
            "A2i ï¿½âˆ‘i=1\n",
            "Bi2\n",
            "\n",
            "(6)\n",
            "\n",
            "Mr.Yontas\n",
            "ak\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Supason\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Tourism\n",
            "Authority\n",
            "of Thailand\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "\fTable 3 demonstrates an example of a matrix ğ´ğ´, constructed from\n",
            "word count by sentence of a Thai travel news. It was composed of\n",
            "98 words and 9 sentences. This matrix ğ´ğ´ was then applied with the\n",
            "LSA and NMF. The sentence vectors were calculated from the term\n",
            "weight and the semantic feature vectors from Eq. (1) for LSA and\n",
            "Eq. (2) for NMF.\n",
            "\n",
            "sentences from all concepts. The Generic Sentence Relevance score\n",
            "for NMF also collected one sentence for each concept, the same as\n",
            "Gong, Y. et al. but with the highest score calculated by Eq. (5). As\n",
            "multiple important sentences could be selected from a more\n",
            "important concept, Murray, G. et al. outperformed both Gong, Y. et\n",
            "al. and the GRS method.\n",
            "\n",
            "6. EXPERIMENT AND RESULTS\n",
            "6.1 Performance Evaluations Measure\n",
            "\n",
            "7.\n",
            "\n",
            "We evaluated the results of the summarization by using standard\n",
            "accuracy, precision, recall, and F1 score [21]. These measurements\n",
            "quantify the differences between the summary from human and the\n",
            "experimental methods. The precision shows the correctness of the\n",
            "extracted sentences and the recall reflects the number of good\n",
            "sentences missed by the method.\n",
            "\n",
            "6. 2 Experiment Results\n",
            "\n",
            "In this experimental set, we would like to explore how the different\n",
            "sentence selection methods: the Generic Sentence Relevance score\n",
            "and K-means clustering, affected the text summarization result.\n",
            "For K-means clustering, both SVD and NMF had similar\n",
            "summarization efficiency. The F1 score of SVD with K-means\n",
            "clustering was 0.83, 0.72, and 0.62 for the compression rate of 20%,\n",
            "30%, and 40%. For the NMF with K-means clustering, the F1 score\n",
            "for the three compression rates was 0.83, 0.74 and 0.64.\n",
            "For the Generic Sentence Relevance score, the best F1 score for the\n",
            "compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\n",
            "respectively and the best F1 scores for all compression rates were\n",
            "from the approach of Murray, G. et al.\n",
            "\n",
            "Figure 2. Thai text summarization efficiency of 5 models\n",
            "Figure 2 shows the Thai text summarization efficiency of 5 models:\n",
            "(1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\n",
            "score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\n",
            "sentence score by Murray, G. et al. applied to 400 Thai travel news,\n",
            "divided into 5 sets of 80 news each, with the varied compression\n",
            "rates of 20%, 30% and 40%.\n",
            "From this experiment, the best model based on keyword score for\n",
            "Thai travel news summarization was SVD with sentence selection\n",
            "by Murray, G. et al. This model with the compression rate of 20%\n",
            "got the highest score because Murray G. et al. method determined\n",
            "the number of sentences to be extracted from each concept based on\n",
            "the importance of that concept. The method of Gong, Y. et al., on\n",
            "the other hand was proposed to select only one sentence with the\n",
            "highest score from each concept so that the summary would include\n",
            "\n",
            "CONCLUSIONS\n",
            "\n",
            "In this paper, we applied several text summarization methods to\n",
            "Thai Travel News based on keyword scored in Thai language by\n",
            "extracting the most relevant sentences from the original document.\n",
            "We compared LSA and NMF together with different sentence\n",
            "selection methods, to find the algorithm suitable with this paper's\n",
            "data source. We concluded that keyword scored calculation by LSA\n",
            "with sentence selection by Generic Sentence Relevance score by\n",
            "Murray, G. et al. was the best algorithm while the best compression\n",
            "rate of all models was 20%, for summarizing Thai Travel News\n",
            "compared with humans.\n",
            "In future work, we plan to perform the experiments with different\n",
            "types of documents and improve word segmentation of compound\n",
            "nouns that was not handled by Cutkum.\n",
            "\n",
            "8. ACKNOWLEDGMENTS\n",
            "\n",
            "We would like to thank the department of computer engineering,\n",
            "faculty of engineering, Chulalongkorn University for providing\n",
            "computing facilities.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qei3mvKpxkY",
        "outputId": "295c01f3-d742-49c5-cffc-29840b959cdd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S1\n",
            "\n",
            "3.6\n",
            "\n",
            "Aâˆ™B\n",
            "=\n",
            "n\n",
            "n\n",
            "||A|| ||B||\n",
            "ï¿½âˆ‘i=1\n",
            "A2i ï¿½âˆ‘i=1\n",
            "Bi2\n",
            "\n",
            "(6)\n",
            "\n",
            "Mr.Yontas\n",
            "ak\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Supason\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Tourism\n",
            "Authority\n",
            "of Thailand\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "\fTable 3 demonstrates an example of a matrix ğ´ğ´, constructed from\n",
            "word count by sentence of a Thai travel news. [10]\n",
            "\n",
            "0.7267\n",
            "\n",
            "0.7709\n",
            "\n",
            "0.6994\n",
            "\n",
            "0.7701\n",
            "\n",
            "Cutkum\n",
            "wordcutpy [11]\n",
            "\n",
            "0.9322\n",
            "0.6212\n",
            "\n",
            "0.9299\n",
            "0.6286\n",
            "\n",
            "0.8987\n",
            "0.6571\n",
            "\n",
            "0.7140\n",
            "0.6247\n",
            "\n",
            "cunlp [12]\n",
            "\n",
            "0.6910\n",
            "\n",
            "0.6172\n",
            "\n",
            "0.5748\n",
            "\n",
            "0.0000\n",
            "\n",
            "SWATH Example of Word by Sentence Matrix A\n",
            "S8\n",
            "\n",
            "Round 3\n",
            "\n",
            "Avg. Number\n",
            "of Sentences\n",
            "\n",
            "S7\n",
            "\n",
            "21\n",
            "16\n",
            "\n",
            "Round 1\n",
            "Round 2\n",
            "\n",
            "Min.\n",
            "Number of\n",
            "Sentences\n",
            "\n",
            "S6\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "Max.\n",
            " Ai Bi\n",
            "\n",
            "Similarity(A, B) = cos(Î¸) =\n",
            "\n",
            "(7)\n",
            "\n",
            "K-means Clustering\n",
            "\n",
            "15\n",
            "\n",
            "67\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "13\n",
            "13\n",
            "\n",
            "55\n",
            "38\n",
            "\n",
            "Table 2 shows the overall number of sentences of news within each\n",
            "dataset. CCS Concepts\n",
            "\n",
            "â€¢ Information systems â Information retrieval â Retrieval\n",
            "tasks and goalsâ Summarization\n",
            "\n",
            "Keywords\n",
            "\n",
            "Text summarization; extractive summarization; non-negative\n",
            "matrix factorization\n",
            "\n",
            "1.\n",
            "\n",
            "INTRODUCTION\n",
            "\n",
            "Daily newspaper has abundant of data that users do not have\n",
            "enough time for reading them. [13]\n",
            "\n",
            "0.6347\n",
            "\n",
            "0.6858\n",
            "\n",
            "0.6200\n",
            "\n",
            "0.6867\n",
            "\n",
            "3.1\n",
            "\n",
            "Latent Semantic Analysis\n",
            "\n",
            "Latent Semantic Analysis (LSA) ğ‘šğ‘š\n",
            "\n",
            "ğ‘›ğ‘›\n",
            "\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "ğ‘—ğ‘—=1 ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘™ğ‘™=1\n",
            "\n",
            "2\n",
            "\n",
            "ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ğ¹ğ¹(ğ‘Šğ‘Š, ğ»ğ») = || ğ´ğ´ âˆ’ ğ‘Šğ‘Šğ‘Šğ‘Š ||2ğ¹ğ¹ Extractive Text Summarization for Thai Travel News\n",
            "Based on Keyword Scored in Thai Language\n",
            "Sarunya Nathonghor\n",
            "\n",
            "Duangdao Wichadakul\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Sarunya.N@Student.Chula.ac.th\n",
            "ABSTRACT\n",
            "\n",
            "In recent years, people are seeking for a solution to improve text\n",
            "summarization for Thai language. (5) to select a number of\n",
            "sentences based on NMF, which got the highest semantic weight\n",
            "values, where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘—ğ‘—.\n",
            "Generic Relevance of jth sentence\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "(1)\n",
            "\n",
            "= ï¿½ ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– )\n",
            "\n",
            "Document summarization using LSA\n",
            "\n",
            "Gong, Y. et al. [9]\n",
            "\n",
            "Article\n",
            "100\n",
            "0.6155\n",
            "\n",
            "Encyclopedia\n",
            "100\n",
            "0.6932\n",
            "\n",
            "News\n",
            "100\n",
            "0.5987\n",
            "\n",
            "Novel\n",
            "100\n",
            "0.6800\n",
            "\n",
            "Lexto Word\n",
            "\n",
            "S9\n",
            "\n",
            "6\n",
            "\n",
            "Round 4\n",
            "Round 5\n",
            "\n",
            "Table 3. The method of Gong, Y. et al., on\n",
            "the other hand was proposed to select only one sentence with the\n",
            "highest score from each concept so that the summary would include\n",
            "\n",
            "CONCLUSIONS\n",
            "\n",
            "In this paper, we applied several text summarization methods to\n",
            "Thai Travel News based on keyword scored in Thai language by\n",
            "extracting the most relevant sentences from the original document.\n",
            " ACM ISBN 978-1-4503-7539-9/20/08â€¦$15.00\n",
            "\n",
            "DOI: https://doi.org/10.1145/3417473.3417479\n",
            "\n",
            "Duangdao.W@Chula.ac.th\n",
            "\n",
            "sentences to be included in the summary. 3.3\n",
            "\n",
            "(2)\n",
            "\n",
            "A = ğ‘Šğ‘Šğ‘Šğ‘Š\n",
            "\n",
            "Factors W and H can be found by solving the optimization problem\n",
            "as follows, whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— â‰¥ 0, ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– â‰¥ 0.\n",
            " Number of\n",
            "Sentences\n",
            "58\n",
            "58\n",
            "\n",
            "Dataset\n",
            "\n",
            "S5\n",
            "\n",
            "Table 2. Thai text summarization efficiency of 5 models\n",
            "Figure 2 shows the Thai text summarization efficiency of 5 models:\n",
            "(1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\n",
            "score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\n",
            "sentence score by Murray, G. et al. applied to 400 Thai travel news,\n",
            "divided into 5 sets of 80 news each, with the varied compression\n",
            "rates of 20%, 30% and 40%.\n",
            " Overall Sentence Language of each Dataset\n",
            "\n",
            "S4\n",
            "\n",
            "DATA PREPARATION\n",
            "\n",
            " 3.2\n",
            "\n",
            "A â‰ˆ ğ‘ˆğ‘ˆğ‘ˆğ‘ˆğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
            "\n",
            "of the related singular value over the sum of all singular values, for\n",
            "each concept.\n",
            "\n",
            " =\n",
            "\n",
            "âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "âˆ‘ğ‘ğ‘=1 âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘\n",
            "\n",
            "(5)\n",
            "\n",
            " Document summarization pipeline based on LSA\n",
            "and NMF\n",
            "\n",
            "S2\n",
            "\n",
            "For sentence selection by K-means clustering, we grouped similar\n",
            "sentences into the same cluster using the following steps:\n",
            "1. The value was decided by getting the percentage\n",
            "\n",
            "Non-negative Matrix Factorization\n",
            "\n",
            "Non-negative Matrix Factorization (NMF) is a method of matrix\n",
            "factorization subject to the non-negative constraint. = ï¿½ ï¿½ ï¿½ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘– âˆ’ ï¿½ ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘– ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ï¿½\n",
            "\n",
            "(3)\n",
            "\n",
            "NMF and LSA are both matrix factorization algorithms. 3.5\n",
            "\n",
            "Cosine Similarity\n",
            "\n",
            "Cosine similarity [18] is a widely used method to measure the\n",
            "similarity between vectors representing the documents. 3.\n",
            "\n",
            "PREPROCESSING FOR THAI TEXT\n",
            "\n",
            "The first step for working with Thai Text is word tokenization. For the Generic Sentence Relevance score, the best F1 score for the\n",
            "compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\n",
            "respectively and the best F1 scores for all compression rates were\n",
            "from the approach of Murray, G. et al.\n",
            "\n",
            "Figure 2. Comparison of Thai word tokenization programs\n",
            "Tools\n",
            "\n",
            "F1 Score\n",
            "\n",
            "Validate\n",
            "PyICU In this paper, we applied LSA and NMF on the Thai Travel News\n",
            "dataset for calculating the semantic weights, which represented the\n",
            "relationship between sentences and words in order to select the\n",
            "representative sentences for summarization.\n",
            "\n",
            " 6. EXPERIMENT AND RESULTS\n",
            "6.1 Performance Evaluations Measure\n",
            "\n",
            "7.\n",
            "\n",
            " S3\n",
            "\n",
            "4.\n",
            "\n",
            "Figure 1. 2.\n",
            "\n",
            "RELATED WORKS\n",
            "\n",
            " 8. ACKNOWLEDGMENTS\n",
            "\n",
            "We would like to thank the department of computer engineering,\n",
            "faculty of engineering, Chulalongkorn University for providing\n",
            "computing facilities.\n",
            "\n",
            " (4)\n",
            "\n",
            "ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) As\n",
            "multiple important sentences could be selected from a more\n",
            "important concept, Murray, G. et al. outperformed both Gong, Y. et\n",
            "al. and the GRS method.\n",
            "\n",
            " From these experiments, we concluded that keyword\n",
            "scored calculation by LSA with sentence selection by GRS is the\n",
            "best algorithm for summarizing Thai Travel News, compared with\n",
            "human with the best compression rate of 20%.\n",
            "\n",
            " The average numbers of sentences per news of the 5 sets\n",
            "were 21, 16, 15, 13 and 13 sentences, respectively.\n",
            "\n",
            " In this experiment, we compared 5\n",
            "models for 5 rounds with Thai travel news using the compression\n",
            "rates of 20%, 30% and 40% and reported the rate and method that\n",
            "produced the best result from the experiment.\n",
            "\n",
            " The precision shows the correctness of the\n",
            "extracted sentences and the recall reflects the number of good\n",
            "sentences missed by the method.\n",
            "\n",
            " In this section, we demonstrate our pipeline (Figure 1) used for text\n",
            "summarization to generate a summary for a Thai travel news.\n",
            "\n",
            " 5. Select a sentence with the maximum similarity score with\n",
            "the centroid of the group and add it into the summary.\n",
            "\n",
            " The original matrix A can be separated into three\n",
            "matrices, where U is the m x r (words x extracted concept) matrix,\n",
            "V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\n",
            "r x r diagonal matrix, which can be reconstructed to find the original\n",
            "matrix A. The SVD can be represented in Eq. This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF). 3.4 Generic document summarization by\n",
            "NMF\n",
            "\n",
            "Lee, J., et al. proposed Eq. In this paper, we deployed cosine similarity to measure the similarity\n",
            "of sentences in K-means clustering.\n",
            "\n",
            " Sentences\n",
            "with the maximum score will be selected into the summary.\n",
            "\n",
            " We concluded that keyword scored calculation by LSA\n",
            "with sentence selection by Generic Sentence Relevance score by\n",
            "Murray, G. et al. was the best algorithm while the best compression\n",
            "rate of all models was 20%, for summarizing Thai Travel News\n",
            "compared with humans.\n",
            " For sentence selection, we used Gong, Y. et al. and\n",
            "Murray, G. et al. approaches for calculating weight of the sentence\n",
            "scores then selected sentences with the highest scores into the\n",
            "summary. Therefore, they proposed\n",
            "that NMF provided a greater possibility for extracting important\n",
            "sentences.\n",
            "\n",
            " We then evaluated the performance of\n",
            "text summarization methods which were LSA and NMF by\n",
            "comparing their results with the summaries manually curated by\n",
            "two experts from the Faculty of Liberal Arts, Ubon Ratchathani\n",
            "University.\n",
            "\n",
            " This meant 80%, 70% and 60% of the\n",
            "sentences will be selected into the summary.\n",
            "\n",
            " (2) for NMF.\n",
            "\n",
            "sentences from all concepts. This model with the compression rate of 20%\n",
            "got the highest score because Murray G. et al. method determined\n",
            "the number of sentences to be extracted from each concept based on\n",
            "the importance of that concept.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write summary to file\n",
        "with open('/content/summarize_result.txt', 'w') as file:\n",
        "    file.write(summary)"
      ],
      "metadata": {
        "id": "AInX8aC9pzfy"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}