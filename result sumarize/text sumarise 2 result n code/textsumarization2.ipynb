{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU2vZO0SDbQP",
        "outputId": "a5fbd347-37d4-411e-ebc5-719ea73090a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.27.1)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (22.3.5)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def calculate_word_frequencies(text):\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "    freqTable = {}\n",
        "\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in stopwords_set:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_scores(text, freqTable):\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence in sentenceValue:\n",
        "                    sentenceValue[sentence] += freq\n",
        "                else:\n",
        "                    sentenceValue[sentence] = freq\n",
        "\n",
        "    return sentenceValue"
      ],
      "metadata": {
        "id": "DgWIiviMDk2V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average_score(sentenceValue):\n",
        "    sumValues = sum(sentenceValue.values())\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "    return average\n",
        "\n",
        "def generate_summary(sentences, sentenceValue, average):\n",
        "    summary = ''\n",
        "    for sentence in sentences:\n",
        "        if sentence in sentenceValue and sentenceValue[sentence] > (1.2 * average):\n",
        "            summary += sentence\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "2aNRwxtGOh_Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve(text):\n",
        "    freqTable = calculate_word_frequencies(text)\n",
        "    sentenceValue = calculate_sentence_scores(text, freqTable)\n",
        "    average = calculate_average_score(sentenceValue)\n",
        "    sentences = sent_tokenize(text)\n",
        "    summary = generate_summary(sentences, sentenceValue, average)\n",
        "\n",
        "    return summary\n",
        "\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = solve(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/solve_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE2FbTmODrnl",
        "outputId": "4d40367b-5c95-4536-c05d-6e789f8b803c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive Text Summarization for Thai Travel News\n",
            "Based on Keyword Scored in Thai Language\n",
            "Sarunya Nathonghor\n",
            "\n",
            "Duangdao Wichadakul\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Department of Computer Engineering\n",
            "Chulalongkorn University\n",
            "Bangkok, Thailand\n",
            "\n",
            "Sarunya.N@Student.Chula.ac.th\n",
            "ABSTRACT\n",
            "\n",
            "In recent years, people are seeking for a solution to improve text\n",
            "summarization for Thai language.Although several solutions such\n",
            "as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\n",
            "models, etc., have been proposed, research results in Thai text\n",
            "summarization were restricted due to limited corpus in Thai\n",
            "language with complex grammar.This paper applied a text\n",
            "summarization system for Thai travel news based on keyword\n",
            "scored in Thai language by extracting the most relevant sentences\n",
            "from the original document.From these experiments, we concluded that keyword\n",
            "scored calculation by LSA with sentence selection by GRS is the\n",
            "best algorithm for summarizing Thai Travel News, compared with\n",
            "human with the best compression rate of 20%.It consists of three steps: 1) create an\n",
            "intermediate representation of the input text, 2) calculate score for\n",
            "the sentences based on the concepts, and 3) choose important\n",
            "Permission to make digital or hard copies of all or part of this work for\n",
            "personal or classroom use is granted without fee provided that copies are\n",
            "not made or distributed for profit or commercial advantage and that copies\n",
            "bear this notice and the full citation on the first page.The first approach is the extractive\n",
            "summarization, which relies on a method for extracting words and\n",
            "searching for keywords from the original document.This research focused on the sentence extraction function based on\n",
            "keyword score calculation then selecting important sentences based\n",
            "on the Generic Sentence Relevance score (GRS), calculated from\n",
            "Latent Semantic Analysis (LSA) and Non-negative Matrix\n",
            "Factorization (NMF).In this experiment, we compared 5\n",
            "models for 5 rounds with Thai travel news using the compression\n",
            "rates of 20%, 30% and 40% and reported the rate and method that\n",
            "produced the best result from the experiment.[2] proposed a Thai text\n",
            "summarization that extracted a paragraph from a document based\n",
            "on Thai compound nouns, term frequency method, and headline\n",
            "score for generating a summary.[4] proposed a method to summarize\n",
            "documents by extracting important sentences from combining the\n",
            "specific properties (Local Property) and the overall properties\n",
            "(Global Property) of the sentences.From\n",
            "their experiments, the summarization of the industrial news got\n",
            "60% precision, 44% recall, and 50.9% F-measure, the general news\n",
            "got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\n",
            "the fashion magazines got 53.0% precision, 33.0% recall, and\n",
            "40.4% F-measure.The researcher used A* search algorithm to\n",
            "find the shortest sequence of sentences from keyword group by\n",
            "topic calculation, sentence segmentation and word grouping.Their summarization of the agricultural news\n",
            "got 68.57% precision, 51.95% recall and 56.72% F-measure.Lee, J., et al.[6] proposed a document summarization method using\n",
            "Non-negative Matrix Factorization (NMF).Comparison of Thai word tokenization programs\n",
            "Tools\n",
            "\n",
            "F1 Score\n",
            "\n",
            "Validate\n",
            "PyICU [9]\n",
            "\n",
            "Article\n",
            "100\n",
            "0.6155\n",
            "\n",
            "Encyclopedia\n",
            "100\n",
            "0.6932\n",
            "\n",
            "News\n",
            "100\n",
            "0.5987\n",
            "\n",
            "Novel\n",
            "100\n",
            "0.6800\n",
            "\n",
            "Lexto [10]\n",
            "\n",
            "0.7267\n",
            "\n",
            "0.7709\n",
            "\n",
            "0.6994\n",
            "\n",
            "0.7701\n",
            "\n",
            "Cutkum\n",
            "wordcutpy [11]\n",
            "\n",
            "0.9322\n",
            "0.6212\n",
            "\n",
            "0.9299\n",
            "0.6286\n",
            "\n",
            "0.8987\n",
            "0.6571\n",
            "\n",
            "0.7140\n",
            "0.6247\n",
            "\n",
            "cunlp [12]\n",
            "\n",
            "0.6910\n",
            "\n",
            "0.6172\n",
            "\n",
            "0.5748\n",
            "\n",
            "0.0000\n",
            "\n",
            "SWATH [13]\n",
            "\n",
            "0.6347\n",
            "\n",
            "0.6858\n",
            "\n",
            "0.6200\n",
            "\n",
            "0.6867\n",
            "\n",
            "3.1\n",
            "\n",
            "Latent Semantic Analysis\n",
            "\n",
            "Latent Semantic Analysis (LSA) [14] is the algorithm, which\n",
            "reduces the dimensionality of term document.The algorithm\n",
            "creates a matrix by using word frequency, applies the singular value\n",
            "decomposition (SVD) [15], and then finds closely related terms and\n",
            "documents.The original matrix A can be separated into three\n",
            "matrices, where U is the m x r (words x extracted concept) matrix,\n",
            "V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\n",
            "r x r diagonal matrix, which can be reconstructed to find the original\n",
            "matrix A.3.3\n",
            "\n",
            "(2)\n",
            "\n",
            "A = ğ‘Šğ‘Šğ‘Šğ‘Š\n",
            "\n",
            "Factors W and H can be found by solving the optimization problem\n",
            "as follows, whereğ‘Šğ‘Šğ‘—ğ‘—ğ‘—ğ‘— â‰¥ 0, ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– â‰¥ 0.\n",
            "ğ‘šğ‘š\n",
            "\n",
            "ğ‘›ğ‘›\n",
            "\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "ğ‘—ğ‘—=1 ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘™ğ‘™=1\n",
            "\n",
            "2\n",
            "\n",
            "ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ğ¹ğ¹(ğ‘Šğ‘Š, ğ»ğ») = || ğ´ğ´ âˆ’ ğ‘Šğ‘Šğ‘Šğ‘Š ||2ğ¹ğ¹ = ï¿½ ï¿½ ï¿½ğ´ğ´ğ‘–ğ‘–ğ‘–ğ‘– âˆ’ ï¿½ ğ‘Šğ‘Šğ‘–ğ‘–ğ‘–ğ‘– ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ï¿½\n",
            "\n",
            "(3)\n",
            "\n",
            "NMF and LSA are both matrix factorization algorithms.In this paper, we applied LSA and NMF on the Thai Travel News\n",
            "dataset for calculating the semantic weights, which represented the\n",
            "relationship between sentences and words in order to select the\n",
            "representative sentences for summarization.(5) to select a number of\n",
            "sentences based on NMF, which got the highest semantic weight\n",
            "values, where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘—ğ‘—.Generic Relevance of jth sentence\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "\n",
            "(1)\n",
            "\n",
            "= ï¿½ ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– )\n",
            "\n",
            "Document summarization using LSA\n",
            "\n",
            "Gong, Y. et al.In our work, after applying SVD to matrix A, ğ‘‰ğ‘‰ ğ‘‡ğ‘‡\n",
            "matrix used for selecting the important sentences.Lee, J., et al.The first matrix ğ‘šğ‘š x ğ‘Ÿğ‘Ÿ is a non-negative semantic\n",
            "feature matrix (NSFM), ğ‘Šğ‘Š .(4)\n",
            "\n",
            "ğ‘–ğ‘–=1\n",
            "\n",
            "ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) =\n",
            "\n",
            "âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘–\n",
            "ğ‘Ÿğ‘Ÿ\n",
            "âˆ‘ğ‘ğ‘=1 âˆ‘ğ‘›ğ‘›ğ‘ğ‘=1 ğ»ğ»ğ‘ğ‘ğ‘ğ‘\n",
            "\n",
            "(5)\n",
            "\n",
            "The ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤â„ğ‘¡ğ‘¡(ğ»ğ»ğ‘–ğ‘– ) is the relative relevance of the ith semantic feature\n",
            "(ğ‘Šğ‘Šğ‘–ğ‘– ), where ğ»ğ»ğ‘–ğ‘–ğ‘–ğ‘– is the weight of the topic ğ‘–ğ‘– in the sentence ğ‘ğ‘ and\n",
            "ğ»ğ»ğ‘ğ‘ğ‘ğ‘ is the weight of the topic ğ‘ğ‘ in the sentence ğ‘ğ‘.(7) represents the cosine\n",
            "\n",
            "\fsimilarity equation, where cos(Î¸) is the dot product between vectors\n",
            "of sentences A and B and divided by the product of the two vectors'\n",
            "lengths.Aâˆ™B\n",
            "||A|| ||B||\n",
            "âˆ‘ni=1 Ai Bi\n",
            "\n",
            "Similarity(A, B) = cos(Î¸) =\n",
            "\n",
            "(7)\n",
            "\n",
            "K-means Clustering\n",
            "\n",
            "15\n",
            "\n",
            "67\n",
            "\n",
            "7\n",
            "7\n",
            "\n",
            "13\n",
            "13\n",
            "\n",
            "55\n",
            "38\n",
            "\n",
            "Table 2 shows the overall number of sentences of news within each\n",
            "dataset.The average numbers of sentences per news of the 5 sets\n",
            "were 21, 16, 15, 13 and 13 sentences, respectively.PIPELINE FOR GENERATING\n",
            "SUMMARIES\n",
            "In this section, we demonstrate our pipeline (Figure 1) used for text\n",
            "summarization to generate a summary for a Thai travel news.Therefore, we collected 400\n",
            "Thai travel news from Thairath and Manager online newspapers to\n",
            "be used as datasets for our experiments.We then evaluated the performance of\n",
            "text summarization methods which were LSA and NMF by\n",
            "comparing their results with the summaries manually curated by\n",
            "two experts from the Faculty of Liberal Arts, Ubon Ratchathani\n",
            "University.Then, the sentences of each news were segmented by human\n",
            "with the following format: Si = â€˜xxxâ€™, where Si represents the order\n",
            "of the sentence in the original document and â€˜xxxâ€™ represents the\n",
            "content of that sentence.This meant 80%, 70% and 60% of the\n",
            "sentences will be selected into the summary.Document summarization pipeline based on LSA\n",
            "and NMF\n",
            "\n",
            "S2\n",
            "\n",
            "For sentence selection by K-means clustering, we grouped similar\n",
            "sentences into the same cluster using the following steps:\n",
            "1.Repeat steps 2-3 until all sentences are assigned to a\n",
            "group, no sentences change the group, or the similarity\n",
            "between sentences and their centroid is close.S1\n",
            "\n",
            "3.6\n",
            "\n",
            "Aâˆ™B\n",
            "=\n",
            "n\n",
            "n\n",
            "||A|| ||B||\n",
            "ï¿½âˆ‘i=1\n",
            "A2i ï¿½âˆ‘i=1\n",
            "Bi2\n",
            "\n",
            "(6)\n",
            "\n",
            "Mr.Yontas\n",
            "ak\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Supason\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "Tourism\n",
            "Authority\n",
            "of Thailand\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "â€¦\n",
            "\n",
            "\fTable 3 demonstrates an example of a matrix ğ´ğ´, constructed from\n",
            "word count by sentence of a Thai travel news.The Generic Sentence Relevance score\n",
            "for NMF also collected one sentence for each concept, the same as\n",
            "Gong, Y. et al.As\n",
            "multiple important sentences could be selected from a more\n",
            "important concept, Murray, G. et al.We evaluated the results of the summarization by using standard\n",
            "accuracy, precision, recall, and F1 score [21].2 Experiment Results\n",
            "\n",
            "In this experimental set, we would like to explore how the different\n",
            "sentence selection methods: the Generic Sentence Relevance score\n",
            "and K-means clustering, affected the text summarization result.The F1 score of SVD with K-means\n",
            "clustering was 0.83, 0.72, and 0.62 for the compression rate of 20%,\n",
            "30%, and 40%.For the NMF with K-means clustering, the F1 score\n",
            "for the three compression rates was 0.83, 0.74 and 0.64.For the Generic Sentence Relevance score, the best F1 score for the\n",
            "compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\n",
            "respectively and the best F1 scores for all compression rates were\n",
            "from the approach of Murray, G. et al.Thai text summarization efficiency of 5 models\n",
            "Figure 2 shows the Thai text summarization efficiency of 5 models:\n",
            "(1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\n",
            "score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\n",
            "sentence score by Murray, G. et al.applied to 400 Thai travel news,\n",
            "divided into 5 sets of 80 news each, with the varied compression\n",
            "rates of 20%, 30% and 40%.From this experiment, the best model based on keyword score for\n",
            "Thai travel news summarization was SVD with sentence selection\n",
            "by Murray, G. et al.The method of Gong, Y. et al., on\n",
            "the other hand was proposed to select only one sentence with the\n",
            "highest score from each concept so that the summary would include\n",
            "\n",
            "CONCLUSIONS\n",
            "\n",
            "In this paper, we applied several text summarization methods to\n",
            "Thai Travel News based on keyword scored in Thai language by\n",
            "extracting the most relevant sentences from the original document.We compared LSA and NMF together with different sentence\n",
            "selection methods, to find the algorithm suitable with this paper's\n",
            "data source.We concluded that keyword scored calculation by LSA\n",
            "with sentence selection by Generic Sentence Relevance score by\n",
            "Murray, G. et al.was the best algorithm while the best compression\n",
            "rate of all models was 20%, for summarizing Thai Travel News\n",
            "compared with humans.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sumy_method(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = summarizer(parser.document, 2)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    final_sentence = ' '.join(summary_sentences)\n",
        "    return final_sentence\n",
        "\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = sumy_method(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/sumy_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lymIrOmSDz1G",
        "outputId": "476a4e90-5e1e-42c7-fc18-ca73768b101a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this paper, we applied LSA and NMF on the Thai Travel News dataset for calculating the semantic weights, which represented the relationship between sentences and words in order to select the representative sentences for summarization. Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models: (1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with sentence score by Murray, G. et al. applied to 400 Thai travel news, divided into 5 sets of 80 news each, with the varied compression rates of 20%, 30% and 40%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def luhn_method(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer_luhn = LuhnSummarizer()\n",
        "    summary = summarizer_luhn(parser.document, 2)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    final_sentence = ' '.join(summary_sentences)\n",
        "    return final_sentence\n",
        "\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = luhn_method(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/luhn_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7lHFYhRN3V3",
        "outputId": "5b917779-8fcc-411c-a643-df2a92434878"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original matrix A can be separated into three matrices, where U is the m x r (words x extracted concept) matrix, V is the n x r (sentences x extracted concepts) matrix, and Î£ is the r x r diagonal matrix, which can be reconstructed to find the original matrix A. Thai text summarization efficiency of 5 models Figure 2 shows the Thai text summarization efficiency of 5 models: (1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with sentence score by Murray, G. et al. applied to 400 Thai travel news, divided into 5 sets of 80 news each, with the varied compression rates of 20%, 30% and 40%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lsa_method(text):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer_lsa = LsaSummarizer()\n",
        "    summary = summarizer_lsa(parser.document, 2)\n",
        "    summary_sentences = [str(sentence) for sentence in summary]\n",
        "    final_sentence = ' '.join(summary_sentences)\n",
        "    return final_sentence\n",
        "\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    result = lsa_method(text)\n",
        "    print(result)\n",
        "\n",
        "with open('/content/lsa_method_result.txt', 'w') as file:\n",
        "    file.write(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3vE_BMWN6ck",
        "outputId": "00d485c8-b84b-4e98-e7a2-c45b7f42aff8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Therefore, we collected 400 Thai travel news from Thairath and Manager online newspapers to be used as datasets for our experiments. From this experiment, the best model based on keyword score for Thai travel news summarization was SVD with sentence selection by Murray, G. et al.\n"
          ]
        }
      ]
    }
  ]
}