{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Preprocessing Data\n",
        "1) Load Data\n",
        "2) Clean Data:-\n",
        "   1) Remove special characters\n",
        "   2) Remove unwanted spaces\n",
        "3) Write data back to a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'project compilation\\\\paperThailand.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Read the text file\u001b[39;00m\n\u001b[0;32m     17\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mproject compilation\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpaperThailand.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     19\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     21\u001b[0m \u001b[39m# Clean the text\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'project compilation\\\\paperThailand.txt'"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize text by converting to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove excess whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Read the text file\n",
        "file_path = 'project compilation\\paperThailand.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Clean the text\n",
        "cleaned_text = clean_text(text)\n",
        "\n",
        "# Print the cleaned text\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD DATA\n",
        "with open('paperThailand.txt', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "f.close()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEAN DATA\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize text by converting to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove excess whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Clean the text\n",
        "cleaned_text = clean_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write Data to Text File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Preprocessed Text to a File\n",
        "textFileOutput = 'CleanedText.txt'\n",
        "\n",
        "# Save Preprocessed Text\n",
        "with open(textFileOutput, 'w', encoding='utf8') as file:\n",
        "    for sentence in cleaned_text:\n",
        "        file.write(sentence)\n",
        "        file.write('\\n')\n",
        "\n",
        "print('ClenedText File saved to: ', textFileOutput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB2JTFhwuv3r",
        "outputId": "59be1d2f-e6e2-48e4-8da0-1487e94aec6e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import tokenize, stem, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = tokenize.sent_tokenize(text)\n",
        "print('Total sentences in the given text:', len(sentences))\n",
        "print(sentences)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = tokenize.word_tokenize(text)\n",
        "print('Total words in the given text:', len(words))\n",
        "print(words)\n",
        "\n",
        "# Stemming using PorterStemmer\n",
        "stemmer = stem.PorterStemmer()\n",
        "stem_words = [stemmer.stem(word) for word in words]\n",
        "print('After stemming:', stem_words)\n",
        "\n",
        "# Lemmatization using WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print('After lemmatization:', lemma_words)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print('After removing stopwords:', filtered_words)\n",
        "\n",
        "# Part-of-speech tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print('POS tags:', pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOwDS8D1vG_Q",
        "outputId": "254cb335-f60d-487d-cb5f-119ce8289c50"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import tokenize, stem, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import heapq\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Preprocess the text\n",
        "sentences = tokenize.sent_tokenize(text)\n",
        "words = tokenize.word_tokenize(text)\n",
        "stemmer = stem.PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_word(word):\n",
        "    word = word.lower()\n",
        "    if word not in stop_words:\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "        word = stemmer.stem(word)\n",
        "        return word\n",
        "    return None\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    processed_sentence = []\n",
        "    for word in tokenize.word_tokenize(sentence):\n",
        "        processed_word = preprocess_word(word)\n",
        "        if processed_word:\n",
        "            processed_sentence.append(processed_word)\n",
        "    return processed_sentence\n",
        "\n",
        "preprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "# Text summarization using TF-IDF\n",
        "word_frequencies = {}\n",
        "for sentence in preprocessed_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "maximum_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "sentence_scores = {}\n",
        "for i, sentence in enumerate(preprocessed_sentences):\n",
        "    score = 0\n",
        "    for word in sentence:\n",
        "        if word in word_frequencies.keys():\n",
        "            score += word_frequencies[word]\n",
        "    sentence_scores[i] = score\n",
        "\n",
        "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "summary = [sentences[i] for i in summary_sentences]\n",
        "\n",
        "# Print the summary\n",
        "print(\"Summary:\")\n",
        "for sentence in summary:\n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNsbiu207rkC",
        "outputId": "20962780-6ac0-444d-b070-5aff0243be35"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import tokenize, stem, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import heapq\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Preprocess the text\n",
        "sentences = tokenize.sent_tokenize(text)\n",
        "words = tokenize.word_tokenize(text)\n",
        "stemmer = stem.PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_word(word):\n",
        "    word = word.lower()\n",
        "    if word not in stop_words:\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "        word = stemmer.stem(word)\n",
        "        return word\n",
        "    return None\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    processed_sentence = []\n",
        "    for word in tokenize.word_tokenize(sentence):\n",
        "        processed_word = preprocess_word(word)\n",
        "        if processed_word:\n",
        "            processed_sentence.append(processed_word)\n",
        "    return processed_sentence\n",
        "\n",
        "preprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "# Text summarization using TF-IDF\n",
        "word_frequencies = {}\n",
        "for sentence in preprocessed_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "maximum_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "sentence_scores = {}\n",
        "for i, sentence in enumerate(preprocessed_sentences):\n",
        "    score = 0\n",
        "    for word in sentence:\n",
        "        if word in word_frequencies.keys():\n",
        "            score += word_frequencies[word]\n",
        "    sentence_scores[i] = score\n",
        "\n",
        "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "summary = [sentences[i] for i in summary_sentences]\n",
        "\n",
        "# Save preprocessed text and summary to a new file\n",
        "preprocessed_filename = 'preprocessed_text.txt'\n",
        "summary_filename = 'summary.txt'\n",
        "\n",
        "# Save preprocessed text\n",
        "with open(preprocessed_filename, 'w') as file:\n",
        "    for sentence in preprocessed_sentences:\n",
        "        file.write(' '.join(sentence))\n",
        "        file.write('\\n')\n",
        "\n",
        "# Save summary\n",
        "with open(summary_filename, 'w') as file:\n",
        "    for sentence in summary:\n",
        "        file.write(sentence)\n",
        "        file.write('\\n')\n",
        "\n",
        "print(\"Preprocessed text saved to:\", preprocessed_filename)\n",
        "print(\"Summary saved to:\", summary_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
