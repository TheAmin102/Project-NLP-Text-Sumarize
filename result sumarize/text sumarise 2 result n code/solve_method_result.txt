Extractive Text Summarization for Thai Travel News
Based on Keyword Scored in Thai Language
Sarunya Nathonghor

Duangdao Wichadakul

Department of Computer Engineering
Chulalongkorn University
Bangkok, Thailand

Department of Computer Engineering
Chulalongkorn University
Bangkok, Thailand

Sarunya.N@Student.Chula.ac.th
ABSTRACT

In recent years, people are seeking for a solution to improve text
summarization for Thai language.Although several solutions such
as PageRank, Graph Rank, Latent Semantic Analysis (LSA)
models, etc., have been proposed, research results in Thai text
summarization were restricted due to limited corpus in Thai
language with complex grammar.This paper applied a text
summarization system for Thai travel news based on keyword
scored in Thai language by extracting the most relevant sentences
from the original document.From these experiments, we concluded that keyword
scored calculation by LSA with sentence selection by GRS is the
best algorithm for summarizing Thai Travel News, compared with
human with the best compression rate of 20%.It consists of three steps: 1) create an
intermediate representation of the input text, 2) calculate score for
the sentences based on the concepts, and 3) choose important
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page.The first approach is the extractive
summarization, which relies on a method for extracting words and
searching for keywords from the original document.This research focused on the sentence extraction function based on
keyword score calculation then selecting important sentences based
on the Generic Sentence Relevance score (GRS), calculated from
Latent Semantic Analysis (LSA) and Non-negative Matrix
Factorization (NMF).In this experiment, we compared 5
models for 5 rounds with Thai travel news using the compression
rates of 20%, 30% and 40% and reported the rate and method that
produced the best result from the experiment.[2] proposed a Thai text
summarization that extracted a paragraph from a document based
on Thai compound nouns, term frequency method, and headline
score for generating a summary.[4] proposed a method to summarize
documents by extracting important sentences from combining the
specific properties (Local Property) and the overall properties
(Global Property) of the sentences.From
their experiments, the summarization of the industrial news got
60% precision, 44% recall, and 50.9% F-measure, the general news
got the 51.8% precision, 38.5% recall, and 43.1% F-measure while
the fashion magazines got 53.0% precision, 33.0% recall, and
40.4% F-measure.The researcher used A* search algorithm to
find the shortest sequence of sentences from keyword group by
topic calculation, sentence segmentation and word grouping.Their summarization of the agricultural news
got 68.57% precision, 51.95% recall and 56.72% F-measure.Lee, J., et al.[6] proposed a document summarization method using
Non-negative Matrix Factorization (NMF).Comparison of Thai word tokenization programs
Tools

F1 Score

Validate
PyICU [9]

Article
100
0.6155

Encyclopedia
100
0.6932

News
100
0.5987

Novel
100
0.6800

Lexto [10]

0.7267

0.7709

0.6994

0.7701

Cutkum
wordcutpy [11]

0.9322
0.6212

0.9299
0.6286

0.8987
0.6571

0.7140
0.6247

cunlp [12]

0.6910

0.6172

0.5748

0.0000

SWATH [13]

0.6347

0.6858

0.6200

0.6867

3.1

Latent Semantic Analysis

Latent Semantic Analysis (LSA) [14] is the algorithm, which
reduces the dimensionality of term document.The algorithm
creates a matrix by using word frequency, applies the singular value
decomposition (SVD) [15], and then finds closely related terms and
documents.The original matrix A can be separated into three
matrices, where U is the m x r (words x extracted concept) matrix,
V is the n x r (sentences x extracted concepts) matrix, and Σ is the
r x r diagonal matrix, which can be reconstructed to find the original
matrix A.3.3

(2)

A = 𝑊𝑊𝑊𝑊

Factors W and H can be found by solving the optimization problem
as follows, where𝑊𝑊𝑗𝑗𝑗𝑗 ≥ 0, 𝐻𝐻𝑖𝑖𝑖𝑖 ≥ 0.
𝑚𝑚

𝑛𝑛

𝑟𝑟

𝑗𝑗=1 𝑖𝑖=1

𝑙𝑙=1

2

𝑚𝑚𝑚𝑚𝑚𝑚 𝐹𝐹(𝑊𝑊, 𝐻𝐻) = || 𝐴𝐴 − 𝑊𝑊𝑊𝑊 ||2𝐹𝐹 = � � �𝐴𝐴𝑖𝑖𝑖𝑖 − � 𝑊𝑊𝑖𝑖𝑖𝑖 𝐻𝐻𝑖𝑖𝑖𝑖 �

(3)

NMF and LSA are both matrix factorization algorithms.In this paper, we applied LSA and NMF on the Thai Travel News
dataset for calculating the semantic weights, which represented the
relationship between sentences and words in order to select the
representative sentences for summarization.(5) to select a number of
sentences based on NMF, which got the highest semantic weight
values, where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑗𝑗.Generic Relevance of jth sentence
𝑟𝑟

(1)

= � 𝐻𝐻𝑖𝑖𝑖𝑖 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡(𝐻𝐻𝑖𝑖 )

Document summarization using LSA

Gong, Y. et al.In our work, after applying SVD to matrix A, 𝑉𝑉 𝑇𝑇
matrix used for selecting the important sentences.Lee, J., et al.The first matrix 𝑚𝑚 x 𝑟𝑟 is a non-negative semantic
feature matrix (NSFM), 𝑊𝑊 .(4)

𝑖𝑖=1

𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡(𝐻𝐻𝑖𝑖 ) =

∑𝑛𝑛𝑞𝑞=1 𝐻𝐻𝑖𝑖𝑖𝑖
𝑟𝑟
∑𝑝𝑝=1 ∑𝑛𝑛𝑞𝑞=1 𝐻𝐻𝑝𝑝𝑝𝑝

(5)

The 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤ℎ𝑡𝑡(𝐻𝐻𝑖𝑖 ) is the relative relevance of the ith semantic feature
(𝑊𝑊𝑖𝑖 ), where 𝐻𝐻𝑖𝑖𝑖𝑖 is the weight of the topic 𝑖𝑖 in the sentence 𝑞𝑞 and
𝐻𝐻𝑝𝑝𝑝𝑝 is the weight of the topic 𝑝𝑝 in the sentence 𝑞𝑞.(7) represents the cosine

similarity equation, where cos(θ) is the dot product between vectors
of sentences A and B and divided by the product of the two vectors'
lengths.A∙B
||A|| ||B||
∑ni=1 Ai Bi

Similarity(A, B) = cos(θ) =

(7)

K-means Clustering

15

67

7
7

13
13

55
38

Table 2 shows the overall number of sentences of news within each
dataset.The average numbers of sentences per news of the 5 sets
were 21, 16, 15, 13 and 13 sentences, respectively.PIPELINE FOR GENERATING
SUMMARIES
In this section, we demonstrate our pipeline (Figure 1) used for text
summarization to generate a summary for a Thai travel news.Therefore, we collected 400
Thai travel news from Thairath and Manager online newspapers to
be used as datasets for our experiments.We then evaluated the performance of
text summarization methods which were LSA and NMF by
comparing their results with the summaries manually curated by
two experts from the Faculty of Liberal Arts, Ubon Ratchathani
University.Then, the sentences of each news were segmented by human
with the following format: Si = ‘xxx’, where Si represents the order
of the sentence in the original document and ‘xxx’ represents the
content of that sentence.This meant 80%, 70% and 60% of the
sentences will be selected into the summary.Document summarization pipeline based on LSA
and NMF

S2

For sentence selection by K-means clustering, we grouped similar
sentences into the same cluster using the following steps:
1.Repeat steps 2-3 until all sentences are assigned to a
group, no sentences change the group, or the similarity
between sentences and their centroid is close.S1

3.6

A∙B
=
n
n
||A|| ||B||
�∑i=1
A2i �∑i=1
Bi2

(6)

Mr.Yontas
ak

1

0

0

0

0

0

0

0

0

Supason

1

0

0

0

0

0

0

0

0

Tourism
Authority
of Thailand

1

0

0

0

0

0

0

0

0

…

…

…

…

…

…

…

…

…

…

Table 3 demonstrates an example of a matrix 𝐴𝐴, constructed from
word count by sentence of a Thai travel news.The Generic Sentence Relevance score
for NMF also collected one sentence for each concept, the same as
Gong, Y. et al.As
multiple important sentences could be selected from a more
important concept, Murray, G. et al.We evaluated the results of the summarization by using standard
accuracy, precision, recall, and F1 score [21].2 Experiment Results

In this experimental set, we would like to explore how the different
sentence selection methods: the Generic Sentence Relevance score
and K-means clustering, affected the text summarization result.The F1 score of SVD with K-means
clustering was 0.83, 0.72, and 0.62 for the compression rate of 20%,
30%, and 40%.For the NMF with K-means clustering, the F1 score
for the three compression rates was 0.83, 0.74 and 0.64.For the Generic Sentence Relevance score, the best F1 score for the
compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68
respectively and the best F1 scores for all compression rates were
from the approach of Murray, G. et al.Thai text summarization efficiency of 5 models
Figure 2 shows the Thai text summarization efficiency of 5 models:
(1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence
score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with
sentence score by Murray, G. et al.applied to 400 Thai travel news,
divided into 5 sets of 80 news each, with the varied compression
rates of 20%, 30% and 40%.From this experiment, the best model based on keyword score for
Thai travel news summarization was SVD with sentence selection
by Murray, G. et al.The method of Gong, Y. et al., on
the other hand was proposed to select only one sentence with the
highest score from each concept so that the summary would include

CONCLUSIONS

In this paper, we applied several text summarization methods to
Thai Travel News based on keyword scored in Thai language by
extracting the most relevant sentences from the original document.We compared LSA and NMF together with different sentence
selection methods, to find the algorithm suitable with this paper's
data source.We concluded that keyword scored calculation by LSA
with sentence selection by Generic Sentence Relevance score by
Murray, G. et al.was the best algorithm while the best compression
rate of all models was 20%, for summarizing Thai Travel News
compared with humans.