{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFdYhuWzB8Me",
        "outputId": "26956d0e-5b4f-4f3f-9c9c-d3492be3f391"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _create_frequency_table(text_string) -> dict:\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n",
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = len(word_tokenize(sentence))\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from the original text\n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > threshold:\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Read text from TXT file\n",
        "with open('/content/paperThailand.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# 1 Create the word frequency table\n",
        "freq_table = _create_frequency_table(text)\n",
        "\n",
        "# 2 Tokenize the sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# 3 Important Algorithm: score the sentences\n",
        "sentence_scores = _score_sentences(sentences, freq_table)\n",
        "\n",
        "# 4 Find the threshold\n",
        "threshold = _find_average_score(sentence_scores)\n",
        "\n",
        "# 5 Important Algorithm: Generate the summary\n",
        "summary = _generate_summary(sentences, sentence_scores, 1.5 * threshold)\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URnUikKM_m58",
        "outputId": "d3dee08f-469d-42a3-c9e1-bcd85a5c936d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2. Suwanno, N. et al. Chongsuntornsri, A., et al. Jaruskulchai C., et al. Mani, I., et al. Lee, J., et al. 3. Table 1. (1). proposed Eq. (4) and Eq. Murray, G. et al. Lee, J., et al. (2) and Eq. (3). Eq. (6) and Eq. 5. and\n",
            "Murray, G. et al. S3\n",
            "\n",
            "4. Figure 1. 2. 3. 4. 5. (2) for NMF. sentences from all concepts. (5). outperformed both Gong, Y. et\n",
            "al. 6. 6. Figure 2. 8.\n"
          ]
        }
      ]
    }
  ]
}