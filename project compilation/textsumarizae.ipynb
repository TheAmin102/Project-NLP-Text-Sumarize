{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C46oF36Bg0V",
        "outputId": "2d6daa44-62ee-40b2-b87a-d2399a1b74d0"
      },
      "outputs": [],
      "source": [
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install tensorflow\n",
        "# !pip install transformers\n",
        "# !pip install nltk bs4\n",
        "# !pip install lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Preprocessing Data\n",
        "1) Load Data\n",
        "2) Clean Data:-\n",
        "   1) Remove special characters\n",
        "   2) Remove unwanted spaces\n",
        "3) Write data back to a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wuhz3v4zwIcn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Extractive Text Summarization for Thai Travel News\\n', 'Based on Keyword Scored in Thai Language\\n', 'Sarunya Nathonghor\\n', '\\n', 'Duangdao Wichadakul\\n', '\\n', 'Department of Computer Engineering\\n', 'Chulalongkorn University\\n', 'Bangkok, Thailand\\n', '\\n', 'Department of Computer Engineering\\n', 'Chulalongkorn University\\n', 'Bangkok, Thailand\\n', '\\n', 'Sarunya.N@Student.Chula.ac.th\\n', 'ABSTRACT\\n', '\\n', 'In recent years, people are seeking for a solution to improve text\\n', 'summarization for Thai language. Although several solutions such\\n', 'as PageRank, Graph Rank, Latent Semantic Analysis (LSA)\\n', 'models, etc., have been proposed, research results in Thai text\\n', 'summarization were restricted due to limited corpus in Thai\\n', 'language with complex grammar. This paper applied a text\\n', 'summarization system for Thai travel news based on keyword\\n', 'scored in Thai language by extracting the most relevant sentences\\n', 'from the original document. We compared LSA and Non-negative\\n', 'Matrix Factorization (NMF) to find the algorithm that is suitable\\n', 'with Thai travel news. The suitable compression rates for Generic\\n', 'Sentence Relevance score (GRS) and K-means clustering were also\\n', 'evaluated. From these experiments, we concluded that keyword\\n', 'scored calculation by LSA with sentence selection by GRS is the\\n', 'best algorithm for summarizing Thai Travel News, compared with\\n', 'human with the best compression rate of 20%.\\n', '\\n', 'CCS Concepts\\n', '\\n', 'â€¢ Information systems âž Information retrieval âž Retrieval\\n', 'tasks and goalsâž Summarization\\n', '\\n', 'Keywords\\n', '\\n', 'Text summarization; extractive summarization; non-negative\\n', 'matrix factorization\\n', '\\n', '1.\\n', '\\n', 'INTRODUCTION\\n', '\\n', 'Daily newspaper has abundant of data that users do not have\\n', 'enough time for reading them. It is difficult to identify the relevant\\n', 'information to satisfy the information needed by users. Automatic\\n', 'summarization can reduce the problem of information overloading\\n', 'and it has been proposed previously in English and other languages.\\n', 'However, there were only a few research results in Thai text\\n', 'summarization due to the lack of corpus in Thai language and the\\n', 'complicated grammar.\\n', 'Text Summarization [1] is a technique for summarizing the content\\n', 'of the documents. It consists of three steps: 1) create an\\n', 'intermediate representation of the input text, 2) calculate score for\\n', 'the sentences based on the concepts, and 3) choose important\\n', 'Permission to make digital or hard copies of all or part of this work for\\n', 'personal or classroom use is granted without fee provided that copies are\\n', 'not made or distributed for profit or commercial advantage and that copies\\n', 'bear this notice and the full citation on the first page. Copyrights for\\n', 'components of this work owned by others than ACM must be honored.\\n', 'Abstracting with credit is permitted. To copy otherwise, or republish, to\\n', 'post on servers or to redistribute to lists, requires prior specific permission\\n', 'and/or a fee. Request permissions from Permissions@acm.org.\\n', 'ITCC 2020, August 12â€“14, 2020, Kuala Lumpur, Malaysia\\n', 'Â© 2020 Association for Computing Machinery.\\n', 'ACM ISBN 978-1-4503-7539-9/20/08â€¦$15.00\\n', '\\n', 'DOI: https://doi.org/10.1145/3417473.3417479\\n', '\\n', 'Duangdao.W@Chula.ac.th\\n', '\\n', 'sentences to be included in the summary. Text summarization can\\n', 'be divided into 2 approaches. The first approach is the extractive\\n', 'summarization, which relies on a method for extracting words and\\n', 'searching for keywords from the original document. The second\\n', 'approach is the abstractive summarization, which analyzes words\\n', 'by linguistic principles with transcription or interpretation from the\\n', 'original document. This approach implies more effective and\\n', 'accurate summary than the extractive methods. However, with the\\n', 'lack of Thai corpus, we chose to apply an extractive summarization\\n', 'method for Thai text summarization.\\n', 'This research focused on the sentence extraction function based on\\n', 'keyword score calculation then selecting important sentences based\\n', 'on the Generic Sentence Relevance score (GRS), calculated from\\n', 'Latent Semantic Analysis (LSA) and Non-negative Matrix\\n', 'Factorization (NMF). We also tried using K-means clustering for\\n', 'document summarization. In this experiment, we compared 5\\n', 'models for 5 rounds with Thai travel news using the compression\\n', 'rates of 20%, 30% and 40% and reported the rate and method that\\n', 'produced the best result from the experiment.\\n', '\\n', '2.\\n', '\\n', 'RELATED WORKS\\n', '\\n', 'In recent years, several models in Thai Text summarization have\\n', 'been introduced. Suwanno, N. et al. [2] proposed a Thai text\\n', 'summarization that extracted a paragraph from a document based\\n', 'on Thai compound nouns, term frequency method, and headline\\n', 'score for generating a summary. Chongsuntornsri, A., et al. [3]\\n', 'proposed a new approach for Text summarization in Thai based on\\n', 'content- and graph-based with the use of Topic Sensitive PageRank\\n', 'algorithm for summarizing and ranking of text segments.\\n', 'Jaruskulchai C., et al. [4] proposed a method to summarize\\n', 'documents by extracting important sentences from combining the\\n', 'specific properties (Local Property) and the overall properties\\n', '(Global Property) of the sentences. The overall properties were\\n', 'based on the relationship between sentences in the document. From\\n', 'their experiments, the summarization of the industrial news got\\n', '60% precision, 44% recall, and 50.9% F-measure, the general news\\n', 'got the 51.8% precision, 38.5% recall, and 43.1% F-measure while\\n', 'the fashion magazines got 53.0% precision, 33.0% recall, and\\n', '40.4% F-measure.\\n', 'Mani, I., et al. [5] proposed techniques of text summarization by\\n', 'using word frequency in the document and calculated the weight of\\n', 'word to create a keyword group. They then calculated the cosine\\n', 'similarity of sentences. The researcher used A* search algorithm to\\n', 'find the shortest sequence of sentences from keyword group by\\n', 'topic calculation, sentence segmentation and word grouping. The\\n', 'sequence of sentences that were in the main group were selected as\\n', 'important sentences. Their summarization of the agricultural news\\n', 'got 68.57% precision, 51.95% recall and 56.72% F-measure.\\n', 'Lee, J., et al. [6] proposed a document summarization method using\\n', 'Non-negative Matrix Factorization (NMF). They compared\\n', '\\n', '\\x0cbetween Latent Semantic Analysis (LSA) and NMF to find the\\n', 'weight of each word and calculated the summation of weights. The\\n', 'important sentences were ranked and selected into the summary\\n', 'based on their summed weight. Based on LSA, they found many\\n', 'weights with zero and negative values. However, when applied\\n', 'NMF, they found only the positive values and the scope of the\\n', 'semantic featuresâ€™ meaning was narrow. Therefore, they proposed\\n', 'that NMF provided a greater possibility for extracting important\\n', 'sentences.\\n', '\\n', '3.\\n', '\\n', 'PREPROCESSING FOR THAI TEXT\\n', '\\n', 'The first step for working with Thai Text is word tokenization. Even\\n', 'though Thai writing system has no delimiters to indicate word\\n', 'boundaries together with many rules for word segmentation, several\\n', 'Thai word tokenization programs have been proposed. Table 1\\n', 'shows F1 score of the recent programs trained and tested by one of\\n', 'our laboratory members with the data from BEST2010 corpus [7].\\n', 'Cutkum [8] got the highest F1 score, hence, we used Cutkum for this\\n', 'step.\\n', 'Table 1. Comparison of Thai word tokenization programs\\n', 'Tools\\n', '\\n', 'F1 Score\\n', '\\n', 'Validate\\n', 'PyICU [9]\\n', '\\n', 'Article\\n', '100\\n', '0.6155\\n', '\\n', 'Encyclopedia\\n', '100\\n', '0.6932\\n', '\\n', 'News\\n', '100\\n', '0.5987\\n', '\\n', 'Novel\\n', '100\\n', '0.6800\\n', '\\n', 'Lexto [10]\\n', '\\n', '0.7267\\n', '\\n', '0.7709\\n', '\\n', '0.6994\\n', '\\n', '0.7701\\n', '\\n', 'Cutkum\\n', 'wordcutpy [11]\\n', '\\n', '0.9322\\n', '0.6212\\n', '\\n', '0.9299\\n', '0.6286\\n', '\\n', '0.8987\\n', '0.6571\\n', '\\n', '0.7140\\n', '0.6247\\n', '\\n', 'cunlp [12]\\n', '\\n', '0.6910\\n', '\\n', '0.6172\\n', '\\n', '0.5748\\n', '\\n', '0.0000\\n', '\\n', 'SWATH [13]\\n', '\\n', '0.6347\\n', '\\n', '0.6858\\n', '\\n', '0.6200\\n', '\\n', '0.6867\\n', '\\n', '3.1\\n', '\\n', 'Latent Semantic Analysis\\n', '\\n', 'Latent Semantic Analysis (LSA) [14] is the algorithm, which\\n', 'reduces the dimensionality of term document. The algorithm\\n', 'creates a matrix by using word frequency, applies the singular value\\n', 'decomposition (SVD) [15], and then finds closely related terms and\\n', 'documents. The original matrix A can be separated into three\\n', 'matrices, where U is the m x r (words x extracted concept) matrix,\\n', 'V is the n x r (sentences x extracted concepts) matrix, and Î£ is the\\n', 'r x r diagonal matrix, which can be reconstructed to find the original\\n', 'matrix A. The SVD can be represented in Eq. (1).\\n', '\\n', '3.2\\n', '\\n', 'A â‰ˆ ð‘ˆð‘ˆð‘ˆð‘ˆð‘‰ð‘‰ ð‘‡ð‘‡\\n', '\\n', 'of the related singular value over the sum of all singular values, for\\n', 'each concept.\\n', '\\n', '3.3\\n', '\\n', '(2)\\n', '\\n', 'A = ð‘Šð‘Šð‘Šð‘Š\\n', '\\n', 'Factors W and H can be found by solving the optimization problem\\n', 'as follows, whereð‘Šð‘Šð‘—ð‘—ð‘—ð‘— â‰¥ 0, ð»ð»ð‘–ð‘–ð‘–ð‘– â‰¥ 0.\\n', 'ð‘šð‘š\\n', '\\n', 'ð‘›ð‘›\\n', '\\n', 'ð‘Ÿð‘Ÿ\\n', '\\n', 'ð‘—ð‘—=1 ð‘–ð‘–=1\\n', '\\n', 'ð‘™ð‘™=1\\n', '\\n', '2\\n', '\\n', 'ð‘šð‘šð‘šð‘šð‘šð‘š ð¹ð¹(ð‘Šð‘Š, ð»ð») = || ð´ð´ âˆ’ ð‘Šð‘Šð‘Šð‘Š ||2ð¹ð¹ = ï¿½ ï¿½ ï¿½ð´ð´ð‘–ð‘–ð‘–ð‘– âˆ’ ï¿½ ð‘Šð‘Šð‘–ð‘–ð‘–ð‘– ð»ð»ð‘–ð‘–ð‘–ð‘– ï¿½\\n', '\\n', '(3)\\n', '\\n', 'NMF and LSA are both matrix factorization algorithms. However,\\n', 'when using NMF to find keywords, NMF will return the keywords\\n', 'that are closely related because its components have only nonnegative values. As LSA has both positive and negative values as\\n', 'well as some zeroes, it gets a wider distribution. The semantic\\n', 'feature represents a concept of meaning for root of words that have\\n', 'a relationship. For example, man, human, male and adult have the\\n', 'same semantic, hence their semantic values are close.\\n', 'In this paper, we applied LSA and NMF on the Thai Travel News\\n', 'dataset for calculating the semantic weights, which represented the\\n', 'relationship between sentences and words in order to select the\\n', 'representative sentences for summarization.\\n', '\\n', '3.4 Generic document summarization by\\n', 'NMF\\n', '\\n', 'Lee, J., et al. proposed Eq. (4) and Eq. (5) to select a number of\\n', 'sentences based on NMF, which got the highest semantic weight\\n', 'values, where ð»ð»ð‘–ð‘–ð‘–ð‘– is the weight of the topic ð‘–ð‘– in the sentence ð‘—ð‘—.\\n', 'Generic Relevance of jth sentence\\n', 'ð‘Ÿð‘Ÿ\\n', '\\n', '(1)\\n', '\\n', '= ï¿½ ð»ð»ð‘–ð‘–ð‘–ð‘– ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤â„Žð‘¡ð‘¡(ð»ð»ð‘–ð‘– )\\n', '\\n', 'Document summarization using LSA\\n', '\\n', 'Gong, Y. et al. [16] proposed a document summarization based on\\n', 'SVD matrices. In our work, after applying SVD to matrix A, ð‘‰ð‘‰ ð‘‡ð‘‡\\n', 'matrix used for selecting the important sentences. The cell value of\\n', 'the matrix shows the relationship between sentence and extracted\\n', 'concepts. A sentence with the highest cell value of each concept\\n', 'will be selected into the summary starting from the most important\\n', 'concept. The total number of sentences in the summary will be\\n', 'equal to the number all detected concepts.\\n', 'Murray, G. et al. [17] proposed a document summarization based\\n', 'on SVD matrices using ð‘‰ð‘‰ ð‘‡ð‘‡ and Î£ matrices for sentence selection.\\n', 'The authors proposed that more than one sentence could be\\n', 'collected from the more important concepts. The decision of how\\n', 'many sentences would be collected from each concept depending\\n', 'on the Î£ matrix. The value was decided by getting the percentage\\n', '\\n', 'Non-negative Matrix Factorization\\n', '\\n', 'Non-negative Matrix Factorization (NMF) is a method of matrix\\n', 'factorization subject to the non-negative constraint. Lee, J., et al.\\n', 'proposed the model based on NMF for document summarization.\\n', 'NMF decomposes a non-negative matrix ð´ð´ âˆˆ ð‘…ð‘…ð‘šð‘šð‘šð‘šð‘šð‘š into two nonnegative matrices. The first matrix ð‘šð‘š x ð‘Ÿð‘Ÿ is a non-negative semantic\\n', 'feature matrix (NSFM), ð‘Šð‘Š . The second matrix ð‘Ÿð‘Ÿ x ð‘›ð‘› is a nonnegative semantic variable matrix (NSVM), ð»ð». So, we have ð‘Šð‘Š âˆˆ\\n', 'ð‘…ð‘…ð‘šð‘šð‘šð‘šð‘šð‘š and ð»ð» âˆˆ ð‘…ð‘…ð‘Ÿð‘Ÿð‘Ÿð‘Ÿð‘Ÿð‘Ÿ and both terms are non-negative as shown in\\n', 'Eq. (2) and Eq. (3).\\n', '\\n', '(4)\\n', '\\n', 'ð‘–ð‘–=1\\n', '\\n', 'ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤â„Žð‘¡ð‘¡(ð»ð»ð‘–ð‘– ) =\\n', '\\n', 'âˆ‘ð‘›ð‘›ð‘žð‘ž=1 ð»ð»ð‘–ð‘–ð‘–ð‘–\\n', 'ð‘Ÿð‘Ÿ\\n', 'âˆ‘ð‘ð‘=1 âˆ‘ð‘›ð‘›ð‘žð‘ž=1 ð»ð»ð‘ð‘ð‘ð‘\\n', '\\n', '(5)\\n', '\\n', 'The ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤ð‘¤â„Žð‘¡ð‘¡(ð»ð»ð‘–ð‘– ) is the relative relevance of the ith semantic feature\\n', '(ð‘Šð‘Šð‘–ð‘– ), where ð»ð»ð‘–ð‘–ð‘–ð‘– is the weight of the topic ð‘–ð‘– in the sentence ð‘žð‘ž and\\n', 'ð»ð»ð‘ð‘ð‘ð‘ is the weight of the topic ð‘ð‘ in the sentence ð‘žð‘ž. The sentences\\n', 'can be ranked by Generic Relevance Sentence scores. Sentences\\n', 'with the maximum score will be selected into the summary.\\n', '\\n', '3.5\\n', '\\n', 'Cosine Similarity\\n', '\\n', 'Cosine similarity [18] is a widely used method to measure the\\n', 'similarity between vectors representing the documents. The result of\\n', 'cosine similarity is ranging from 0 to 1. If it is closer to 1, that means\\n', 'both vectors are similar. Eq. (6) and Eq. (7) represents the cosine\\n', '\\n', '\\x0csimilarity equation, where cos(Î¸) is the dot product between vectors\\n', \"of sentences A and B and divided by the product of the two vectors'\\n\", 'lengths.\\n', 'In this paper, we deployed cosine similarity to measure the similarity\\n', 'of sentences in K-means clustering.\\n', '\\n', 'Aâˆ™B\\n', '||A|| ||B||\\n', 'âˆ‘ni=1 Ai Bi\\n', '\\n', 'Similarity(A, B) = cos(Î¸) =\\n', '\\n', '(7)\\n', '\\n', 'K-means Clustering\\n', '\\n', '15\\n', '\\n', '67\\n', '\\n', '7\\n', '7\\n', '\\n', '13\\n', '13\\n', '\\n', '55\\n', '38\\n', '\\n', 'Table 2 shows the overall number of sentences of news within each\\n', 'dataset. The average numbers of sentences per news of the 5 sets\\n', 'were 21, 16, 15, 13 and 13 sentences, respectively.\\n', '\\n', '5.\\n', 'PIPELINE FOR GENERATING\\n', 'SUMMARIES\\n', 'In this section, we demonstrate our pipeline (Figure 1) used for text\\n', 'summarization to generate a summary for a Thai travel news.\\n', '\\n', 'Word\\n', '\\n', 'S9\\n', '\\n', '6\\n', '\\n', 'Round 4\\n', 'Round 5\\n', '\\n', 'Table 3. Example of Word by Sentence Matrix A\\n', 'S8\\n', '\\n', 'Round 3\\n', '\\n', 'Avg. Number\\n', 'of Sentences\\n', '\\n', 'S7\\n', '\\n', '21\\n', '16\\n', '\\n', 'Round 1\\n', 'Round 2\\n', '\\n', 'Min.\\n', 'Number of\\n', 'Sentences\\n', '\\n', 'S6\\n', '\\n', '7\\n', '7\\n', '\\n', 'Max.\\n', 'Number of\\n', 'Sentences\\n', '58\\n', '58\\n', '\\n', 'Dataset\\n', '\\n', 'S5\\n', '\\n', 'Table 2. Overall Sentence Language of each Dataset\\n', '\\n', 'S4\\n', '\\n', 'DATA PREPARATION\\n', '\\n', 'The standard data sets in Thai language are unavailable for\\n', 'evaluating text summarization system. Therefore, we collected 400\\n', 'Thai travel news from Thairath and Manager online newspapers to\\n', 'be used as datasets for our experiments. We split 400 travel news\\n', 'into 5 sets of 80 news each. We then evaluated the performance of\\n', 'text summarization methods which were LSA and NMF by\\n', 'comparing their results with the summaries manually curated by\\n', 'two experts from the Faculty of Liberal Arts, Ubon Ratchathani\\n', 'University.\\n', '\\n', 'The open-source python libraries such as numpy [19] and sklearn\\n', '[20] were used in our system. We converted the Thai travel news\\n', 'obtained from Thairath and Manager online newspapers to plain\\n', 'text. Then, the sentences of each news were segmented by human\\n', 'with the following format: Si = â€˜xxxâ€™, where Si represents the order\\n', 'of the sentence in the original document and â€˜xxxâ€™ represents the\\n', 'content of that sentence. After removing stop words and duplicate\\n', 'words, we built a document term matrix or matrix A then applied\\n', 'SVD and NMF to the matrix. Then, we used python modules\\n', 'numpy.linalg.svd to calculate SVD and sklearn.decomposition to\\n', 'calculate NMF. For sentence selection, we used Gong, Y. et al. and\\n', 'Murray, G. et al. approaches for calculating weight of the sentence\\n', 'scores then selected sentences with the highest scores into the\\n', 'summary. For keyword score calculation of NMF, we calculated\\n', 'the keyword score from Eq. (5) and then selected the sentence with\\n', 'the highest score from each concept. The python module\\n', 'sklearn.cluster was used for K-means clustering. The selected\\n', 'sentences from all approaches were in the same order as the original\\n', 'document. In this paper, we performed the 20%, 30% and 40%\\n', 'document compression. This meant 80%, 70% and 60% of the\\n', 'sentences will be selected into the summary.\\n', '\\n', 'S3\\n', '\\n', '4.\\n', '\\n', 'Figure 1. Document summarization pipeline based on LSA\\n', 'and NMF\\n', '\\n', 'S2\\n', '\\n', 'For sentence selection by K-means clustering, we grouped similar\\n', 'sentences into the same cluster using the following steps:\\n', '1. Randomly select K sentences as the representative of K\\n', 'groups. K in this paper is the number of sentences that\\n', 'will be selected into the summary.\\n', '2. Calculate centroid of each group by using the value of\\n', 'sentence vector from V matrix for LSA and ð»ð»ð‘‡ð‘‡ matrix\\n', 'for NMF.\\n', '3. Use cosine similarity to calculate sentence similarity\\n', 'between a sentence and the centroid of each group. Then\\n', 'assign that sentence to the group with the highest\\n', 'similarity.\\n', '4. Repeat steps 2-3 until all sentences are assigned to a\\n', 'group, no sentences change the group, or the similarity\\n', 'between sentences and their centroid is close.\\n', '5. Select a sentence with the maximum similarity score with\\n', 'the centroid of the group and add it into the summary.\\n', '\\n', 'S1\\n', '\\n', '3.6\\n', '\\n', 'Aâˆ™B\\n', '=\\n', 'n\\n', 'n\\n', '||A|| ||B||\\n', 'ï¿½âˆ‘i=1\\n', 'A2i ï¿½âˆ‘i=1\\n', 'Bi2\\n', '\\n', '(6)\\n', '\\n', 'Mr.Yontas\\n', 'ak\\n', '\\n', '1\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', 'Supason\\n', '\\n', '1\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', 'Tourism\\n', 'Authority\\n', 'of Thailand\\n', '\\n', '1\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', '0\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', 'â€¦\\n', '\\n', '\\x0cTable 3 demonstrates an example of a matrix ð´ð´, constructed from\\n', 'word count by sentence of a Thai travel news. It was composed of\\n', '98 words and 9 sentences. This matrix ð´ð´ was then applied with the\\n', 'LSA and NMF. The sentence vectors were calculated from the term\\n', 'weight and the semantic feature vectors from Eq. (1) for LSA and\\n', 'Eq. (2) for NMF.\\n', '\\n', 'sentences from all concepts. The Generic Sentence Relevance score\\n', 'for NMF also collected one sentence for each concept, the same as\\n', 'Gong, Y. et al. but with the highest score calculated by Eq. (5). As\\n', 'multiple important sentences could be selected from a more\\n', 'important concept, Murray, G. et al. outperformed both Gong, Y. et\\n', 'al. and the GRS method.\\n', '\\n', '6. EXPERIMENT AND RESULTS\\n', '6.1 Performance Evaluations Measure\\n', '\\n', '7.\\n', '\\n', 'We evaluated the results of the summarization by using standard\\n', 'accuracy, precision, recall, and F1 score [21]. These measurements\\n', 'quantify the differences between the summary from human and the\\n', 'experimental methods. The precision shows the correctness of the\\n', 'extracted sentences and the recall reflects the number of good\\n', 'sentences missed by the method.\\n', '\\n', '6. 2 Experiment Results\\n', '\\n', 'In this experimental set, we would like to explore how the different\\n', 'sentence selection methods: the Generic Sentence Relevance score\\n', 'and K-means clustering, affected the text summarization result.\\n', 'For K-means clustering, both SVD and NMF had similar\\n', 'summarization efficiency. The F1 score of SVD with K-means\\n', 'clustering was 0.83, 0.72, and 0.62 for the compression rate of 20%,\\n', '30%, and 40%. For the NMF with K-means clustering, the F1 score\\n', 'for the three compression rates was 0.83, 0.74 and 0.64.\\n', 'For the Generic Sentence Relevance score, the best F1 score for the\\n', 'compression rate of 20%, 30%, and 40% was 0.86, 0.78 and 0.68\\n', 'respectively and the best F1 scores for all compression rates were\\n', 'from the approach of Murray, G. et al.\\n', '\\n', 'Figure 2. Thai text summarization efficiency of 5 models\\n', 'Figure 2 shows the Thai text summarization efficiency of 5 models:\\n', '(1) NMF with GRS, (2) NMF with K-means, (3) SVD with sentence\\n', 'score by Gong, Y. et al., (4) SVD with K-means, and (5) SVD with\\n', 'sentence score by Murray, G. et al. applied to 400 Thai travel news,\\n', 'divided into 5 sets of 80 news each, with the varied compression\\n', 'rates of 20%, 30% and 40%.\\n', 'From this experiment, the best model based on keyword score for\\n', 'Thai travel news summarization was SVD with sentence selection\\n', 'by Murray, G. et al. This model with the compression rate of 20%\\n', 'got the highest score because Murray G. et al. method determined\\n', 'the number of sentences to be extracted from each concept based on\\n', 'the importance of that concept. The method of Gong, Y. et al., on\\n', 'the other hand was proposed to select only one sentence with the\\n', 'highest score from each concept so that the summary would include\\n', '\\n', 'CONCLUSIONS\\n', '\\n', 'In this paper, we applied several text summarization methods to\\n', 'Thai Travel News based on keyword scored in Thai language by\\n', 'extracting the most relevant sentences from the original document.\\n', 'We compared LSA and NMF together with different sentence\\n', \"selection methods, to find the algorithm suitable with this paper's\\n\", 'data source. We concluded that keyword scored calculation by LSA\\n', 'with sentence selection by Generic Sentence Relevance score by\\n', 'Murray, G. et al. was the best algorithm while the best compression\\n', 'rate of all models was 20%, for summarizing Thai Travel News\\n', 'compared with humans.\\n', 'In future work, we plan to perform the experiments with different\\n', 'types of documents and improve word segmentation of compound\\n', 'nouns that was not handled by Cutkum.\\n', '\\n', '8. ACKNOWLEDGMENTS\\n', '\\n', 'We would like to thank the department of computer engineering,\\n', 'faculty of engineering, Chulalongkorn University for providing\\n', 'computing facilities.\\n', '\\n']\n"
          ]
        }
      ],
      "source": [
        "# LOAD PAPER\n",
        "with open('paperThailand.txt', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "f.close()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\miki\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\miki\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'startswith'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbs\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m parsed_text \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39;49mBeautifulSoup(text, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Removing Square Brackets and Extra Spaces\u001b[39;00m\n\u001b[0;32m     11\u001b[0m article_text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[[0-9]*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, parsed_text\u001b[39m.\u001b[39mget_text())\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\__init__.py:328\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m rejections \u001b[39m=\u001b[39m []\n\u001b[0;32m    327\u001b[0m success \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[39mfor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarkup, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_encoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeclared_html_encoding,\n\u001b[0;32m    329\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontains_replacement_characters) \u001b[39min\u001b[39;00m (\n\u001b[0;32m    330\u001b[0m      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mprepare_markup(\n\u001b[0;32m    331\u001b[0m          markup, from_encoding, exclude_encodings\u001b[39m=\u001b[39mexclude_encodings)):\n\u001b[0;32m    332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    333\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\builder\\_lxml.py:182\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.prepare_markup\u001b[1;34m(self, markup, user_specified_encoding, exclude_encodings, document_declared_encoding)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessing_instruction_class \u001b[39m=\u001b[39m ProcessingInstruction\n\u001b[0;32m    180\u001b[0m     \u001b[39m# We're in HTML mode, so if we're given XML, that's worth\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39m# noting.\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     DetectsXMLParsedAsHTML\u001b[39m.\u001b[39;49mwarn_if_markup_looks_like_xml(markup)\n\u001b[0;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessing_instruction_class \u001b[39m=\u001b[39m XMLProcessingInstruction\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\builder\\__init__.py:535\u001b[0m, in \u001b[0;36mDetectsXMLParsedAsHTML.warn_if_markup_looks_like_xml\u001b[1;34m(cls, markup)\u001b[0m\n\u001b[0;32m    531\u001b[0m     prefix \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mXML_PREFIX\n\u001b[0;32m    532\u001b[0m     looks_like_html \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mLOOKS_LIKE_HTML\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m (markup \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[39mand\u001b[39;00m markup\u001b[39m.\u001b[39;49mstartswith(prefix)\n\u001b[0;32m    536\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m looks_like_html\u001b[39m.\u001b[39msearch(markup[:\u001b[39m500\u001b[39m])\n\u001b[0;32m    537\u001b[0m ):\n\u001b[0;32m    538\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_warn()\n\u001b[0;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'startswith'"
          ]
        }
      ],
      "source": [
        "# CLEAN DATA\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import bs4 as bs\n",
        "import re\n",
        "\n",
        "parsed_text = bs.BeautifulSoup(text, 'lxml')\n",
        "\n",
        "# Removing Square Brackets and Extra Spaces\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', parsed_text.get_text())\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "\n",
        "# Removing special characters and digits\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE DATA BACK TO TEXT FILE\n",
        "# Save Preprocessed file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95urUhWR2rh7"
      },
      "outputs": [],
      "source": [
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaJzkGHn2uIN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4fFmkkg2wCF"
      },
      "outputs": [],
      "source": [
        "stopwords = list(STOP_WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOXYtmbn2x9x"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# For some OS you need to make a correctino installing the package separately using this command:\n",
        "# !pip3 install -U spacy\n",
        "# !python3 -m spacy download en_core_web_sm\n",
        "# run both command above to install a separate package from the spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehejsUuR2zfs"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KttOAXos272b",
        "outputId": "530e4132-cbab-40cb-a2f8-ade43b4f5ad6"
      },
      "outputs": [],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2y6WfuOJ28OQ",
        "outputId": "5fb27a6e-5eaf-4a31-fc58-909d649624f1"
      },
      "outputs": [],
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNk7VDiI29-s"
      },
      "outputs": [],
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOh7L2ZD3AFD",
        "outputId": "6983aa8c-6b67-4333-da58-98e09ab69dce"
      },
      "outputs": [],
      "source": [
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbRUtgdR3BuR"
      },
      "outputs": [],
      "source": [
        "max_frequency = max(word_frequencies.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hF7k2wP3DYR",
        "outputId": "aa809d5b-9d4f-41a0-f512-4448bd607c1c"
      },
      "outputs": [],
      "source": [
        "max_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4q0KDAr3GAC"
      },
      "outputs": [],
      "source": [
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlBB0xdJ3INI",
        "outputId": "4f536368-21f2-4b9b-fbc3-3fbeed5c8e4e"
      },
      "outputs": [],
      "source": [
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyDY-NLw3J4b",
        "outputId": "b96554c6-de90-4a30-f4ce-c38a931b24a3"
      },
      "outputs": [],
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPZdMgs_3KP2"
      },
      "outputs": [],
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8vLdWTa3MdB",
        "outputId": "3d70da4e-58ce-4244-d890-12a50bf865cf"
      },
      "outputs": [],
      "source": [
        "sentence_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfZLsCg53wCp"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiRTSRGj3xg6",
        "outputId": "dcd4a9b3-173c-408a-f34d-eb9a827d0d87"
      },
      "outputs": [],
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xw5ftgc3y_d"
      },
      "outputs": [],
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rBD_6w_30dv",
        "outputId": "9a8dc1f3-a9e5-491e-ac37-a5a10801c10f"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuiWQWIB311O"
      },
      "outputs": [],
      "source": [
        "final_summary = [word.text for word in summary]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGs6IsSb33tk"
      },
      "outputs": [],
      "source": [
        "summary = ' '.join(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIpxcY4X35CS",
        "outputId": "9f092df7-8971-4898-c5cc-c578e05d900d"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmbwOjsO36qg",
        "outputId": "3580101a-eb3c-4d1d-d5ab-fffa6c5da12b"
      },
      "outputs": [],
      "source": [
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MDcZib3Q_rTh"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'words' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m     words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m sentences, words\n\u001b[1;32m---> 25\u001b[0m \u001b[39mprint\u001b[39m(words)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    cleaned_text = re.sub('<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters\n",
        "    cleaned_text = re.sub('[^a-zA-Z0-9]', ' ', cleaned_text)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = [word.lower() for sent in sentences for word in word_tokenize(sent)]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return sentences, words\n",
        "\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYr1m1cy_tza"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def score_sentences(sentences, words):\n",
        "    word_frequencies = Counter(words)\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for index, sent in enumerate(sentences):\n",
        "        for word in word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    if index not in sentence_scores:\n",
        "                        sentence_scores[index] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[index] += word_frequencies[word]\n",
        "\n",
        "    return sentence_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciawg8Fe_71O"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "def select_sentences(sentences, sentence_scores, num_sentences):\n",
        "    selected_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    summary = [sentences[index] for index in selected_sentences]\n",
        "    return ' '.join(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UeU9J-5vACUE",
        "outputId": "990696b1-b2d9-4d13-e57e-f6eefccc1231"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2hRLgM1sAxUf",
        "outputId": "525e98bb-fa6b-4137-fc49-4c6ebb2af1b6"
      },
      "outputs": [],
      "source": [
        "#import gensim\n",
        "\n",
        "# Training Word2Vec model\n",
        "#sentences = [['sentence', 'one'], ['sentence', 'two'], ...]\n",
        "#model = gensim.models.Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Get word vector representation\n",
        "#word_vector = model['word']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3buEXMQAyPG"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Build LSTM model\n",
        "#model = Sequential()\n",
        "#model.add(LSTM(128, input_shape=(max_sequence_length, embedding_dim)))\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Train the model\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.fit(X_train, y_train, batch_size=32, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n5iWT0EA3BF"
      },
      "outputs": [],
      "source": [
        "#from transformers import pipeline\n",
        "\n",
        "# Load BERT-based summarization model\n",
        "#summarizer = pipeline('summarization')\n",
        "\n",
        "# Summarize text using BERT\n",
        "#text = \"Your input text here\"\n",
        "#summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# Access summary\n",
        "#print(summary[0]['summary_text'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
