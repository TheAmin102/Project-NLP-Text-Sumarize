{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/"
      ],
      "metadata": {
        "id": "NLhyyTCGPZ4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Tokenization –\n",
        "In this step, we decompose our text data into the smallest unit called tokens. Generally, our dataset consists long paragraph which is made up of many lines and lines are made up of words. It is quite difficult to analyze the long paragraphs so first, we decompose the paragraphs into separate lines and then lines are decomposed into words."
      ],
      "metadata": {
        "id": "9liL8OD6PcnU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJsY1DsfPLA9",
        "outputId": "9d527f0d-8e4c-4f5f-c96d-15452ce60993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP is a systematic proces that help to do various task.', 'It is used to analyze, organize and summarize the data']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "text = \"NLP is a systematic proces that help to do various task. It is used to analyze, organize and summarize the data\"\n",
        "#decomposing the paragraph into lines\n",
        "lines=tokenize.sent_tokenize(text)\n",
        "print(lines)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total line in the given paragraph\",len(lines))\n",
        "['NLP is a systematic proces that help to do various task.', 'It is used to analyze, organize and summarize the data']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taVLq5JpPhLu",
        "outputId": "96d405ef-f27d-414f-8795-0b47a8096afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total line in the given paragraph 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP is a systematic proces that help to do various task.',\n",
              " 'It is used to analyze, organize and summarize the data']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decomposing the text into words\n",
        "words=tokenize.word_tokenize(text)\n",
        "print(words)\n",
        "print(\"Total number of words in the paragraph are\",len(words))\n",
        "['NLP', 'is', 'a', 'systematic', 'proces', 'that', 'help', 'to', 'do', 'various', 'task', '.', 'It', 'is', 'used', 'to', 'analyze', ',', 'organize', 'and', 'summarize', 'the', 'data']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zPF8vIgPjmF",
        "outputId": "3c8e2e09-de6d-4adc-d23e-2a662a63a835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'a', 'systematic', 'proces', 'that', 'help', 'to', 'do', 'various', 'task', '.', 'It', 'is', 'used', 'to', 'analyze', ',', 'organize', 'and', 'summarize', 'the', 'data']\n",
            "Total number of words in the paragraph are 23\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP',\n",
              " 'is',\n",
              " 'a',\n",
              " 'systematic',\n",
              " 'proces',\n",
              " 'that',\n",
              " 'help',\n",
              " 'to',\n",
              " 'do',\n",
              " 'various',\n",
              " 'task',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'used',\n",
              " 'to',\n",
              " 'analyze',\n",
              " ',',\n",
              " 'organize',\n",
              " 'and',\n",
              " 'summarize',\n",
              " 'the',\n",
              " 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Normalization –\n",
        "Most of the dataset contains many words that are generated from a single word by adding some suffix or prefix. These conditions can cause redundancy in our dataset and it will not give a better output. So it is an important task to convert those words into their root form that also decreases the count of unique words in our dataset and improves our outcomes.\n",
        "\n",
        "In NLP, Two methods are used to perform the normalization of the dataset:-\n",
        "\n",
        "       a) Stemming –\n",
        "Stemming is used to remove any kind of suffix from the word and return the word in its original form that is the root word but sometimes the root word that is generated is a non-meaningful word or it does not belong to the English dictionary.\n",
        "\n",
        "Eg- the words ”playful”, ”played”, ”playing” will be converted to “play” after stemming."
      ],
      "metadata": {
        "id": "9HRLZacyPeza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing module for stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words=[\"playing\",\"playful\",\"played\"]\n",
        "print('Original words-', words)\n",
        "stem_words=[]\n",
        "for word in words:\n",
        "    root_word= ps.stem(word)\n",
        "    stem_words.append(root_word)\n",
        "print(\"After stemming -\", stem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTrBUuNDPqqf",
        "outputId": "ae9919fd-e904-436d-cf6f-8b036965da10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words- ['playing', 'playful', 'played']\n",
            "After stemming - ['play', 'play', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Lemmatization –\n",
        "Lemmatization is similar to stemming but it works with much better efficiency. In lemmatization, the word that is generated after chopping off the suffix is always meaningful and belongs to the dictionary that means it does not produce any incorrect word. The word generated after lemmatization is also called a lemma."
      ],
      "metadata": {
        "id": "_sNCmJNQPuXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wml = WordNetLemmatizer()\n",
        "words_orig = [\"cries\", \"crys\", \"cried\"]\n",
        "print('Original words-', words_orig)\n",
        "lemma_words = []\n",
        "for word in words_orig:\n",
        "    tokens = wml.lemmatize(word)\n",
        "    lemma_words.append(tokens)\n",
        "print(\"After lemmatization\", lemma_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuzTZHu9Ptc_",
        "outputId": "ba2f299e-a149-48fa-ecfb-68aa44734298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words- ['cries', 'crys', 'cried']\n",
            "After lemmatization ['cry', 'cry', 'cried']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference between lemmatization and stemming-\n",
        "Lemmatization is a better way to obtain the original form of any given text rather than stemming because lemmatization returns the actual word that has some meaning in the dictionary."
      ],
      "metadata": {
        "id": "AgAklB8QP1_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "wml = WordNetLemmatizer()\n",
        "lemma_words=[]\n",
        "stem_words=[]\n",
        "words=[\"increases\"]\n",
        "print('Original words-', words)\n",
        "for word in words:\n",
        "    root_word= ps.stem(word)\n",
        "    stem_words.append(root_word)\n",
        "    tokens = wml.lemmatize(word)\n",
        "    lemma_words.append(tokens)\n",
        "print(\"After stemming -\", stem_words)\n",
        "print(\"After lemmatization\", lemma_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJ4HJ4kP40e",
        "outputId": "fecad777-77f7-4033-c9e2-5c14879203ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words- ['increases']\n",
            "After stemming - ['increas']\n",
            "After lemmatization ['increase']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Removing Stop-words\n",
        "Stop words are those words in any language that helps to combine the sentence and make it meaningful. for eg. In the English language various words like “I, am, are, is to, etc. are all known as stop-wards. But these stop-words are not that much useful for our model so there is a need to remove these stop-words from our dataset so that we can focus on only important words rather than these supporting words."
      ],
      "metadata": {
        "id": "IRt78u_fP6Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53jHlSqEP8Yt",
        "outputId": "4d686edb-3094-4fdc-fae3-4844572ad143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"NLP consists of a systematic process to organize the massive data and help to solve the numerous automated tasks in various fields like – machine translation, speech recognition, automatic summarization etc.\"\n",
        "words = word_tokenize(text)\n",
        "print(\"Total words in the paragraph:\", len(words))\n",
        "print(words)\n",
        "\n",
        "filter_words = []\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "# Removing the stop-words\n",
        "for w in words:\n",
        "    if w not in stopwords:\n",
        "        filter_words.append(w)\n",
        "\n",
        "print(\"Total words after removing stop-words:\", len(filter_words))\n",
        "print(filter_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gCeeJXxP_Kl",
        "outputId": "1cb3d85b-018b-4328-f4a0-22a640b31637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in the paragraph: 34\n",
            "['NLP', 'consists', 'of', 'a', 'systematic', 'process', 'to', 'organize', 'the', 'massive', 'data', 'and', 'help', 'to', 'solve', 'the', 'numerous', 'automated', 'tasks', 'in', 'various', 'fields', 'like', '–', 'machine', 'translation', ',', 'speech', 'recognition', ',', 'automatic', 'summarization', 'etc', '.']\n",
            "Total words after removing stop-words: 26\n",
            "['NLP', 'consists', 'systematic', 'process', 'organize', 'massive', 'data', 'help', 'solve', 'numerous', 'automated', 'tasks', 'various', 'fields', 'like', '–', 'machine', 'translation', ',', 'speech', 'recognition', ',', 'automatic', 'summarization', 'etc', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Part of Speech tag (POS)\n",
        "In this process, each token is listed according to its part of speech that whether it is a noun, adjective, verb, etc.\n",
        "Some basics tags used for part of speech –\n",
        "Lable(tags)           Part of Speech\n",
        "\n",
        "NN                                Singular Noun\n",
        "\n",
        "NNP                             Proper Noun\n",
        "\n",
        "JJ                                 Adjective\n",
        "\n",
        "VBD                             Past Tense Verb\n",
        "\n",
        "IN                                Preposition\n",
        "\n",
        "DT                               Determiner"
      ],
      "metadata": {
        "id": "GIFJutZxQLIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"NLP consists of a systematic process to organize the massive data and help to solve the numerous automated tasks in various fields like – machine translation, speech recognition, automatic summarization etc.\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "# Removing the stop-words\n",
        "filter_words = [w for w in words if w not in stopwords]\n",
        "\n",
        "# Part-of-speech tagging\n",
        "pos = pos_tag(filter_words)\n",
        "\n",
        "print(pos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ZQr3trQLlR",
        "outputId": "e148d9a9-3e89-467c-a614-bc95bfe78532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLP', 'NNP'), ('consists', 'VBZ'), ('systematic', 'JJ'), ('process', 'NN'), ('organize', 'VBP'), ('massive', 'JJ'), ('data', 'NNS'), ('help', 'NN'), ('solve', 'VBP'), ('numerous', 'JJ'), ('automated', 'VBN'), ('tasks', 'NNS'), ('various', 'JJ'), ('fields', 'NNS'), ('like', 'IN'), ('–', 'NNP'), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('automatic', 'JJ'), ('summarization', 'NN'), ('etc', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}